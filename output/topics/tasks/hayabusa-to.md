I'll help you synthesize this documentation. Let me first check if there are other related documents in the provided content.

Looking at this content, I can see it's a single document about "Hayabusa to BigQuery" integration. Since only one document was provided, I'll output it as a complete, self-contained topic with enhanced clarity and organization:

---

# Hayabusa to BigQuery Integration

## Overview

The BigQuery output integration enables you to send Hayabusa analysis results directly to a BigQuery table, allowing SQL-like queries against forensic event log data at scale. This is particularly valuable for analyzing event logs from tens, hundreds, or thousands of systems simultaneously.

For guidance on using Hayabusa within LimaCharlie, see the [Hayabusa Extension documentation](../reference/ext-hayabusa.md).

## Use Cases

When analyzing event logs from multiple systems with Hayabusa, you have two primary options:

1. **Timesketch Integration**: Send the resulting CSV artifact to platforms like [Timesketch](https://timesketch.org/) for further analysis. The CSV generated by Hayabusa in LimaCharlie is compatible with Timesketch.
2. **BigQuery Analysis**: Run SQL queries against all Hayabusa results directly in BigQuery for scalable, cloud-based analysis.

## Architecture

Once configured, Hayabusa analysis results flow automatically from LimaCharlie to a BigQuery dataset, where they can be queried using standard SQL syntax.

![BigQuery dataset containing Hayabusa results](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Screenshot%202024-02-27%2010.50.46%20AM.png)

## Setup Instructions

### Step 1: Google Cloud Project Configuration

#### Create a Service Account

1. Navigate to your Google Cloud project
2. Go to **IAM** → **Service Accounts**
3. Click **Create Service Account**
4. Configure the service account with appropriate permissions
5. Generate credentials:
   - Click on the newly created service account
   - Navigate to the **Keys** tab
   - Click **Add Key** → **Create New Key**
   - Select **JSON** format
   - Download the JSON key file (you'll need this for LimaCharlie configuration)

![Service Account Key Creation](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28188%29.png)

#### Create BigQuery Dataset and Table

1. Open BigQuery in your Google Cloud Console
2. Create a new **Dataset**:
   - Dataset ID: `hayabusa` (or your preferred name)
   - Location: Choose appropriate region
3. Create a new **Table** within the dataset:
   - Table ID: `hayabusa` (or your preferred name)
   - Schema: Define the following fields:

```
computer:STRING
message:STRING
timestamp:STRING
details:STRING
channel:STRING
event_id:STRING
level:STRING
mitre_tactics:STRING
mitre_tags:STRING
extra:STRING
```

**Important Notes:**
- Dataset and table names are arbitrary but must match your LimaCharlie output configuration
- This schema is based on the CSV output using the `timesketch-verbose` profile
- You can customize the schema to include any Hayabusa event fields you need

### Step 2: LimaCharlie Output Configuration

1. In LimaCharlie, navigate to **Outputs** in the side menu
2. Click **Add Output**
3. Configure the output with the following settings:

#### Basic Configuration

- **Output stream**: `Events`
- **Destination**: `Google Cloud BigQuery`
- **Name**: `hayabusa-bigquery` (or your preferred name - note this for later steps)

#### BigQuery Settings

- **schema**: 
```
computer:STRING, message:STRING, timestamp:STRING, details:STRING, channel:STRING, event_id:STRING, level:STRING, mitre_tactics:STRING, mitre_tags:STRING, extra:STRING
```

- **Dataset**: Your BigQuery dataset name (e.g., `hayabusa`)
- **Table**: Your BigQuery table name (e.g., `hayabusa`)
- **Project**: Your GCP project ID
- **Secret Key**: Paste the complete JSON secret key from your GCP service account

#### Advanced Options

##### Custom Transform

Paste the following JSON transform to map Hayabusa event fields to BigQuery columns:

```json
{
  "channel": "event.results.Channel",
  "computer": "event.results.Computer",
  "message": "event.results.message",
  "timestamp": "event.results.datetime",
  "details": "event.results.Details",
  "event_id": "event.results.EventID",
  "level": "event.results.Level",
  "mitre_tactics": "event.results.MitreTactics",
  "mitre_tags": "event.results.MitreTags",
  "extra": "event.results.ExtraFieldInfo"
}
```

**Note**: This transform is based on the `timesketch-verbose` profile. You can customize it to include any Hayabusa event fields based on your schema.

##### Event Filtering

- **Specific Event Types**: `hayabusa_event`
- **Sensor**: `ext-hayabusa`

These filters ensure only Hayabusa events from the Hayabusa extension are sent to BigQuery.

### Step 3: Verification

Once configured, Hayabusa analysis results will automatically flow to your BigQuery table. You can verify the integration by:

1. Running Hayabusa analysis on a sensor
2. Checking your BigQuery table for new records
3. Running test queries against the data

## Schema Customization

The default schema supports the most common Hayabusa fields using the `timesketch-verbose` profile. You can customize both the BigQuery schema and the LimaCharlie transform to include additional fields:

### Available Hayabusa Fields

Common fields available for mapping include:
- `Computer` - Hostname
- `message` - Event description
- `datetime` - Event timestamp
- `Details` - Additional event details
- `Channel` - Windows event log channel
- `EventID` - Windows event ID
- `Level` - Event severity level
- `MitreTactics` - MITRE ATT&CK tactics
- `MitreTags` - MITRE ATT&CK techniques
- `ExtraFieldInfo` - Additional metadata

To add custom fields, update both the BigQuery table schema and the LimaCharlie output transform JSON accordingly.

## Querying Results

Once data is flowing to BigQuery, you can run SQL queries to analyze Hayabusa results at scale. Example queries might include:

- Filtering events by MITRE ATT&CK tactics
- Aggregating events by computer or time period
- Searching for specific event IDs or patterns
- Correlating events across multiple systems

The SQL-based approach enables complex analysis that would be difficult with traditional log analysis tools.