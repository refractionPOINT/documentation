# LimaCharlie Complete Documentation

This file contains all LimaCharlie documentation in a single document.

**Generated from 395 documentation pages**

---

# Add-ons & Extensions

# Add-Ons

LimaCharlie allows you to extend the capability of the platform via various add-ons. These can be enabled via the add-ons marketplace.

## Types of Add-Ons

We categorize our add-ons into four different categories, depending on the functionality or method with which it augments the LimaCharlie platform.

* `api` add-ons are tightly integrated add-ons that enable LimaCharlie's core features
* `extension` add-ons are cloud services that can perform jobs on behalf of or add new capabilities to an Organization.
* `lookup` add-ons maintain reference to dynamic lists for use within D&R rules.
* `ruleset` add-ons provide managed sets of rules to use within D&R rules.

## Subscribing to Add-Ons

Add-ons can be found and added to organizations through the add-ons marketplace or by searching from within the Add-ons view in an organization (see below). The description of the add-on may include usage information about how to use it once it's installed.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/addons.png)

The following add-ons enable additional functionality in the web application:

* `atomic-red-team` - scan Windows sensors right from their `Overview` page
* `exfil` - enables `Exfil Control` to configure which events should be collected per platform
* `infrastructure-service` - enable `Templates` in the UI to manage org config in `yaml`
* `insight` - enables retention & browsing events and detections via `Timeline` and `Detections`
* `logging` - enables `Artifact Collection` to configure which paths to collect from
* `replay` - adds a component next to rules for testing them against known / historical events
* `responder` - sweep sensors right from their `Overview` page to find preliminary IoCs
* `yara` - enables `YARA Scanners` view to pull in sources of YARA rules and automate scans with them

## Creating Add-ons

Users can create their own add-ons and optionally share them in the marketplace. Add-ons are your property, but may be evaluated and approved / dismissed due to quality or performance concerns. If you have questions, contact us.

> **Got an idea?**
> 
> Are you interested in creating an add-on or developing another project for LimaCharlie? Check out our Developer Grant Program.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Building Extensions

## Why Extensions?

Building functionality as a LimaCharlie Extension provides you specific convenience:

* **Multi-tenancy**: LC organizations can subscribe to your extension and you can replicate the features you're building across tenants.
* **Credentials handling**: you don't need to store any credentials from LC organizations. Every callback you receive will include an authenticated LimaCharlie SDK for the Organization relevant to the callback, with the permissions you requested for the extension.
* **Configuration**: you're always welcome to store some configuration wherever the extension lives, but as a convenience LC will provide you with a configuration JSON object for your extension (stored in Hive) and with a callback for you to validate the content of the configuration when a user makes a modification.
* **GUI**: each extension defines its own Schema, a structure indicating to LimaCharlie what actions the extension exposes, how to call it and what to expect as a return value from actions. This information is then automatically interpreted by LimaCharlie to generate a custom user interface for your extension, making it extremely easy to expose new functionality in LimaCharlie without having to build any kind of UI (though you're always free to build one if you'd like).

### Public/Private Limitations

Anyone can build Extensions for LimaCharlie. The only limit is put on making an Extension public. Private extensions require the owner of the extension to have the `billing.ctrl` and `user.ctrl` permission on an organization in order to subscribe the organization to the private extension.

### Want to take your Extension public?

If you'd like to make your extension public (and/or monetize it), reach out to `answers@limacharlie.io` and we'll help you out. Once public, an extension is visible by everyone and can subscribed by everyone.

## High Level Structure

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28252%29.png)

Extensions are small services that receive webhooks from LimaCharlie. This means building an extension requires exposing a small HTTPS service to the internet. We recommend using something like [Google Cloud Run](https://cloud.google.com/run/), but ultimately you could also use AWS Lambdas or even host on your own hardware.

This https server will communicate with the LimaCharlie cloud according to a simple protocol using JSON.

That being said, don't worry, you don't need to know the underlying way the extension protocol works as long as you're comfortable with our public implementations.

## Getting Started

Want to get your hands on an example? We recommend using one of the following frameworks to get started.

* Golang: <https://github.com/refractionPOINT/lc-extension>
* Python: <https://github.com/refractionPOINT/lc-extension/tree/master/python>

For a more step-by-step overview, let's dig into some of the core concepts of building an extension. We will reference Golang since it provides stricter typing, but conceptually it's the same across implementations.

### Extension Definition

To create an extension, start by creating a definition - accessible through the [web interface for your personal add-ons](https://app.limacharlie.io/add-ons/published).

The required aspects of your definition are as follows:

* **Destination URL:** this is the HTTPS URL where your extension will be reachable at.
* **Required Extensions:** this is the list of other extensions your extension assumes it will have access to. When an org subscribes and is missing one of those, the user will be prompted to subscribe to these.
* **Shared Secret:** this is an arbitrary string that will be used by LimaCharlie and your extension to sign webhooks to your extension, allowing it to very the authenticity of the hook. Make it something at least 32 characters and random.
* **Extension Flairs:** these are modifiers that will be applied to your extension. Namely the `segment` flair will isolate the resources the extension can access so that it can only see and modify things (like rules) that it has created, making it great for extensions that need a narrow scope, you should enable it unless you know you need it off. The `bulk` flair tells LimaCharlie that it expects to make a lot of API calls to the LC cloud, which will increase the API quota for the extension.
* **Permissions:** the list of permissions this extension requires on each organization subscribed to it. Use the least amount of permissions possible.

### Schema

The Extension Schema is the next important piece of building your extension. It describes what your extension can do and helps define the GUI.

Here's an example high-level structure of a schema.

```
{
  "config_schema": {
    "fields": { ... }
    "requirements": null
  },
  "request_schema": {
    // defines two custom requests, 'dir_list' and 'refresh'
    "dir_list": {
      "is_impersonated": false,
      "is_user_facing": false,
      "long_description": "directory listing",
      "parameters": {
        "fields": { ... },
        "requirements": null
      },
      "short_description": "directory listing"
    },
    "refresh": {
      "is_impersonated": false,
      "is_user_facing": true,
      "long_description": "refresh data",
      "parameters": {
        "fields": { ... },
        "requirements": null
      },
      "short_description": "refresh data"
    },
  },
  "required_events": [
    "subscribe",
    "unsubscribe"
  ]
}
```

**The Field Configuration**
Notice that for both the `config_schema` and the `request_schema` there is a recurring object structure that looks like the following:

```
"fields": { .. }, // key-value pair
"requirements": [[]],
```

While hidden in the example above, each `field` key-value pair shares the same structure and has a minimal implementation as such:

```
field_name: {
  data_type: "string",
  description: "",
},
```

The `requirements` field references the field keys to define whether or not certain fields individually or as a set are required. You can think of the first array to join elements with an AND, while the nested array serves as an OR.
For example:

* `[['denominator'], ['numerator']]` means: (denominator AND numerator),
* `[['denominator'], ['numerator', 'default']]` means: (denominator AND (one of numerator OR default)).

When getting started, we recommend utilizing the simplest data type applicable. This will enable you to get a grasp of the whole extensions framework and allow you to quickly test our your service. Such as `string`, `boolean`, `json`, etc.

Afterwards, we recommend you define the data_type and other optional fields further, so that the UI may adapt to your defined data types. For more details, please see the [page on data types](/v2/docs/schema-data-types) or review the code definitions [here](https://github.com/refractionPOINT/lc-extension/blob/master/common/config_schema.go).

#### Config Schema (optional)

The config schema is a description of what the extension's config should look like, when stored as a Hive record in the `extension_configuration` [Hive](/v2/docs/config-hive) for convenience.

Not all extensions will have a configuration, feel free to reach out on the community slack if you need help determining whether or not your extension needs a configuration.

At the core, the config schema is simply a list of fields.

#### Request Schema

Every Request Schema exists as a key value pair of the request name, and a corresponding schema contents. The critical contents include the following fields:

* **is_impersonated**: Whether or not the request impersonates the user through it's authentication
* **is_user_facing**: Whether or not this request should be visisble to the user in the UI. It does not prevent this request from bieng used through the API or as a `supported_action` (more on that later).
* **parameters**: This contains the data_type and other fields *(recall the same fields format as the config schema)*

Other optional fields exist to facilitate the user experience, such as:

* **short_description**
* **long_description**
* **messages**: Includes 3 nested fields, `in_progress`, `success`, `error` to provide additional context for each case.

#### Response Schema (optional)

Each request schema may optionally contain a response schema in the same fields format as a config schema and the request parameters.

When getting started, we recommend that you skip this until you are ready to refine the extension's GUI, or you wish to clarify that kind of response a user should expect.

### Callbacks

Callbacks are functionality that an extension can specify whenever some type of event occurs.

#### Configuration Validation Callback

This callback is used by LimaCharlie to check the validity of a change in configuration done in Hive. If the configuration is valid, return success, otherwise you can return an error.

#### Event Callback

Events are events generated by the LimaCharlie platform outside your control. Currently, these 3 events are supported:

* **subscribe**: called when an organization subscribes to an extension.
* **unsubscribe**: called when an organization unsubscribes from an extension.
* **update**: called once a day per organization subscribed to the extension. It is a convenient way to perform updates to an organization like when needing to update D&R rules used by the extension.

Your extension will only receive these events if they were specified as of-interest in the extension's Schema.

#### Request Callback

The requests are the core way users, D&R rules or other extensions can interact with your extension. You can define one callback per `action`. It is common for an extension to have multiple actions, some public (for user-generated requests) and some private (to be used internally by the extension in the course of doing whatever it does).

## Simplified Frameworks

The Golang implementation of Extensions provides 3 different simplified frameworks to make the job of producing a new extension more straight forward in specific cases: <https://github.com/refractionPOINT/lc-extension/tree/master/simplified>

### D&R

This simplified framework, found in `dr.go` allows you to package D&R rules as an extension making it easy for you to distribute and update D&R rules to many orgs. Its core mechanism is based on defining the `GetRules()` function and returning a structure like `map[DR-Namespace]map[RuleName]RuleContent`. The simplified framework takes care of the recurring updates and everything else.

### Lookup

Similar to the D&R simplified framework, but is used to package Lookups. Example: <https://github.com/refractionPOINT/lc-extension/blob/master/examples/lookup/main.go>

### CLI

This simplified framework serves to streamline the integration of 3rd party Command Line Interface tools so that they can be automated using LimaCharlie, often bringing bi-directionality to the platform.

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Extensions

LimaCharlie Extensions provide a robust framework for enhancing and customizing the LimaCharlie Cloud's capabilities, enabling seamless integration with third-party tools, services, and custom logic. Extensions serve as modular components that expand LimaCharlie's native functionality, allowing organizations to tailor their security environments to meet specific needs without being constrained by built-in features.

## Overview of Extensions

Extensions are defined at the global level across all LimaCharlie datacenters. This means that once an Extension is created, it is available for use in any Organization, provided that organization subscribes to it. Once subscribed, Extensions can interact with various components within the subscribing organization, whether through automated workflows or user-driven actions.

## Subscription and Permissions

To use an Extension, an organization must first subscribe to it. Upon subscription, the Extension is granted a set of predefined permissions that govern what actions it can perform within the organization. These permissions ensure that each Extension has controlled access to sensitive areas of the organization's security infrastructure.

Some Extensions are also capable of "impersonating" the user or automation component that triggered them, which allows the Extension to perform actions as if it were the user itself. This feature is particularly useful for Extensions that need to execute actions in response to detection and response rules or other automated triggers, without needing separate user credentials.

## Types of Extensions: Public, Private, Labs

**Private Extensions**: These are created by individual users (known as Owners) and are restricted for use only within the organizations where the Owner holds specific permissions, such as `billing.ctrl` and `user.ctrl`. Private Extensions are ideal for internal integrations or for extending capabilities within a limited scope, such as an organization's specific infrastructure or toolset.

**Public Extensions**: Public Extensions are available for subscription by any organization across the LimaCharlie platform. To make an Extension public, users first create it as private and, once fully developed and tested, they can submit it to LimaCharlie for approval. Once approved, the Extension becomes available for any organization to subscribe to. This process ensures that public Extensions are stable, secure, and beneficial to the broader community.

**LimaCharlie Labs**: Labs extensions are capabilities that LimaCharlie is making available on the SecOps Cloud Platform in an preview state.

> **Note:** Extensions with this label are expected to grow and change. The extension has been tested and is ready to be used by users, but some things such as a polished user interface may be missing or in prototype form.
>
> **Your feedback is greatly appreciated!**

Here is the list of current capabilities under LimaCharlie Labs:

- Playbooks extension
- AI Agent Engine

## Use Cases for Extensions

Extensions can be used for a wide range of purposes, including but not limited to:

- **Third-party Integrations**: Organizations can use Extensions to integrate LimaCharlie with external systems like SIEMs, threat intelligence platforms, or incident response tools. This enables seamless data flow between LimaCharlie and other components of the security stack.
- **Custom Automation**: Extensions can be designed to handle organization-specific automation tasks, such as alerting, incident response workflows, or data enrichment based on internal processes or external APIs.
- **Augmenting Detection and Response**: Some Extensions provide enhanced detection and response capabilities, offering additional insights or actions that can be executed in response to specific triggers or threats. These could range from triggering a custom script when a detection occurs to pulling data from external systems to enrich an alert.

## Developing Extensions

Any user with the necessary permissions can develop their own Extension, allowing for high levels of customization and control. Private Extensions offer flexibility for organizations looking to develop internal tools or workflows without exposing them to the public.

Any User (called Owners) can create an Extension, but those can only be "private", meaning only Organizations where the Owner has the `billing.ctrl` and `user.ctrl` permissions can subscribe to the Extension. To make an Extension "public" (where anyone can subscribe to it), first create your private Extension and once ready, reach out to LimaCharlie `answers@limacharlie.io`.

The process of developing an Extension is relatively straightforward:

1. **Creation**: The Owner of the Extension defines its purpose, functionality, and permissions. These permissions dictate what resources the Extension can interact with and what actions it can perform.
2. **Testing and Deployment**: Private Extensions can be deployed within the Owner's organization and tested before being considered for broader release. This allows organizations to ensure that the Extension works as expected without impacting production environments.
3. **Public Release**: When an Extension is ready for broader use, the Owner can request that LimaCharlie make the Extension public. LimaCharlie reviews the Extension to ensure it meets security and functionality standards before it's made available to the community.

For more information, see Building Extensions.

## Security Considerations

Extensions operate with a defined set of permissions to prevent misuse or overreach within an organization's infrastructure. By allowing granular control over what an Extension can do, LimaCharlie ensures that even third-party or public Extensions operate within the boundaries of an organization's security policies.

Furthermore, because Extensions can impersonate users to perform actions, organizations must carefully manage which Extensions they subscribe to and ensure they align with their security posture. Public Extensions go through an additional vetting process by LimaCharlie, offering another layer of trust for organizations that want to extend their capabilities through third-party tools.

## Scaling and Flexibility

As organizations grow and their security needs evolve, Extensions offer a flexible, scalable solution to integrate new tools, automate complex workflows, or enhance existing functionality. Organizations can subscribe to the Extensions that align with their specific use cases, effectively creating a customized security environment that adapts to both current and future challenges.

By leveraging LimaCharlie's Extension framework, organizations can expand their security capabilities without needing to constantly switch platforms or build integrations from scratch. Whether it's to incorporate new detection rules, automate response actions, or add support for a new tool, Extensions enable organizations to stay agile in an ever-changing security landscape.

In summary, LimaCharlie Extensions provide a powerful way for organizations to customize and scale their security infrastructure, offering a balance of flexibility, security, and control. With the ability to create both private and public Extensions, users can tailor LimaCharlie to their specific needs while contributing to the broader security community when ready.

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Atomic Red Team

Atomic Red Team is a library of tests mapped to the MITRE ATT&CK framework. Security teams can use these tests to validate their detection capabilities and security controls. The LimaCharlie Atomic Red Team integration allows you to run these security tests directly from the LimaCharlie platform.

## Overview

The Atomic Red Team add-on enables you to:

- Execute atomic tests from the Atomic Red Team library on your endpoints
- Validate detection and response rules
- Test security controls across the MITRE ATT&CK framework
- Integrate adversary emulation into your security testing workflow

## Prerequisites

- Active LimaCharlie organization
- Sensors deployed on endpoints where you want to run tests
- Appropriate permissions to execute commands on endpoints

## Installation

1. Navigate to the Add-ons section in your LimaCharlie organization
2. Find the Atomic Red Team add-on
3. Click "Subscribe" or "Enable"
4. Configure any required settings

## Usage

### Running Atomic Tests

You can execute Atomic Red Team tests through:

1. **Web Interface**: Select tests from the catalog and run them directly
2. **API**: Programmatically execute tests via the LimaCharlie API
3. **Detection & Response Rules**: Trigger tests as part of automated workflows

### Test Selection

Tests are organized by:

- MITRE ATT&CK Technique ID
- Platform (Windows, Linux, macOS)
- Test complexity
- Required dependencies

### Monitoring Results

Test execution results are captured as standard LimaCharlie telemetry:

- Command execution events
- Process creation events
- Network activity
- File system changes

You can create Detection & Response rules to alert on or respond to test activities.

## Best Practices

1. **Test in Non-Production First**: Always validate tests in a controlled environment
2. **Document Testing**: Keep records of which tests were run and when
3. **Review Test Details**: Understand what each test does before execution
4. **Monitor for Detection**: Verify your security controls detect the test activities
5. **Clean Up**: Some tests may leave artifacts; ensure proper cleanup procedures

## Example Workflow

1. Identify a MITRE ATT&CK technique you want to test (e.g., T1059.001 - PowerShell)
2. Select relevant atomic tests for your environment
3. Run the test on a designated test endpoint
4. Review LimaCharlie telemetry to confirm the activity was captured
5. Verify your Detection & Response rules triggered appropriately
6. Document results and gaps
7. Refine detection rules as needed

## Security Considerations

> **Warning**: Atomic Red Team tests simulate real adversary behavior. Some tests may:
> - Modify system configurations
> - Create files or registry entries
> - Execute potentially suspicious commands
> - Trigger security alerts

Always ensure you have proper authorization and are testing in appropriate environments.

## Additional Resources

- [Atomic Red Team GitHub Repository](https://github.com/redcanaryco/atomic-red-team)
- [MITRE ATT&CK Framework](https://attack.mitre.org/)
- LimaCharlie Detection & Response documentation

---

# Integrity

The Integrity add-on provides file integrity monitoring (FIM) capabilities for your endpoints. It tracks changes to critical files and directories, helping you detect unauthorized modifications that could indicate a security breach or system compromise.

## Overview

File Integrity Monitoring helps you:

- Detect unauthorized changes to critical system files
- Monitor configuration file modifications
- Track changes to sensitive directories
- Maintain compliance requirements
- Identify potential indicators of compromise

## Features

- Real-time file change detection
- Configurable monitoring paths
- Change detail capture (content hash, timestamps, permissions)
- Integration with Detection & Response rules
- Historical change tracking

## Prerequisites

- Active LimaCharlie organization
- Deployed sensors on endpoints to monitor
- Appropriate storage allocation for change logs

## Installation

1. Navigate to Add-ons in your LimaCharlie organization
2. Locate the Integrity add-on
3. Click "Subscribe" or "Enable"
4. Configure monitoring parameters

## Configuration

### Defining Monitoring Paths

Specify which files and directories to monitor:

**Windows Example:**
```
C:\Windows\System32\drivers\
C:\Windows\System32\config\
C:\Program Files\
```

**Linux Example:**
```
/etc/
/usr/bin/
/usr/sbin/
/boot/
```

**macOS Example:**
```
/System/Library/
/Library/
/usr/bin/
/usr/sbin/
```

### Monitoring Options

Configure what changes to track:

- **Content Changes**: Hash-based detection of file modifications
- **Attribute Changes**: Permissions, ownership, timestamps
- **Creation/Deletion**: New files or removed files
- **Recursive Monitoring**: Include subdirectories

### Exclusions

Define patterns to exclude from monitoring:

- Temporary files
- Log files that change frequently
- Cache directories
- Known benign application updates

## Event Types

The Integrity add-on generates events for:

1. **FILE_MODIFIED**: Content hash changed
2. **FILE_CREATED**: New file detected
3. **FILE_DELETED**: File removed
4. **FILE_ATTRIBUTES_CHANGED**: Permissions, ownership, or timestamps modified

## Detection & Response Integration

### Example Rule: Alert on Critical System File Change

```yaml
detect:
  event: FILE_MODIFIED
  op: and
  rules:
    - op: contains
      path: event/FILE_PATH
      value: "/etc/passwd"
respond:
  - action: report
    name: critical_file_modified
  - action: task
    command: history_dump
```

### Example Rule: Monitor Windows Registry Hive Changes

```yaml
detect:
  event: FILE_MODIFIED
  op: and
  rules:
    - op: contains
      path: event/FILE_PATH
      value: "\\System32\\config\\"
respond:
  - action: report
    name: registry_hive_modified
```

## Best Practices

1. **Start Small**: Begin monitoring critical paths before expanding scope
2. **Tune Exclusions**: Reduce noise by excluding expected changes
3. **Baseline Normal Activity**: Understand typical change patterns
4. **Prioritize Alerts**: Focus on the most critical assets first
5. **Regular Reviews**: Periodically review and update monitoring configurations
6. **Storage Management**: Monitor storage usage for integrity logs

## Common Use Cases

### Compliance Monitoring

Monitor files required by compliance frameworks:

- PCI-DSS: Payment system configuration files
- HIPAA: Healthcare application configurations
- SOX: Financial system files

### Malware Detection

Detect indicators of compromise:

- System binary modifications
- Startup location changes
- Configuration file tampering

### Incident Response

During investigations:

- Identify compromised files
- Establish timeline of changes
- Determine scope of breach

## Performance Considerations

File integrity monitoring can impact system performance:

- **Monitor Selectively**: Only track truly critical paths
- **Avoid High-Change Directories**: Exclude logs, temp files, caches
- **Hash Algorithm**: Balance security with performance needs
- **Polling Frequency**: Adjust based on criticality and system capacity

## Troubleshooting

### High Volume of Events

- Review and expand exclusion patterns
- Reduce monitoring scope
- Check for applications causing frequent changes

### Missing Events

- Verify paths are correctly specified
- Check sensor connectivity
- Confirm sufficient permissions for file access

### Storage Issues

- Implement retention policies
- Archive older integrity logs
- Adjust monitoring scope

## Additional Resources

- LimaCharlie Detection & Response documentation
- Sensor configuration guides
- Compliance framework requirements

---

# Extensions

## Articles

### Atomic Red Team
10 Oct 2025
[View Article](/docs/en/ext-atomic-red-team)

### Integrity
10 Oct 2025
[View Article](/docs/en/ext-integrity)

### Twilio
10 Oct 2025
[View Article](/docs/en/ext-twilio)

### Velociraptor
10 Oct 2025
[View Article](/docs/en/ext-velociraptor)

### Zeek
09 Oct 2025
[View Article](/docs/en/ext-zeek)

### Plaso
09 Oct 2025
[View Article](/docs/en/ext-plaso)

### Artifact
08 Oct 2025
[View Article](/docs/en/ext-artifact)

### BinLib
08 Oct 2025
[View Article](/docs/en/binlib)

### Exfil (Event Collection)
08 Oct 2025
[View Article](/docs/en/ext-exfil)

### Infrastructure
08 Oct 2025
[View Article](/docs/en/ext-infrastructure)

### Sensor Cull
08 Oct 2025
[View Article](/docs/en/ext-sensor-cull)

### Velociraptor to BigQuery
08 Oct 2025
[View Article](/docs/en/velociraptor-to-bigquery)

### Building the User Interface
08 Oct 2025
[View Article](/docs/en/building-the-user-interface)

### YARA
07 Oct 2025
[View Article](/docs/en/ext-yara)

### Hayabusa
07 Oct 2025
[View Article](/docs/en/ext-hayabusa)

### Govee
07 Oct 2025
[View Article](/docs/en/ext-govee)

### VirusTotal Integration
07 Oct 2025
[View Article](/docs/en/tutorials-integratons-virustotal-integration)

### Reliable Tasking
12 Aug 2025
[View Article](/docs/en/ext-reliable-tasking)

### Lookup Manager
06 Aug 2025
[View Article](/docs/en/ext-lookup-manager)

### YARA Manager
31 Jul 2025
[View Article](/docs/en/ext-yara-manager)

---

# Add-ons

No documentation content is currently available for this tag.

---

# Extensions Documentation

This section contains documentation for various LimaCharlie extensions and integrations that extend platform capabilities.

## Available Extensions

### Security Testing & Validation

**Atomic Red Team**
Execute Atomic Red Team tests to validate detection rules and security controls.
[View Documentation](/docs/ext-atomic-red-team)

**Integrity**
Monitor and validate system integrity across endpoints.
[View Documentation](/docs/ext-integrity)

### Threat Detection & Analysis

**YARA**
Deploy and manage YARA rules for malware detection and threat hunting.
[View Documentation](/docs/ext-yara)

**YARA Manager**
Centralized management interface for YARA rules across your organization.
[View Documentation](/docs/ext-yara-manager)

**VirusTotal Integration**
Integrate VirusTotal threat intelligence for file and URL analysis.
[View Documentation](/docs/tutorials-integratons-virustotal-integration)

### Forensics & Incident Response

**Velociraptor**
Deploy Velociraptor for endpoint visibility and forensic investigations.
[View Documentation](/docs/ext-velociraptor)

**Velociraptor to BigQuery**
Stream Velociraptor artifacts directly to BigQuery for analysis.
[View Documentation](/docs/velociraptor-to-bigquery)

**Plaso**
Process and analyze timeline data using the Plaso framework.
[View Documentation](/docs/ext-plaso)

**Hayabusa**
Fast Windows Event Log forensics with Hayabusa integration.
[View Documentation](/docs/ext-hayabusa)

**Zeek**
Network security monitoring using Zeek (formerly Bro).
[View Documentation](/docs/ext-zeek)

**Artifact**
Collect and analyze forensic artifacts from endpoints.
[View Documentation](/docs/ext-artifact)

### Data Collection & Management

**Exfil (Event Collection)**
Configure event exfiltration and collection workflows.
[View Documentation](/docs/ext-exfil)

**Lookup Manager**
Manage threat intelligence lookups and enrichment data.
[View Documentation](/docs/ext-lookup-manager)

**BinLib**
Binary library management for detection engineering.
[View Documentation](/docs/binlib)

### Automation & Operations

**Reliable Tasking**
Ensure reliable task execution across distributed sensors.
[View Documentation](/docs/ext-reliable-tasking)

**Sensor Cull**
Automatically manage inactive or obsolete sensors.
[View Documentation](/docs/ext-sensor-cull)

**Infrastructure**
Manage infrastructure resources and configurations.
[View Documentation](/docs/ext-infrastructure)

### Integrations

**Twilio**
SMS and voice notification integration via Twilio.
[View Documentation](/docs/ext-twilio)

**Govee**
IoT device integration for physical security indicators.
[View Documentation](/docs/ext-govee)

### Development

**Building the User Interface**
Guidelines for building custom UI extensions.
[View Documentation](/docs/building-the-user-interface)

---

**Note:** This documentation covers v2 of the extensions API. For legacy v1 documentation, see [v1 Deprecated](/v1/docs/tags/extensions).

---

# Using Extensions

## Components

Extensions can be interacted with using two main components:

### Configurations

Extension Configurations are records in [Hive](/v2/docs/config-hive). Each Extension has its configuration in the Hive record of the same name in the `extension_configuration` Hive.

These configurations are manipulated by simply storing the value in the record, LimaCharlie takes care of validating and notifying the Extension with the new value.

Configurations are a great way of storing rarely-written settings for an Extension without the developer of the Extension having to manage secure storage for it.

The structure of the configuration for a given Extension is published by the Extension via its "schema".

Schemas are available through the [Schema API](https://api.limacharlie.io/static/swagger/#/Extension-Schema/getExtensionSchema) or the LimaCharlie CLI: `limacharlie extension get_schema --help`.

### Requests

Requests are, as the name implies, direct individual requests to an Extension. A request contains an "action" and a "payload" (JSON object) to be sent to the Extension. Some requests can be flagged to have the Extension impersonate the requester (identity and permissions) during execution.

The "action" and "payload" entirely depends on the Extension it is destined to. The list of actions and individual payload structures available for an Extension is documented by each Extension using the "schema" they publish.

Schemas are available through the [Schema API](https://api.limacharlie.io/static/swagger/#/Extension-Schema/getExtensionSchema) or the LimaCharlie CLI: `limacharlie extension get_schema --help`.

## Interacting

### Interactively

The LimaCharlie webapp automatically displays a machine-generated user interface for each Extension based on the schema it publishes.

### Automation

[Detection & Response Rules](/docs/detection-and-response), the main automation mechanism in LimaCharlie can interact with Extensions using the `extension request` action in the Response component.

### API

Extensions can be interacted with using a few different APIs:

* Getting the schema for an Extension: [https://api.limacharlie.io/static/swagger/#/Extension-Schema](https://api.limacharlie.io/static/swagger/#/Extension-Request)
* Making requests to an Extension: <https://api.limacharlie.io/static/swagger/#/Extension-Request>

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

---

# VirusTotal Integration

You can easily integrate LimaCharlie with VirusTotal to enhance your data enrichment and detections. You will need a VirusTotal API key in order to utilize this add-on.

## VirusTotal Data Caching

The free tier of VirusTotal allows four lookups per minute via the API. LimaCharlie employs a global cache of VirusTotal requests which should significantly reduce costs if you are using VirusTotal at scale. VirusTotal requests are cached for 3 days.

Once you have your VirusTotal API key, you can add it in the Organization integrations section of the LimaCharlie web app.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/vt-key.png)

Once you have entered your API key, you can then create a rule to perform a lookup of a hash. For example, the following rule will let you know if there is a hit from VirusTotal on a hash with at least two different engines.

```
path: event/HASH
op: lookup
resource: hives://lookup/vt
event: CODE_IDENTITY
metadata_rules:
  path: /
  value: 2
  length of: true
  op: is greater than
```

---

# VirusTotal Integration

You can easily integrate LimaCharlie with VirusTotal to enhance your data enrichment and detections. You will need a VirusTotal API key in order to utilize this add-on.

## VirusTotal Data Caching

The free tier of VirusTotal allows four lookups per minute via the API. LimaCharlie employs a global cache of VirusTotal requests which should significantly reduce costs if you are using VirusTotal at scale. VirusTotal requests are cached for 3 days.

Once you have your VirusTotal API key, you can add it in the Organization integrations section of the LimaCharlie web app.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/vt-key.png)

Once you have entered your API key, you can then create a rule to perform a lookup of a hash. For example, the following rule will let you know if there is a hit from VirusTotal on a hash with at least two different engines.

```
path: event/HASH
op: lookup
resource: hives://lookup/vt
event: CODE_IDENTITY
metadata_rules:
  path: /
  value: 2
  length of: true
  op: is greater than
```

---

# Detection & Response

# Community Rules

The LimaCharlie Endpoint Agent is a cross platform endpoint Sensor. It is a low-level, light-weight sensor which executes detection and response functionality in real-time.

Our Community Rules feature leverages the power of AI to quickly transform a plethora of third-party rules into LimaCharlie syntax so you can make them your own. The process is fast and efficient: Browse thousands of community rules, select one as a starting point, convert it to LimaCharlie syntax with one click, and customize it to your liking.

## Accessing the Community Rules

To access the Community Rules:

1. Log into LimaCharlie
2. Select an Organization
3. Click the Automation drop down on the left panel
4. Select Rules
5. Look in the upper right corner of the D&R Rules page for the Add Rule button
6. Click the Add Rule button
7. Look in the upper right corner of the rule creation page for the Community Library button
8. Click the Community Library button

This takes you to the Community Rules search page, and gives you access to thousands of third-party detection rules. The library currently contains detection rules written by [Anvilogic](https://github.com/anvilogic-forge/armory/blob/main/detections/cloud/aws_disableawsserviceaccess/aws_disableawsserviceaccess-splunk-awscloudtrail.yml), [Sigma](https://github.com/SigmaHQ/sigma/blob/master/rules/network/zeek/zeek_http_susp_file_ext_from_susp_tld.yml), [Panther](https://github.com/panther-labs/panther-analysis/blob/develop/rules/gsuite_activityevent_rules/google_workspace_many_docs_downloaded.yml), and [Okta](https://github.com/okta/customer-detections).

> Rules are searchable by CVE number, keyword, or pre-defined descriptors (Tags). Searchable tags include attack techniques, MITRE ATT&CK id codes and other key rule identificators.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(337).png)

## Loading a Community Rule

Once you find the rule you want to use, import it to the organization by clicking "Load Rule", and our AI engine will create it using verified LimaCharlie syntax.

> This process may take a few seconds so please be patient.

Once the rule is ready, it will return you to the Add Rule page in LimaCharlie. The Detect and Response sections of the rule will be filled out with LimaCharlie logic that includes explanatory comments. From here you can manage this rule just as you would any other D&R rule.

## Digging Deeper

As these rules are the property of third parties you may be interested in knowing more about their licensing or source code. This information is accessible through the Community Rules search page. To see these details click on a rule.

The example below shows what appears when you click Anvilogic's Potential CVE-2021-44228 - Log4Shell rule

Under the rule name you will see the options to load the rule, check its source code, and read additional licensing information. There is also a reference section at the bottom left corner of the window providing links related to the rule.

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf3SZQZu4j4kEp2Y0wpnoeHA0t_XaR5VqaoB9SupPHl0t91e-12QhMj0epDi742peW0gpu8e44HhJ4lDN1esspiMRUfpFr3W2aNiQcIeff2HhNCxmgp1h3oLqphpqJ8AohoDDxFdA?key=7BgiNipN3DxRQXGQyEk06w)

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Tags in LimaCharlie are strings linked to sensors for classifying endpoints, automating detection and response, and triggering workflows. Tags appear in every event under the `routing` component and help simplify rule writing. Tags can be added manually, via API, or through detection & response rules. System tags like `lc:latest`, `lc:stable`, and `lc:debug` offer special functionality. Tags can be checked, added, or removed through the API or web app, streamlining device management.

---

# Config Hive: Detection & Response Rules

## Format

## Permissions

There are three "sub-categories" within detection and response rules contained in Hive.

* `dr-general` pertains to rules that your Organization has created and/or controls.
* `dr-managed` pertains to rules that you can use for detection, however are managed or curated by another party (i.e. Soteria rules).
* `dr-service` is a protected namespace, and users will only ever have metadata permissions.

### dr-general

* `dr.list`
* `dr.set`
* `dr.del`

### dr-managed

* `dr.list.managed`
* `dr.set.managed`
* `dr.del.managed`

### dr-service

* `dr.list` or `dr.list.managed` (metadata only)
* `dr.set` or `dr.set.managed` (metadata only)

## Command-Line Usage

```
usage: limacharlie hive [-h] [-k KEY] [-d DATA] [-pk PARTITIONKEY] [--etag ETAG] [--expiry EXPIRY] [--enabled ENABLED] [--tags TAGS] action hive_name

positional arguments:
  action                the action to take, one of: list, list_mtd, get, get_mtd, set, update, remove
  hive_name             the hive name

options:
  -h, --help            show this help message and exit
  -k KEY, --key KEY     the name of the key.
  -d DATA, --data DATA  file containing the JSON data for the record, or "-" for stdin.
  -pk PARTITIONKEY, --partition-key PARTITIONKEY
                        the partition key to use instead of the default OID.
  --etag ETAG           the optional previous etag expected for transactions.
  --expiry EXPIRY       a millisecond epoch timestamp when the record should expire.
  --enabled ENABLED     whether the record is enabled or disabled.
  --tags TAGS           comma separated list of tags.
```

## Usage

## Example

```
{
  "detect": {
    "event": "WEL",
    "op": "and",
    "rules": [
      {
        "op": "is",
        "path": "event/EVENT/System/Channel",
        "value": "Microsoft-Windows-Windows Defender/Operational"
      },
      {
        "op": "is",
        "path": "event/EVENT/System/EventID",
        "value": "1006"
      }
    ]
  },
  "respond": [
    {
      "action": "report",
      "name": "windows-defender-malware-detected"
    }
  ]
}
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Config Hive: Detection & Response Rules

## Format

## Permissions

There are three "sub-categories" within detection and response rules contained in Hive.

* `dr-general` pertains to rules that your Organization has created and/or controls.
* `dr-managed` pertains to rules that you can use for detection, however are managed or curated by another party (i.e. Soteria rules).
* `dr-service` is a protected namespace, and users will only ever have metadata permissions.

### dr-general

* `dr.list`
* `dr.set`
* `dr.del`

### dr-managed

* `dr.list.managed`
* `dr.set.managed`
* `dr.del.managed`

### dr-service

* `dr.list` or `dr.list.managed` (metadata only)
* `dr.set` or `dr.set.managed` (metadata only)

## Command-Line Usage

```
usage: limacharlie hive [-h] [-k KEY] [-d DATA] [-pk PARTITIONKEY] [--etag ETAG] [--expiry EXPIRY] [--enabled ENABLED] [--tags TAGS] action hive_name

positional arguments:
  action                the action to take, one of: list, list_mtd, get, get_mtd, set, update, remove
  hive_name             the hive name

options:
  -h, --help            show this help message and exit
  -k KEY, --key KEY     the name of the key.
  -d DATA, --data DATA  file containing the JSON data for the record, or "-" for stdin.
  -pk PARTITIONKEY, --partition-key PARTITIONKEY
                        the partition key to use instead of the default OID.
  --etag ETAG           the optional previous etag expected for transactions.
  --expiry EXPIRY       a millisecond epoch timestamp when the record should expire.
  --enabled ENABLED     whether the record is enabled or disabled.
  --tags TAGS           comma separated list of tags.
```

## Usage

## Example

```
{
  "detect": {
    "event": "WEL",
    "op": "and",
    "rules": [
      {
        "op": "is",
        "path": "event/EVENT/System/Channel",
        "value": "Microsoft-Windows-Windows Defender/Operational"
      },
      {
        "op": "is",
        "path": "event/EVENT/System/EventID",
        "value": "1006"
      }
    ]
  },
  "respond": [
    {
      "action": "report",
      "name": "windows-defender-malware-detected"
    }
  ]
}
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Create a D&R Rule Using a Threat Feed

A common use case for rules is to use them to compare telemetry against known malicious IPs, domain names, or file hashes via threat feeds. With LimaCharlie, it is easy to leverage public threat feeds or create your own.

To configure a threat feed, it must first be enabled within the Add-ons Marketplace. First, select a threat feed from the plethora available for free. In the following example, we will enable `crimeware-ips`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/crimeware-ips(1).png)

Select `Subscribe`, which will make the feed available to the respective Organization.

Once subscribed, you can write a D&R rule to detect whenever there is a match to an IP within the threat feed. Navigate to `D&R Rules` within the web application main page, and select `+ New Rule`. Begin your rule with the following template:

```
event: NETWORK_CONNECTIONS
op: lookup
path: event/NETWORK_ACTIVITY/?/IP_ADDRESS
resource: hive://lookup/crimeware-ips
```

## Additional Telemetry Points

Configure a lookup based on file hash:

```
op: lookup
event: CODE_IDENTITY
path: event/HASH
resource: hive://lookup/my-hash-lookup
```

Configure a lookup based on domain name(s):

```
op: lookup
event: DNS_REQUEST
path: event/DOMAIN_NAME
resource: hive://lookup/my-dns-lookup
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Create a D&R Rule Using a Threat Feed

A common use case for rules is to use them to compare telemetry against known malicious IPs, domain names, or file hashes via threat feeds. With LimaCharlie, it is easy to leverage public threat feeds or create your own.

To configure a threat feed, it must first be enabled within the Add-ons Marketplace. First, select a threat feed from the plethora available for free. In the following example, we will enable `crimeware-ips`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/crimeware-ips(1).png)

Select `Subscribe`, which will make the feed available to the respective Organization.

Once subscribed, you can write a D&R rule to detect whenever there is a match to an IP within the threat feed. Navigate to `D&R Rules` within the web application main page, and select `+ New Rule`. Begin your rule with the following template:

```
event: NETWORK_CONNECTIONS
op: lookup
path: event/NETWORK_ACTIVITY/?/IP_ADDRESS
resource: hive://lookup/crimeware-ips
```

## Additional Telemetry Points

Configure a lookup based on file hash:

```
op: lookup
event: CODE_IDENTITY
path: event/HASH
resource: hive://lookup/my-hash-lookup
```

Configure a lookup based on domain name(s):

```
op: lookup
event: DNS_REQUEST
path: event/DOMAIN_NAME
resource: hive://lookup/my-dns-lookup
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Detection Logic Operators

Operators are used in the Detection part of a Detection & Response rule. Operators may also be accompanied by other available parameters, such as transforms, times, and others, referenced later in this page.

> For more information on how to use operators, read [Detection & Response Rules](/v2/docs/detection-and-response).

## Operators

### and, or

The standard logical boolean operations to combine other logical operations. Takes a single `rules:` parameter that contains a list of other operators to "AND" or "OR" together.

Example:

```
op: or
rules:
  - ...rule1...
  - ...rule2...
  - ...
```

### is

Tests for equality between the value of the `"value": <>` parameter and the value found in the event at the `"path": <>` parameter.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example rule:

```
event: NEW_PROCESS
op: is
path: event/PARENT/PROCESS_ID
value: 9999
```

### exists

Tests if any elements exist at the given path (regardless of its value).

Example rule:

```
event: NEW_PROCESS
op: exists
path: event/PARENT
```

The `exists` operator also supports an optional `truthy` parameter. When `true`, this parameter indicates the `exists` should treat `null` and `""` (empty string) values as if they were non-existent like:

The rule:

```
op: exists
path: some/path
truthy: true
```

applied to:

```
{
  "some": {
    "path": ""
  }
}
```

would NOT match.

### contains

The `contains` checks if a substring can be found in the value at the path.

An optional parameter `count: 3` can be specified to only match if the given
 substring is found *at least* 3 times in path.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example rule:

```
event: NEW_PROCESS
op: contains
path: event/COMMAND_LINE
value: reg
count: 2
```

### ends with, starts with

The `starts with` checks for a prefix match and `ends with` checks for a suffix match.

They both check if the value found at `path` matches the given `value`, based on the operator.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

### is greater than, is lower than

Check to see if a value is greater or lower (numerically) than a value in the event.

They both use the `path` and `value` parameters. They also both support the `length of` parameter as a boolean (true or false). If set to true, instead of comparing
 the value at the specified path, it compares the length of the value at that path.

### matches

The `matches` op compares the value at `path` with a regular expression supplied in the `re` parameter. Under the hood, this uses the Golang's `regexp` [package](https://golang.org/pkg/regexp/), which also enables you to apply the regexp to log files.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example:

```
event: FILE_TYPE_ACCESSED
op: matches
path: event/FILE_PATH
re: .*\\system32\\.*\.scr
case sensitive: false
```

### not

The `not` operator inverts the result of its rule. For example, when applied to an `is` operator, it changes the logic from "equals" to "does not equal". When applied to an or operator, it changes the logic from "any of these conditions are true" to "none of these conditions are true"

Example:

```
event: NEW_PROCESS
op: is
not: true
path: event/PARENT/PROCESS_ID
value: 9999
```

### string distance

The `string distance` op looks up the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings. In other words it generates the minimum number of character changes required for one string to become equal to another.

For example, the Levenshtein Distance between `google.com` and `googlr.com` (`r` instead of `e`) is 1.

This can be used to find variations of file names or domain names that could be used for phishing, for example.

Suppose your company is `onephoton.com`. Looking for the Levenshtein Distance between all `DOMAIN_NAME` in `DNS_REQUEST` events, compared to `onephoton.com` it could detect an attacker using `onephot0n.com` in a phishing email domain.

The operator takes a `path` parameter indicating which field to compare, a `max` parameter indicating the maximum Levenshtein Distance to match and a `value` parameter that is either a string or a list of strings that represent the value(s) to compare to. Note that although `string distance` supports the `value` to be a list, most other operators do not.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example:

```
event: DNS_REQUEST
op: string distance
path: event/DOMAIN_NAME
value:
  - onephoton.com
  - www.onephoton.com
max: 2
```

This would match `onephotom.com` and `0nephotom.com` but NOT `0neph0tom.com`.

Using the [file name](#file-name) transform to apply to a file name in a path:

```
event: NEW_PROCESS
op: string distance
path: event/FILE_PATH
file name: true
value:
  - svchost.exe
  - csrss.exe
max: 2
```

This would match `svhost.exe` and `csrss32.exe` but NOT `csrsswin32.exe`.

### is 32 bit, is 64 bit, is arm

All of these operators take no additional arguments, they simply match if the relevant Sensor characteristic is correct.

Example:

```
op: is 64 bit
```

### is platform

Checks if the event under evaluation is from a sensor of the given platform.

Takes a `name` parameter for the platform name. The current platforms are:

* `windows`
* `linux`
* `macos`
* `ios`
* `android`
* `chrome`
* `vpn`
* `text`
* `json`
* GCP
* AWS
* `carbon_black`
* `crowdstrike`
* `1password`
* `office365`
* `msdefender`

Example:

```
op: is platform
name: 1password
```

### is tagged

Determines if the Tag supplied in the `tag` parameter is already associated with the sensor that the event under evaluation is from.

### lookup

Looks up a value against a [lookup add-on](https://app.limacharlie.io/add-ons/category/lookup) (a.k.a. resource) such as a threat feed.

```
event: DNS_REQUEST
op: lookup
path: event/DOMAIN_NAME
resource: hive://lookups/malwaredomains
case sensitive: false
```

This rule will get the `event/DOMAIN_NAME` of a `DNS_REQUEST` event and check if it's a member of the `lookup` named `malwaredomains`. If it is, then the rule is a match.

The value is supplied via the `path` parameter and the lookup is defined in the `resource` parameter. Resources are of the form `hive://lookups/RESOURCE_NAME`. In order to access a lookup, your Organization must be subscribed to it.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

> API-based lookups, like VirusTotal and IP Geolocation, work a little bit differently. For more information, see [Using API-based lookups](/v2/docs/add-ons-api-integrations).

> You can create your own lookups and optionally publish them in the add-on marketplace. To learn more, see [Lookups](/v2/docs/config-hive-lookups) and [Lookup Manager](/v2/docs/ext-lookup-manager).

### scope

In some cases, you may want to limit the scope of the matching and the `path` you use to be within a specific part of the event. The `scope` operator allows you to do just that, reset the root of the `event/` in paths to be a sub-path of the event.

This comes in as very useful for example when you want to test multiple values of a connection in a `NETWORK_CONNECTIONS` event but always on a per-connection. If you  were to do a rule like:

```
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: starts with
    path: event/NETWORK_ACTIVITY/?/SOURCE/IP_ADDRESS
    value: '10.'
  - op: is
    path: event/NETWORK_ACTIVITY/?/DESTINATION/PORT
    value: 445
```

you would hit on events where *any* connection has a source IP prefix of `10.` and *any* connection has a destination port of `445`. Obviously this is not what we had in mind, we wanted to know if a *single* connection has those two characteristics.

The solution is to use the `scope` operator. The `path` in the operator will become the new `event/` root path in all operators found under the `rule`. So the above would become

Example:

```
event: NETWORK_CONNECTIONS
op: scope
path: event/NETWORK_ACTIVITY/
rule:
  op: and
  rules:
    - op: starts with
      path: event/SOURCE/IP_ADDRESS
      value: '10.'
    - op: is
      path: event/DESTINATION/PORT
      value: 445
```

### cidr

The `cidr` checks if an IP address at the path is contained within a given
[CIDR network mask](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).

Example rule:

```
event: NETWORK_CONNECTIONS
op: cidr
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
cidr: 10.16.1.0/24
```

### is private address

The `is private address` checks if an IP address at the path is a private address
 as defined by [RFC 1918](https://en.wikipedia.org/wiki/Private_network).

Example rule:

```
event: NETWORK_CONNECTIONS
op: is private address
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
```

### is public address

The `is public address` checks if an IP address at the path is a public address
 as defined by [RFC 1918](https://en.wikipedia.org/wiki/Private_network).

Example rule:

```
event: NETWORK_CONNECTIONS
op: is public address
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
```

## Transforms

Transforms are transformations applied to the value being evaluated in an event, prior to the evaluation.

### file name

Sample: `file name: true`

The `file name` transform takes a `path` and replaces it with the file name component of the `path`. This means that a `path` of `c:\windows\system32\wininet.dll` will become `wininet.dll`.

### sub domain

Sample: `sub domain: "-2:"`

The `sub domain` extracts specific components from a domain name. The value of `sub domain` is in [slice notation](https://stackoverflow.com/questions/509211/understanding-slice-notation). It looks like `startIndex:endIndex`, where the index is 0-based and indicates which parts of the domain to keep.

Some examples:

* `0:2` means the first 2 components of the domain: `aa.bb` for `aa.bb.cc.dd`.
* `-1` means the last component of the domain: `cc` for `aa.bb.cc`.
* `1:` means all components starting at 1: `bb.cc` for `aa.bb.cc`.
* `:` means to test the operator to every component individually.

### is older than

Test if a value in event at the `"path": <>` parameter, assumed to be either a second-based epoch or a millisecond-based epoch is older than a number of seconds as specified by the `seconds` parameter, centered in time at "now" during evaluation.

Example rule:

```
event: login-attempt
op: is older than
path: routing/event_time
seconds: 3600
```

where the example above would match on a `login-attempt` event that occurred more than 1h ago.

## Times

All operators support an optional parameter named `times`. When specified, it must contain a list of Time Descriptors when the accompanying operator is valid. Your rule can mix-and-match multiple Time Descriptors as part of a single rule on per-operator basis.

Here's an example rule that matches a Chrome process starting between 11PM and 5AM, Monday through Friday, Pacific Time:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: chrome.exe
case sensitive: false
times:
  - day_of_week_start: 2     # 1 - 7
    day_of_week_end: 6       # 1 - 7
    time_of_day_start: 2200  # 0 - 2359
    time_of_day_end: 2359    # 0 - 2359
    tz: America/Los_Angeles  # time zone
  - day_of_week_start: 2
    day_of_week_end: 6
    time_of_day_start: 0
    time_of_day_end: 500
    tz: America/Los_Angeles
```

#### Time Zone

The `tz` should match a TZ database name from the [Time Zones Database](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).

---

# Detection Logic Operators

Operators are used in the Detection part of a Detection & Response rule. Operators may also be accompanied by other available parameters, such as transforms, times, and others, referenced later in this page.

> For more information on how to use operators, read [Detection & Response Rules](/v2/docs/detection-and-response).

## Operators

### and, or

The standard logical boolean operations to combine other logical operations. Takes a single `rules:` parameter that contains a list of other operators to "AND" or "OR" together.

Example:

```
op: or
rules:
  - ...rule1...
  - ...rule2...
  - ...
```

### is

Tests for equality between the value of the `"value": <>` parameter and the value found in the event at the `"path": <>` parameter.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example rule:

```
event: NEW_PROCESS
op: is
path: event/PARENT/PROCESS_ID
value: 9999
```

### exists

Tests if any elements exist at the given path (regardless of its value).

Example rule:

```
event: NEW_PROCESS
op: exists
path: event/PARENT
```

The `exists` operator also supports an optional `truthy` parameter. When `true`, this parameter indicates the `exists` should treat `null` and `""` (empty string) values as if they were non-existent like:

The rule:

```
op: exists
path: some/path
truthy: true
```

applied to:

```
{
  "some": {
    "path": ""
  }
}
```

would NOT match.

### contains

The `contains` checks if a substring can be found in the value at the path.

An optional parameter `count: 3` can be specified to only match if the given
 substring is found *at least* 3 times in path.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example rule:

```
event: NEW_PROCESS
op: contains
path: event/COMMAND_LINE
value: reg
count: 2
```

### ends with, starts with

The `starts with` checks for a prefix match and `ends with` checks for a suffix match.

They both check if the value found at `path` matches the given `value`, based on the operator.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

### is greater than, is lower than

Check to see if a value is greater or lower (numerically) than a value in the event.

They both use the `path` and `value` parameters. They also both support the `length of` parameter as a boolean (true or false). If set to true, instead of comparing
 the value at the specified path, it compares the length of the value at that path.

### matches

The `matches` op compares the value at `path` with a regular expression supplied in the `re` parameter. Under the hood, this uses the Golang's `regexp` [package](https://golang.org/pkg/regexp/), which also enables you to apply the regexp to log files.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example:

```
event: FILE_TYPE_ACCESSED
op: matches
path: event/FILE_PATH
re: .*\\system32\\.*\.scr
case sensitive: false
```

### not

The `not` operator inverts the result of its rule. For example, when applied to an `is` operator, it changes the logic from "equals" to "does not equal". When applied to an or operator, it changes the logic from "any of these conditions are true" to "none of these conditions are true"

Example:

```
event: NEW_PROCESS
op: is
not: true
path: event/PARENT/PROCESS_ID
value: 9999
```

### string distance

The `string distance` op looks up the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings. In other words it generates the minimum number of character changes required for one string to become equal to another.

For example, the Levenshtein Distance between `google.com` and `googlr.com` (`r` instead of `e`) is 1.

This can be used to find variations of file names or domain names that could be used for phishing, for example.

Suppose your company is `onephoton.com`. Looking for the Levenshtein Distance between all `DOMAIN_NAME` in `DNS_REQUEST` events, compared to `onephoton.com` it could detect an attacker using `onephot0n.com` in a phishing email domain.

The operator takes a `path` parameter indicating which field to compare, a `max` parameter indicating the maximum Levenshtein Distance to match and a `value` parameter that is either a string or a list of strings that represent the value(s) to compare to. Note that although `string distance` supports the `value` to be a list, most other operators do not.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

Example:

```
event: DNS_REQUEST
op: string distance
path: event/DOMAIN_NAME
value:
  - onephoton.com
  - www.onephoton.com
max: 2
```

This would match `onephotom.com` and `0nephotom.com` but NOT `0neph0tom.com`.

Using the [file name](#file-name) transform to apply to a file name in a path:

```
event: NEW_PROCESS
op: string distance
path: event/FILE_PATH
file name: true
value:
  - svchost.exe
  - csrss.exe
max: 2
```

This would match `svhost.exe` and `csrss32.exe` but NOT `csrsswin32.exe`.

### is 32 bit, is 64 bit, is arm

All of these operators take no additional arguments, they simply match if the relevant Sensor characteristic is correct.

Example:

```
op: is 64 bit
```

### is platform

Checks if the event under evaluation is from a sensor of the given platform.

Takes a `name` parameter for the platform name. The current platforms are:

* `windows`
* `linux`
* `macos`
* `ios`
* `android`
* `chrome`
* `vpn`
* `text`
* `json`
* GCP
* AWS
* `carbon_black`
* `crowdstrike`
* `1password`
* `office365`
* `msdefender`

Example:

```
op: is platform
name: 1password
```

### is tagged

Determines if the Tag supplied in the `tag` parameter is already associated with the sensor that the event under evaluation is from.

### lookup

Looks up a value against a [lookup add-on](https://app.limacharlie.io/add-ons/category/lookup) (a.k.a. resource) such as a threat feed.

```
event: DNS_REQUEST
op: lookup
path: event/DOMAIN_NAME
resource: hive://lookups/malwaredomains
case sensitive: false
```

This rule will get the `event/DOMAIN_NAME` of a `DNS_REQUEST` event and check if it's a member of the `lookup` named `malwaredomains`. If it is, then the rule is a match.

The value is supplied via the `path` parameter and the lookup is defined in the `resource` parameter. Resources are of the form `hive://lookups/RESOURCE_NAME`. In order to access a lookup, your Organization must be subscribed to it.

Supports the [file name](#file-name) and [sub domain](#sub-domain) transforms.

> API-based lookups, like VirusTotal and IP Geolocation, work a little bit differently. For more information, see [Using API-based lookups](/v2/docs/add-ons-api-integrations).

> You can create your own lookups and optionally publish them in the add-on marketplace. To learn more, see [Lookups](/v2/docs/config-hive-lookups) and [Lookup Manager](/v2/docs/ext-lookup-manager).

### scope

In some cases, you may want to limit the scope of the matching and the `path` you use to be within a specific part of the event. The `scope` operator allows you to do just that, reset the root of the `event/` in paths to be a sub-path of the event.

This comes in as very useful for example when you want to test multiple values of a connection in a `NETWORK_CONNECTIONS` event but always on a per-connection. If you  were to do a rule like:

```
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: starts with
    path: event/NETWORK_ACTIVITY/?/SOURCE/IP_ADDRESS
    value: '10.'
  - op: is
    path: event/NETWORK_ACTIVITY/?/DESTINATION/PORT
    value: 445
```

you would hit on events where *any* connection has a source IP prefix of `10.` and *any* connection has a destination port of `445`. Obviously this is not what we had in mind, we wanted to know if a *single* connection has those two characteristics.

The solution is to use the `scope` operator. The `path` in the operator will become the new `event/` root path in all operators found under the `rule`. So the above would become

Example:

```
event: NETWORK_CONNECTIONS
op: scope
path: event/NETWORK_ACTIVITY/
rule:
  op: and
  rules:
    - op: starts with
      path: event/SOURCE/IP_ADDRESS
      value: '10.'
    - op: is
      path: event/DESTINATION/PORT
      value: 445
```

### cidr

The `cidr` checks if an IP address at the path is contained within a given
[CIDR network mask](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).

Example rule:

```
event: NETWORK_CONNECTIONS
op: cidr
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
cidr: 10.16.1.0/24
```

### is private address

The `is private address` checks if an IP address at the path is a private address
 as defined by [RFC 1918](https://en.wikipedia.org/wiki/Private_network).

Example rule:

```
event: NETWORK_CONNECTIONS
op: is private address
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
```

### is public address

The `is public address` checks if an IP address at the path is a public address
 as defined by [RFC 1918](https://en.wikipedia.org/wiki/Private_network).

Example rule:

```
event: NETWORK_CONNECTIONS
op: is public address
path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS
```

## Transforms

Transforms are transformations applied to the value being evaluated in an event, prior to the evaluation.

### file name

Sample: `file name: true`

The `file name` transform takes a `path` and replaces it with the file name component of the `path`. This means that a `path` of `c:\windows\system32\wininet.dll` will become `wininet.dll`.

### sub domain

Sample: `sub domain: "-2:"`

The `sub domain` extracts specific components from a domain name. The value of `sub domain` is in [slice notation](https://stackoverflow.com/questions/509211/understanding-slice-notation). It looks like `startIndex:endIndex`, where the index is 0-based and indicates which parts of the domain to keep.

Some examples:

* `0:2` means the first 2 components of the domain: `aa.bb` for `aa.bb.cc.dd`.
* `-1` means the last component of the domain: `cc` for `aa.bb.cc`.
* `1:` means all components starting at 1: `bb.cc` for `aa.bb.cc`.
* `:` means to test the operator to every component individually.

### is older than

Test if a value in event at the `"path": <>` parameter, assumed to be either a second-based epoch or a millisecond-based epoch is older than a number of seconds as specified by the `seconds` parameter, centered in time at "now" during evaluation.

Example rule:

```
event: login-attempt
op: is older than
path: routing/event_time
seconds: 3600
```

where the example above would match on a `login-attempt` event that occurred more than 1h ago.

## Times

All operators support an optional parameter named `times`. When specified, it must contain a list of Time Descriptors when the accompanying operator is valid. Your rule can mix-and-match multiple Time Descriptors as part of a single rule on per-operator basis.

Here's an example rule that matches a Chrome process starting between 11PM and 5AM, Monday through Friday, Pacific Time:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: chrome.exe
case sensitive: false
times:
  - day_of_week_start: 2     # 1 - 7
    day_of_week_end: 6       # 1 - 7
    time_of_day_start: 2200  # 0 - 2359
    time_of_day_end: 2359    # 0 - 2359
    tz: America/Los_Angeles  # time zone
  - day_of_week_start: 2
    day_of_week_end: 6
    time_of_day_start: 0
    time_of_day_end: 500
    tz: America/Los_Angeles
```

#### Time Zone

The `tz` should match a TZ database name from the [Time Zones Database](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).

---

# Detection and Response

Detection & Response rules automate actions based on the real-time events streaming into LimaCharlie. Each rule has two YAML descriptors: one that describes what to detect, and another that describes how to respond.

Note: It's recommended to read about [Events](/v2/docs/events) before diving into rules.

## A Basic Rule

Here's a rule that detects DNS requests to `example.com` and responds by reporting them within the Organization with a category name `DNS Hit example.com`.

```
# Detection
event: DNS_REQUEST
op: is
path: event/DOMAIN_NAME
value: example.com

# Response
- action: report
  name: DNS Hit example.com
```

This rule will detect and respond to requests to `example.com` within 100ms of the `DNS_REQUEST` event occurring. It uses the `is` operator to assess if the given `value` can be found inside the `event` at the given `path`.

Want more detection examples?

For examples, check out the [Detection and Response Examples](/v2/docs/detection-and-response-examples).

## Detection

### Targets and events

Detections must specify an `event` (or `events`), and may optionally specify a `target`. Each target offers different event types. Here are the 5 possible rule targets:

* `edr` (default): telemetry events from LimaCharlie sensors
* `detection`: detections generated by other rules
* `deployment`: lifecycle events around deployment & enrollment of sensors
* `artifact`: artifacts collected via REST API or via `artifact_get` Sensor command
* `artifact_event`: lifecycle events around artifacts such as ingestion

For a full list of events with examples, see [Events Reference](/v2/docs/events).

Most of this page focuses on `edr` events. For information about other targets, see [Detection on Alternate Targets](/v2/docs/detection-on-alternate-targets).

#### Detections against Adapter events

Similar to EDR telemetry, data received via Adapters are observable via Detection & Response rules. D&R rules that action on Adapter-based data are written the same way, with event and operator qualifiers and response actions based on successful detections.

Depending on the type of adapter, you can reference adapter data directly via the `platform` [sensor selector](/v2/docs/reference-sensor-selector-expressions) (e.g. `aws`, `msdefender`, `crowdstrike`, etc.)

### Operators

Detections must specify an `op` (logical operator). The types of operators used are a good indicator for how complex the rule will be.

Here's a simple detection that uses a single `is windows` operator to detect a Windows sensor connecting to the Internet:

```
event: CONNECTED
op: is windows
```

And here's a more complex detection that uses the `and` operator to detect a non-Windows sensor that's making a DNS request to example.com.

```
event: DNS_REQUEST
op: and
rules:
- op: is windows
  not: true
- op: is
  path: event/DOMAIN_NAME
  value: example.com
```

There are 3 operators here:

1. The `and` operator evaluates nested `rules` and will only itself be `true` if both of the rules inside it are `true`
2. The `is windows` operator is accompanied by the `not` parameter, reversing the matching outcome and effectively saying "anything but windows"
3. The `is` operator is comparing the `value` 'example.com' to the content of the event at the given `path`

Each operator may have parameters alongside it. Some parameters, such as `not`, are useable on all operators. Most operators have required parameters specific to them.

> For a full list of operators and their usage, see [Reference: Operators](/v2/docs/detection-logic-operators).

### Paths

The `path` parameter is used commonly in several operators to specify which part of the event should be evaluated.

Here's an example of a standard JSON `DNS_REQUEST` event from a sensor:

```
{
  "event": {
    "DNS_TYPE": 1,
    "TIMESTAMP": 1456285240,
    "DNS_FLAGS": 0,
    "DOMAIN_NAME": "example.com"
  },
  "routing": {
    "event_type": "DNS_REQUEST",
    "oid": "8cbe27f4-agh1-4afb-ba19-138cd51389cd",
    "sid": "d3d17f12-eecf-5287-b3a1-bf267aabb3cf",
    "hostname": "test-host-123"
    // ...and other standardized routing data
  }
}
```

This detection will match the above event's hostname:

```
event: DNS_REQUEST
op: is
path: routing/hostname # where the value lives
value: test-host-123   # the expected value at that path
```

This works a lot like file paths in a directory system. Since LimaCharlie events are always formatted with separate `event` and `routing` data, almost all paths start with either `event/` or `routing/`.

> Tip: you can visit the Timeline view of any Sensor to browse historical events and bring them directly into the D&R rule editor.

Paths may also employ the use of wildcards `*` to represent 0 or more directory levels, or `?` to represent exactly 1 directory level. This can be useful when working with events like `NETWORK_CONNECTIONS`:

```
{
  "event": {
    "NETWORK_ACTIVITY": [
      {
        "SOURCE": {
          "IP_ADDRESS": "172.16.223.138",
          "PORT": 50396
        },
        "IS_OUTGOING": 1,
        "DESTINATION": {
          "IP_ADDRESS": "23.214.49.56",
          "PORT": 80
        }
      },
      {
        "SOURCE": {
          "IP_ADDRESS": "172.16.223.138",
          "PORT": 50397
        },
        "IS_OUTGOING": 1,
        "DESTINATION": {
          "IP_ADDRESS": "189.247.166.18",
          "PORT": 80
        }
      },
      // ...there could be several connections
    ],
    "HASH": "2de228cad2e542b2af2554d61fab5463ecbba3ff8349ba88c3e48637ed8086e9",
    "COMMAND_LINE": "C:\\WINDOWS\\system32\\msfeedssync.exe sync",
    "PROCESS_ID": 6968,
    "FILE_IS_SIGNED": 1,
    "USER_NAME": "WIN-5KC7E0NG1OD\\dev",
    "FILE_PATH": "C:\\WINDOWS\\system32\\msfeedssync.exe",
    "PARENT_PROCESS_ID": 1892
  },
  "routing": { ... } // Omitted for brevity
}
```

Notice that the `NETWORK_ACTIVITY` inside this event is a list.

Here's a rule that would match a known destination IP in any of the entries within `NETWORK_ACTIVITY`:

```
event: NETWORK_CONNECTIONS
op: is
path: event/NETWORK_ACTIVITY/?/DESTINATION/IP_ADDRESS # <---
value: 189.247.166.18
```

The `?` saves us from enumerating each index within the list and instead evaluates *all* values at the indicated level. This can be very powerful when used in combination with lookups: lists of threat indicators such as known bad IPs or domains.

> To learn more about using lookups in detections, see the `lookup` [operator](/v2/docs/detection-logic-operators#lookup).

### Values

The `value` parameter is commonly used by several detection operations but can also be used by some response actions as well.

In most detections `value` will be used to specify a known value like all the previous examples on this page have done. They're also capable of referencing previously set sensor variables using `value: [[var-name]]` double square bracket syntax.

Values from events can also be forwarded in response actions using `value: <<event/FILE_PATH>>` double angle bracket syntax.

> To see how sensor variables and lookback values are used, see the `add var / del var` action in [Reference: Response Actions](/v2/docs/response-actions).

## Response

Responses are much simpler than Detections. They're a list of actions to perform upon a matching detection.

### Actions

The most common action is the `report` action, which creates a Detection that shows up in the LimaCharlie web app and passes it along to the `detections` output stream in real-time.

```
- action: report
  name: detected-something

# Example of accessing map values
- action: report
  name: Event detected by {{ .event.USER_NAME }} from {{ index (index .event.NETWORK_ACTIVITY 0) "SOURCE" "IP_ADDRESS" }}
```

Each item in the response specifies an `action` and any accompanying parameters for that `action`.

A more complex response action could include running an [endpoint agent command](/v2/docs/endpoint-agent-commands) such as `yara_scan` using a field from within the detected event. The following example looks for `NEW_DOCUMENT` events that meet certain criteria, then initiates a YARA scan against the offending file path.

Detect

```
event: NEW_DOCUMENT
op: and
rules:
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .\:\\(users|windows\\temp)\\.*
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .*\.(exe|dll)
```

Respond

```
# Report is optional, but informative
- action: report
  name: Executable written to Users or Temp (yara scan)

# Initiate a sensor command to yara scan the FILE_PATH
- action: task
  command: yara_scan hive://yara/malware-rule -f "{{ .event.FILE_PATH }}"
  investigation: Yara Scan Executable
  suppression:
    is_global: false
    keys:
      - '{{ .event.FILE_PATH }}'
      - Yara Scan Executable
    max_count: 1
    period: 1m
```

Notice the use of `suppression` to prevent the same `FILE_PATH` from being scanned more than once per minute to prevent a resource runaway situation.

Which D&R Rule Triggered a Command?

To determine which D&R rule triggered a command on an endpoint, navigate to the `Platform Logs` section. If a command was triggered by a D&R rule, the audit log will show the associate rule. If the command was sent via the API, the audit logs will show the API key name.

> To learn about all possible actions, see [Reference: Response Actions](/v2/docs/response-actions).

## Putting It All Together

Let's take this knowledge and write a rule to detect something a little more interesting.

On Windows there's a command called `icacls` which can be used to modify access control lists. Let's write a rule which detects any tampering via that command.

The first thing we can do is detect any new `icacls` processes:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: icacls.exe
```

And we'll set a basic response action to report the detection, too:

```
- action: report
  name: win-acl-tampering
```

If we save that, we'll start to see detections for any `icacls` processes spawning. However, not all of them will be particularly interesting from a security perspective. In this case, we only really care about invocations of `icacls` where the `grant` parameter is specified.

Let's make this rule more specific. We can do this by using the `and` operator to match multiple operators. We'll check for the string `"grant"` in the `COMMAND_LINE`, and while we're at it we'll make sure we don't bother evaluating other platforms by using the `is windows` operator.

```
event: NEW_PROCESS
op: and
rules:
- op: is windows
- op: ends with
	path: event/FILE_PATH
	value: icacls.exe
- op: contains
  path: event/COMMAND_LINE
  value: grant
```

This more specific rule means we'll see fewer false positives to look at or exclude later.

However, we still might miss some invocations of `icacls` with this detection if they use any capital letters  our operators are being evaluated with an implicit `case sensitive: true` by default. Let's turn case sensitivity off and observe the final rule:

```
# Detection
event: NEW_PROCESS
op: and
rules:
- op: is windows
- op: ends with
	case sensitive: false
	path: event/FILE_PATH
	value: icacls.exe
- op: contains
	case sensitive: false
  path: event/COMMAND_LINE
  value: grant

# Response
- action: report
  name: win-acl-tampering
```

This rule combines multiple operators to specify the exact conditions which might make an `icacls` process interesting. If it sees one, it'll report it as a `win-acl-tampering` detection which will be forwarded to Outputs and become viewable in the Detections page.

> Tip: test your rules without waiting for events! We recommend enabling the replay add-on for a better D&R rule writing experience.
>
> * Visit Timeline of a sensor and `Build D&R Rule` directly from real events
> * While drafting a rule, `Replay` an event against the rule to see if it would match
> * Replay a rule over historical events to see if any detections would have occurred

---

# Detection and Response Examples

The following are sample detection and response rules can help you get started in crafting efficient rules utilizing LimaCharlie's telemetry. In addition to these rules, we also recommend checking out [Sigma Rules](/v2/docs/sigma-rules) for more rules.

## Translating Existing Rules

Before listing examples, it's worth mentioning [uncoder.io](https://uncoder.io/) by [SOC Prime](https://socprime.com/) is a great resource for learning by analogy. If you're already familiar with another platform for rules or search queries (Sigma, Splunk, Kibana, etc.) you can use uncoder to translate to LimaCharlie's D&R rules.

## Examples

Note that through limacharlie.io, in order to provide an easier to edit format, the same rule configuration is used but is in YAML format instead. For example:

```
# Detection
op: ends with
event: NEW_PROCESS
path: event/FILE_PATH
value: .scr

# Response
- action: report
  name: susp_screensaver
- action: add tag
  tag: uses_screensaver
  ttl: 80000
```

### WanaCry

Simple WanaCry detection and mitigation rule:

```
# Detection
op: ends with
event: NEW_PROCESS
path: event/FILE_PATH
value: wanadecryptor.exe
case sensitive: false

# Response
- action: report
  name: wanacry
- action: task
  command: history_dump
- action: task
  command:
    - deny_tree
    - <<routing/this>>
```

### Classify Users

Tag any Sensor where the CEO logs in with "vip".

```
# Detection
op: is
event: USER_OBSERVED
path: event/USER_NAME
value: stevejobs
case sensitive: false

# Response
- action: add tag
  tag: vip
```

### SSH from External IP Address

The following example looks for connections to/from `sshd` involving a non-RFC1918 IP Address. Be mindful that this is only looking for network connections, not actual logons, so this could be noisy on an internet-facing system but still indicative of an exposed service.

```
# Detection
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: ends with
    path: event/FILE_PATH
    value: /sshd
  - op: is public address
    path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS

 # Response
- action: report
  name: >-
    SSH from EXTERNAL IP - {{ index (index .event.NETWORK_ACTIVITY 0) "SOURCE" "IP_ADDRESS" }}
```

The `report` uses [Go Templates](/v2/docs/template-strings-and-transforms) to include the offending IP address in the detection name.

### RDP from External IP Address

Similar to the above SSH example, this example looks for RDP connections from an external IP address. Be mindful that this is only looking for network connections, not actual logons, so this could be noisy on an internet-facing system but still indicative of an exposed service.

```
# Detection
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: is
    path: event/FILE_PATH
    value: C:\WINDOWS\System32\svchost.exe
  - op: contains
    path: event/COMMAND_LINE
    value: TermService
  - op: is
    path: event/NETWORK_ACTIVITY/DESTINATION/PORT
    value: 3389
  - op: is public address
    path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS

# Response
- action: report
  name: >-
    RDP from EXTERNAL IP - {{ index (index .event.NETWORK_ACTIVITY 0) "SOURCE" "IP_ADDRESS" }}
```

The `report` uses [Go Templates](/v2/docs/template-strings-and-transforms) to include the offending IP address in the detection name.

### Suspicious Windows Executable Names

```
# Detection
event: CODE_IDENTITY
op: matches
path: event/FILE_PATH
case sensitive: false
re: .*((\\.txt)|(\\.doc.?)|(\\.ppt.?)|(\\.xls.?)|(\\.zip)|(\\.rar)|(\\.rtf)|(\\.jpg)|(\\.gif)|(\\.pdf)|(\\.wmi)|(\\.avi)|( {5}.*))\\.exe

# Response
- action: report
  name: Executable with suspicious double extension
```

### Disable an Event at the Source

Turn off the sending of a specific event to the cloud. Useful to limit some verbose data sources when not needed.

```
# Detection
event: CONNECTED
op: is platform
name: windows

# Response
- action: task
  command: exfil_del NEW_DOCUMENT
```

### Windows Event Logs

A simple example of looking for a specific Event ID in WEL events.

```
# Detection
event: WEL
op: and
rules:
  - op: is
    path: event/EVENT/System/EventID
    value: '4625'
  - op: is
    path: event/EVENT/System/Channel
    value: Security

# Response
- action: report
  name: Failed Logon
```

### Nested Logic

An example demonstrating nested boolean logic. This detection looks specifically for the following conditions:
 ((`4697` OR `7045`) in the `System` log) OR (`4698` in the `Security` log)

```
# Detection
event: WEL
op: or
rules:
  - op: and
    rules:
      - op: is
        path: event/EVENT/System/Channel
        value: System
      - op: or
        rules:
          - op: is
            path: event/EVENT/System/EventID
            value: '4697'
          - op: is
            path: event/EVENT/System/EventID
            value: '7045'
  - op: and
    rules:
      - op: is
        path: event/EVENT/System/Channel
        value: Security
      - op: is
        path: event/EVENT/System/EventID
        value: '4698'
```

### File Integrity Monitoring

#### Monitoring Sensitive Directories

Make sure the File Integrity Monitoring of some directories is enabled whenever Windows sensors connect.

```
# Detection
event: CONNECTED
op: is platform
name: windows

# Response
- action: task
  command: fim_add --pattern 'C:\*\Programs\Startup\*' --pattern '\REGISTRY\*\Microsoft\Windows\CurrentVersion\Run*'
```

Similar example for a Linux web server.

```
# Detection
event: CONNECTED
op: is platform
name: linux

# Response
- action: task
  command: fim_add --pattern '/var/www/*'
```

#### FIM Hit Detection

Adding a FIM pattern with `fim_add` by itself will only cause `FIM_HIT` events to be generated on the affected system's timeline. To know that we have positive hits on a FIM rule, we want to capture the relevant event and generate a proper Detection.

```
# Detection
event: FIM_HIT
op: exists
path: event/FILE_PATH

# Response
- action: report
  name: FIM Hit - {{ .event.FILE_PATH }}
```

### YARA Scanning

> **Resource Utilization**
>
> Performing CPU intensive actions such as YARA scanning can impact endpoint performance if not optimized. Be sure to always test rules that carry out sensor commands (like the examples below) before deploying at scale in production. Use [suppression](/v2/docs/response-actions#suppression) to prevent runaway conditions.

Here are a few examples of using D&R rules to initiate automatic YARA scans on an endpoint. Note that the defined YARA rule must exist in your org before using it in a D&R rule.

#### YARA Scan Processes

This example looks for `NEW_PROCESS` events that meet certain criteria, then initiates a YARA scan against the offending process ID in memory. Note, this or a similar D&R rule will also depend on a companion [YARA Detection](/v2/docs/detection-and-response-examples#yara-detections) rule.

```
# Detection
event: NEW_PROCESS
op: and
rules:
  - op: starts with
    path: event/FILE_PATH
    value: C:\Users\
  - op: contains
    path: event/FILE_PATH
    value: \Downloads\

# Response
## Report is optional, but informative
- action: report
  name: Execution from Downloads directory
## Initiate a sensor command to yara scan the PROCESS_ID
- action: task
  command: yara_scan hive://yara/malware-rule --pid "{{ .event.PROCESS_ID }}"
  investigation: Yara Scan Process
  suppression:
    is_global: false
    keys:
      - '{{ .event.PROCESS_ID }}'
      - Yara Scan Process
    max_count: 1
    period: 1m
```

Notice the use of `suppression` to prevent the same `PROCESS_ID` from being scanned more than once per minute to prevent a resource runaway situation.

#### YARA Scan Files

This example looks for `NEW_DOCUMENT` events that meet certain criteria, then initiates a YARA scan against the offending file path. Note, this or a similar D&R rule will also depend on a companion [YARA Detection](/v2/docs/detection-and-response-examples#yara-detections) rule.

```
# Detection
event: NEW_DOCUMENT
op: and
rules:
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .\:\\(users|windows\\temp)\\.*
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .*\.(exe|dll)

# Response
## Report is optional, but informative
- action: report
  name: Executable written to Users or Temp (yara scan)
## Initiate a sensor command to yara scan the FILE_PATH
- action: task
  command: yara_scan hive://yara/malware-rule -f "{{ .event.FILE_PATH }}"
  investigation: Yara Scan Executable
  suppression:
    is_global: false
    keys:
      - '{{ .event.FILE_PATH }}'
      - Yara Scan Executable
    max_count: 1
    period: 1m
```

Notice the use of `suppression` to prevent the same `FILE_PATH` from being scanned more than once per minute to prevent a resource runaway situation.

### YARA Detections

Running a YARA scan by itself only sends a `YARA_DETECTION` event to the affected system's timeline. To know that we have positive hits on a YARA scan, we want to capture the relevant event and generate a proper Detection. The following two examples split out a YARA detection on-disk, versus in-memory. Notice we simply check for the presence of `event/PROCESS/*` fields to determine if it's a file or process detection, which may have different severities to security teams (dormant malware versus running malware).

#### YARA Detection On-Disk (file)

```
# Detection
event: YARA_DETECTION
op: and
rules:
  - not: true
    op: exists
    path: event/PROCESS/*
  - op: exists
    path: event/RULE_NAME

# Response
- action: report
  name: YARA Detection on Disk - {{ .event.RULE_NAME }}
- action: add tag
  tag: yara_detection_disk
  ttl: 80000
```

#### YARA Detection In-Memory (process)

```
# Detection
event: YARA_DETECTION
op: and
rules:
  - op: exists
    path: event/RULE_NAME
  - op: exists
    path: event/PROCESS/*

# Response
- action: report
  name: YARA Detection in Memory - {{ .event.RULE_NAME }}
- action: add tag
  tag: yara_detection_memory
  ttl: 80000
```

Both rules will generate a Detection report and add a tag to the system which the detection occurred on.

### Mention of an Internal Resource

Look for references to private URLs in proxy logs.

```
# Detection
target: artifact
op: contains
path: /text
value: /corp/private/info

# Response
- action: report
  name: web-proxy-private-url
```

### De-duplicate Cloned Sensors

Sometimes users install a sensor on a VM image by mistake. This means every time a new instance of the image gets started the same sensor ID (SID) is used for multiple boxes with different names. When detected, LimaCharlie produces a `sensor_clone` event.

We can use these events to deduplicate. This example targets Windows clones.

```
# Detection
target: deployment
event: sensor_clone
op: is platform
name: windows

# Response
- action: re-enroll
```

---

# Detection and Response Examples

The following are sample detection and response rules can help you get started in crafting efficient rules utilizing LimaCharlie's telemetry. In addition to these rules, we also recommend checking out [Sigma Rules](/v2/docs/sigma-rules) for more rules.

## Translating Existing Rules

Before listing examples, it's worth mentioning [uncoder.io](https://uncoder.io/) by [SOC Prime](https://socprime.com/) is a great resource for learning by analogy. If you're already familiar with another platform for rules or search queries (Sigma, Splunk, Kibana, etc.) you can use uncoder to translate to LimaCharlie's D&R rules.

Check out this video that shows you the power of leveraging community resources with LimaCharlie

## Examples

Note that through limacharlie.io, in order to provide an easier to edit format, the same rule configuration is used but is in YAML format instead. For example:

```
# Detection
op: ends with
event: NEW_PROCESS
path: event/FILE_PATH
value: .scr

# Response
- action: report
  name: susp_screensaver
- action: add tag
  tag: uses_screensaver
  ttl: 80000
```

### WanaCry

Simple WanaCry detection and mitigation rule:

```
# Detection
op: ends with
event: NEW_PROCESS
path: event/FILE_PATH
value: wanadecryptor.exe
case sensitive: false

# Response
- action: report
  name: wanacry
- action: task
  command: history_dump
- action: task
  command:
    - deny_tree
    - <<routing/this>>
```

### Classify Users

Tag any Sensor where the CEO logs in with "vip".

```
# Detection
op: is
event: USER_OBSERVED
path: event/USER_NAME
value: stevejobs
case sensitive: false

# Response
- action: add tag
  tag: vip
```

### SSH from External IP Address

The following example looks for connections to/from `sshd` involving a non-RFC1918 IP Address. Be mindful that this is only looking for network connections, not actual logons, so this could be noisy on an internet-facing system but still indicative of an exposed service.

```
# Detection
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: ends with
    path: event/FILE_PATH
    value: /sshd
  - op: is public address
    path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS

 # Response
- action: report
  name: >-
    SSH from EXTERNAL IP - {{ index (index .event.NETWORK_ACTIVITY 0) "SOURCE" "IP_ADDRESS" }}
```

The `report` uses [Go Templates](/v2/docs/template-strings-and-transforms) to include the offending IP address in the detection name.

### RDP from External IP Address

Similar to the above SSH example, this example looks for RDP connections from an external IP address. Be mindful that this is only looking for network connections, not actual logons, so this could be noisy on an internet-facing system but still indicative of an exposed service.

```
# Detection
event: NETWORK_CONNECTIONS
op: and
rules:
  - op: is
    path: event/FILE_PATH
    value: C:\WINDOWS\System32\svchost.exe
  - op: contains
    path: event/COMMAND_LINE
    value: TermService
  - op: is
    path: event/NETWORK_ACTIVITY/DESTINATION/PORT
    value: 3389
  - op: is public address
    path: event/NETWORK_ACTIVITY/SOURCE/IP_ADDRESS

# Response
- action: report
  name: >-
    RDP from EXTERNAL IP - {{ index (index .event.NETWORK_ACTIVITY 0) "SOURCE" "IP_ADDRESS" }}
```

The `report` uses [Go Templates](/v2/docs/template-strings-and-transforms) to include the offending IP address in the detection name.

### Suspicious Windows Executable Names

```
# Detection
event: CODE_IDENTITY
op: matches
path: event/FILE_PATH
case sensitive: false
re: .*((\\.txt)|(\\.doc.?)|(\\.ppt.?)|(\\.xls.?)|(\\.zip)|(\\.rar)|(\\.rtf)|(\\.jpg)|(\\.gif)|(\\.pdf)|(\\.wmi)|(\\.avi)|( {5}.*))\\.exe

# Response
- action: report
  name: Executable with suspicious double extension
```

### Disable an Event at the Source

Turn off the sending of a specific event to the cloud. Useful to limit some verbose data sources when not needed.

```
# Detection
event: CONNECTED
op: is platform
name: windows

# Response
- action: task
  command: exfil_del NEW_DOCUMENT
```

### Windows Event Logs

A simple example of looking for a specific Event ID in WEL events.

```
# Detection
event: WEL
op: and
rules:
  - op: is
    path: event/EVENT/System/EventID
    value: '4625'
  - op: is
    path: event/EVENT/System/Channel
    value: Security

# Response
- action: report
  name: Failed Logon
```

### Nested Logic

An example demonstrating nested boolean logic. This detection looks specifically for the following conditions:
 ((`4697` OR `7045`) in the `System` log) OR (`4698` in the `Security` log)

```
# Detection
event: WEL
op: or
rules:
  - op: and
    rules:
      - op: is
        path: event/EVENT/System/Channel
        value: System
      - op: or
        rules:
          - op: is
            path: event/EVENT/System/EventID
            value: '4697'
          - op: is
            path: event/EVENT/System/EventID
            value: '7045'
  - op: and
    rules:
      - op: is
        path: event/EVENT/System/Channel
        value: Security
      - op: is
        path: event/EVENT/System/EventID
        value: '4698'
```

### File Integrity Monitoring

#### Monitoring Sensitive Directories

Make sure the File Integrity Monitoring of some directories is enabled whenever Windows sensors connect.

```
# Detection
event: CONNECTED
op: is platform
name: windows

# Response
- action: task
  command: fim_add --pattern 'C:\*\Programs\Startup\*' --pattern '\REGISTRY\*\Microsoft\Windows\CurrentVersion\Run*'
```

Similar example for a Linux web server.

```
# Detection
event: CONNECTED
op: is platform
name: linux

# Response
- action: task
  command: fim_add --pattern '/var/www/*'
```

#### FIM Hit Detection

Adding a FIM pattern with `fim_add` by itself will only cause `FIM_HIT` events to be generated on the affected system's timeline. To know that we have positive hits on a FIM rule, we want to capture the relevant event and generate a proper Detection.

```
# Detection
event: FIM_HIT
op: exists
path: event/FILE_PATH

# Response
- action: report
  name: FIM Hit - {{ .event.FILE_PATH }}
```

### YARA Scanning

**Resource Utilization**

Performing CPU intensive actions such as YARA scanning can impact endpoint performance if not optimized. Be sure to always test rules that carry out sensor commands (like the examples below) before deploying at scale in production. Use [suppression](/v2/docs/response-actions#suppression) to prevent runaway conditions.

Here are a few examples of using D&R rules to initiate automatic YARA scans on an endpoint. Note that the defined YARA rule must exist in your org before using it in a D&R rule.

#### YARA Scan Processes

This example looks for `NEW_PROCESS` events that meet certain criteria, then initiates a YARA scan against the offending process ID in memory. Note, this or a similar D&R rule will also depend on a companion [YARA Detection](/v2/docs/detection-and-response-examples#yara-detections) rule.

```
# Detection
event: NEW_PROCESS
op: and
rules:
  - op: starts with
    path: event/FILE_PATH
    value: C:\Users\
  - op: contains
    path: event/FILE_PATH
    value: \Downloads\

# Response
## Report is optional, but informative
- action: report
  name: Execution from Downloads directory
## Initiate a sensor command to yara scan the PROCESS_ID
- action: task
  command: yara_scan hive://yara/malware-rule --pid "{{ .event.PROCESS_ID }}"
  investigation: Yara Scan Process
  suppression:
    is_global: false
    keys:
      - '{{ .event.PROCESS_ID }}'
      - Yara Scan Process
    max_count: 1
    period: 1m
```

Notice the use of `suppression` to prevent the same `PROCESS_ID` from being scanned more than once per minute to prevent a resource runaway situation.

#### YARA Scan Files

This example looks for `NEW_DOCUMENT` events that meet certain criteria, then initiates a YARA scan against the offending file path. Note, this or a similar D&R rule will also depend on a companion [YARA Detection](/v2/docs/detection-and-response-examples#yara-detections) rule.

```
# Detection
event: NEW_DOCUMENT
op: and
rules:
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .\:\\(users|windows\\temp)\\.*
  - case sensitive: false
    op: matches
    path: event/FILE_PATH
    re: .*\.(exe|dll)

# Response
## Report is optional, but informative
- action: report
  name: Executable written to Users or Temp (yara scan)
## Initiate a sensor command to yara scan the FILE_PATH
- action: task
  command: yara_scan hive://yara/malware-rule -f "{{ .event.FILE_PATH }}"
  investigation: Yara Scan Executable
  suppression:
    is_global: false
    keys:
      - '{{ .event.FILE_PATH }}'
      - Yara Scan Executable
    max_count: 1
    period: 1m
```

Notice the use of `suppression` to prevent the same `FILE_PATH` from being scanned more than once per minute to prevent a resource runaway situation.

### YARA Detections

Running a YARA scan by itself only sends a `YARA_DETECTION` event to the affected system's timeline. To know that we have positive hits on a YARA scan, we want to capture the relevant event and generate a proper Detection. The following two examples split out a YARA detection on-disk, versus in-memory. Notice we simply check for the presence of `event/PROCESS/*` fields to determine if it's a file or process detection, which may have different severities to security teams (dormant malware versus running malware).

#### YARA Detection On-Disk (file)

```
# Detection
event: YARA_DETECTION
op: and
rules:
  - not: true
    op: exists
    path: event/PROCESS/*
  - op: exists
    path: event/RULE_NAME

# Response
- action: report
  name: YARA Detection on Disk - {{ .event.RULE_NAME }}
- action: add tag
  tag: yara_detection_disk
  ttl: 80000
```

#### YARA Detection In-Memory (process)

```
# Detection
event: YARA_DETECTION
op: and
rules:
  - op: exists
    path: event/RULE_NAME
  - op: exists
    path: event/PROCESS/*

# Response
- action: report
  name: YARA Detection in Memory - {{ .event.RULE_NAME }}
- action: add tag
  tag: yara_detection_memory
  ttl: 80000
```

Both rules will generate a Detection report and add a tag to the system which the detection occurred on.

### Mention of an Internal Resource

Look for references to private URLs in proxy logs.

```
# Detection
target: artifact
op: contains
path: /text
value: /corp/private/info

# Response
- action: report
  name: web-proxy-private-url
```

### De-duplicate Cloned Sensors

Sometimes users install a sensor on a VM image by mistake. This means every time a new instance of the image gets started the same sensor ID (SID) is used for multiple boxes with different names. When detected, LimaCharlie produces a `sensor_clone` event.

We can use these events to deduplicate. This example targets Windows clones.

```
# Detection
target: deployment
event: sensor_clone
op: is platform
name: windows

# Response
- action: re-enroll
```

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Detection on Alternate Targets

Detection & Response rules run against `edr` events by default, however, there are 7 other targets:

* `detection`
* `deployment`
* `artifact`
* `artifact_event`
* `schedule`
* `audit`
* `billing`

This article is to give some ideas of what they're used for, and how they're used.

## Target: detection

You can run rules on detections generated by other rules. This allows you to further filter existing detections
and change add a response behavior to certain special cases.

In the `detection` target, the `event:` or `events:` specified refer to the `name` of the detection specified in the
original detection's `report` action.

The `detection` target supports all of the same operators and actions as regular `edr` rules.

### Example

```
# Detection
target: detection
op: and
rules:
- op: is
  path: cat
  value: virus-total-hit
- op: is
  path: routing/hostname
  value: ceo-laptop

# Response
- action: extension request
  extension name: pagerduty
  extension action: run
  request:
    group: '{{ "lc-alerts" }}'
    severity: '{{ "critical" }}'
    component: '{{ "vip-alert" }}'
    summary: '{{ "Alert on a VIP endpoint." }}'
    source: '{{ "limacharlie.io" }}'
    class: '{{ "dr-rules" }}'
```

This rule takes a pre-existing detection report named `virus-total-hit` and sends it to PagerDuty if it occurs on a specific hostname.

## Target: deployment

Deployment events relate to sensors connecting to the cloud: `enrollment`, `sensor_clone`, `sensor_over_quota`, `deleted_sensor`.

Take the `sensor_clone` event as an example. This event can happen when a Sensor is installed in a VM image, leading to duplicate sensor IDs connecting to the cloud. When this is detected we can use this event to automate behavior to de-duplicate the sensor.

The `deployment` target supports all of the same operators and actions as regular `edr` rules.

### Example

```
# Detection
target: deployment
event: sensor_clone
op: is windows

# Response
- action: task
  command: file_del %windir%\system32\hcp.dat
- action: task
  command: file_del %windir%\system32\hcp_hbs.dat
- action: task
  command: file_del %windir%\system32\hcp_conf.dat
- action: task
  command: restart
```

This rule de-duplicates sensors on Windows by deleting `.dat` files specific to the Windows installation and then issuing a `restart` sensor command.

> For samples of each `deployment` event type, see [Reference: Platform Events](/v2/docs/reference-platform-events).

## Target: artifact

Parsed artifacts can be run through the rule engine as if they were regular `edr` events, but there are some key differences. Namely, they support a subset of operators and actions, while adding some special parameters.

### Example

This rule will target parsed `/var/log/auth.log` entries to see if there are are auth failures.

```
# Detection
target: artifact
artifact type: txt
artifact path: /var/log/auth.log
op: matches
re: .*(authentication failure|Failed password).*
path: /text
case sensitive: false

# Response
- action: report
  name: Failed Auth
```

### Supported Operators

* `is`
* `and`
* `or`
* `exists`
* `contains`
* `starts with`
* `ends with`
* `is greater than`
* `is lower than`
* `matches`
* `string distance`

### Supported Resources

`lookup` and `external` resources are supported within rules just like the `edr` target.

### Supported Actions

The only response action supported for the `artifact` target is the `report` action.

### Special Parameters

* `artifact path`: matches the start of the artifact's `path` string, e.g. `/auth.log`
* `artifact type`: matches the artifact's `type` string, e.g. `pcap`, `zeek`, `auth`, `wel`
* `artifact source`: matches the artifact's `source` string, e.g. `hostname-123`

> Note: for duplicate Windows Event Log ingestions, the rule engine will use the log's `EventRecordID` to ensure a rule will not run more than once over the same record.

## Target: artifact_event

For unparsed logs, it can be useful to use the `ingest` and `export_complete` lifecycle events from the `artifact_event` target to automate behaviors in response to artifacts.

> For samples of `ingest` and `export_complete`, see [Reference: Platform Events](/v2/docs/reference-platform-events).

### Example

```
# Detection
target: artifact_event
event: export_complete
op: starts with
path: routing/log_type
value: pcap
case sensitive: false

# Response
- action: report
  name: PCAP Artifact ready to Download
```

## Target: schedule

Schedule events are triggered automatically at various intervals per Organization or per Sensor, observable in rules via the `schedule` target.

For more information, see [Reference: Schedule Events](/v2/docs/reference-schedule-events)

## Target: audit

Audit events are generated by the LimaCharlie platform and track changes and events from within the platform such as tasking, replays, hive changes, etc. These events can be viewed within the "Platform Logs" menu or by viewing events from the `audit-logs` sensor.

## Target: billing

Billing events are generated by the LimaCharlie platform and are related to aspects of the platform such as quotas, thresholds, and other cost-associated events. For an example, see the [Usage Alerts Extension](/v2/docs/ext-usage-alerts) documentation

---

# False Positive Rules

To reduce the number of false positives, you may want to create false positive (FP) rules. FP rules filter out detections generated by the `report` action of detection & response (D&R) rules. These rules apply globally across all rule namespaces and targets.

There are multiple ways to create a false positive rule in LimaCharlie web app.

> **Have multiple organizations?**
>
> Similar to detection & response rules, false positive rules are created on a per tenant level. This means that if you have more than one organization you want to apply the rule to, you will want to:
>
> * re-create the same rule in multiple organizations, or
> * using our infrastructure as code functionality, push your FP rules to multiple tenants within seconds.

## Use Cases

The typical use case for FP rules is to add exceptions from some detections that are cross-cutting (for example ignore all detections from a specific host), organization-specific exceptions (like ignoring alerts relating to a custom piece of software used in an organization), or suppressing errors from managed rules you don't have direct access to.

## Structure

False positive rules are structured roughly the same as the detection part of a D&R rule. The main difference is that instead of a direct event, the rule applies to the content of a detection, as can be seen in the **Detections** section of the web app.

The originating event for the detection can still be accessed at the `detect` path. This means that if ignoring something based on the event content, we only need to add `detect/` to the front of the `path` (see [example](#ignore-detections-for-specific-file-name)).

## Create a False Positive Rule From Detections

This is the quickest and the most common way to create a FP rule. On every detection, you can click the `Mark False Positive` button.

![Mark False Positive button](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-1.png)

Clicking the button will pre-populate the details of the event and automatically generate a draft false positive rule which you can edit before saving.

After the rule is saved, it will appear in the **False Positives Rules** section and can be edited/deleted there.

## Create a False Positive rule from scratch

While creating FP rule from detections is a common and easy way to reduce the number of false positives, you do not need to wait for the detection to happen before creating a FP rule. The `False Positive Rules` section under `Automation` allows you to create a false positive rule from scratch.

To create a new false positive rule, click the `New Rule` button.

![New Rule button](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-2.png)

This will open a rule editor allowing you to create a new rule.

![Rule editor](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-3.png)

An FP rule is structured with the same format at the detection component of a D&R rule. The main difference is that the rule applies to the content of a detection, as can be seen in the Detections section of the web app.

You have the ability to set a rule name as well as an **Expiry Date** (optional). Setting an expiry date allows you to create a rule that will expire at a certain time.

Please note that expiry times must be set in the user's preferred time (not in UTC).

## Examples

### Suppress a Specific Detection

Prevent a specific detection:

```
op: is
path: cat
value: my-detect-name
```

### Ignore Detections for Specific File Name

Ignore any detection that relates to a file name in any path.

```
op: ends with
path: detect/event/FILE_PATH
value: this_is_fine.exe
```

### Ignore Detections on a Specific Host

Any detection originating from a specific host will be ignored.

```
op: is
path: routing/hostname
value: web-server-2
```

---

# False Positive Rules

To reduce the number of false positives, you may want to create false positive (FP) rules. FP rules filter out detections generated by the `report` action of detection & response (D&R) rules. These rules apply globally across all rule namespaces and targets.

There are multiple ways to create a false positive rule in LimaCharlie web app.

> **Have multiple organizations?**
>
> Similar to detection & response rules, false positive rules are created on a per tenant level. This means that if you have more than one organization you want to apply the rule to, you will want to:
>
> * re-create the same rule in multiple organizations, or
> * using our infrastructure as code functionality, push your FP rules to multiple tenants within seconds.

## Use Cases

The typical use case for FP rules is to add exceptions from some detections that are cross-cutting (for example ignore all detections from a specific host), organization-specific exceptions (like ignoring alerts relating to a custom piece of software used in an organization), or suppressing errors from managed rules you don't have direct access to.

## Structure

False positive rules are structured roughly the same as the detection part of a D&R rule. The main difference is that instead of a direct event, the rule applies to the content of a detection, as can be seen in the **Detections** section of the web app.

The originating event for the detection can still be accessed at the `detect` path. This means that if ignoring something based on the event content, we only need to add `detect/` to the front of the `path` (see [example](#ignore-detections-for-specific-file-name)).

## Create a False Positive Rule From Detections

This is the quickest and the most common way to create a FP rule. On every detection, you can click the `Mark False Positive` button.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-1.png)

Clicking the button will pre-populate the details of the event and automatically generate a draft false positive rule which you can edit before saving.

After the rule is saved, it will appear in the **False Positives Rules** section and can be edited/deleted there.

## Create a False Positive rule from scratch

While creating FP rule from detections is a common and easy way to reduce the number of false positives, you do not need to wait for the detection to happen before creating a FP rule. The `False Positive Rules` section under `Automation` allows you to create a false positive rule from scratch.

To create a new false positive rule, click the `New Rule` button.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-2.png)

This will open a rule editor allowing you to create a new rule.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/fp-rules-3.png)

An FP rule is structured with the same format at the detection component of a D&R rule. The main difference is that the rule applies to the content of a detection, as can be seen in the Detections section of the web app.

You have the ability to set a rule name as well as an **Expiry Date** (optional). Setting an expiry date allows you to create a rule that will expire at a certain time.

Please note that expiry times must be set in the user's preferred time (not in UTC).

## Examples

### Suppress a Specific Detection

Prevent a specific detection:

```
op: is
path: cat
value: my-detect-name
```

### Ignore Detections for Specific File Name

Ignore any detection that relates to a file name in any path.

```
op: ends with
path: detect/event/FILE_PATH
value: this_is_fine.exe
```

### Ignore Detections on a Specific Host

Any detection originating from a specific host will be ignored.

```
op: is
path: routing/hostname
value: web-server-2
```

---

# Incident Response

LimaCharlie provides incident response teams with a powerful, centralized solution that unifies threat visibility across diverse data sources, streamlines detection capabilities, enhances threat hunting and analysis, and enables instant deployment. IR teams can respond to incidents with unparalleled speed, accuracy, and effectiveness.

## Incident response problems

* **Limited visibility and data correlation:** Incident response teams often face incomplete or fragmented data from clients, hindering their ability to grasp the full extent of an incident and make informed analysis.
* **Time-consuming manual analysis:** Manually sifting through large volumes of logs, alerts, and endpoint data can be time-consuming and prone to human error, delaying incident response and remediation.
* **Lack of centralized threat hunting capabilities:** Traditional IR toolkits often lack advanced analytics and threat hunting features, making it challenging to proactively uncover hidden threats or investigate complex attack patterns.
* **Slow infrastructure deployment:** Setting up traditional incident response infrastructure can take hours or even days, leaving critical time gaps where attackers have the upper hand.

## LimaCharlie's solution

* **Unified Threat Visibility:** LimaCharlie aggregates data from diverse sources (endpoints, networks, cloud environments, security tools), providing incident response teams with centralized visibility and context for swift analysis.
* **Streamlined Detection Capabilities:** The platform's powerful detection and response capabilities allow IR teams to quickly pinpoint relevant data, identify patterns, and correlate events across multiple impacted systems.
* **Enhanced Threat Hunting and Analysis:** LimaCharlie's advanced analytics and threat intelligence feeds enable IR teams to proactively hunt for hidden threats, investigate attack chains, and attribute attacks with greater confidence.
* **Instant Deployment:** Launch LimaCharlie in seconds, not hours, gaining immediate visibility and control over the compromised environment to outpace attacker timelines and minimize damage.

---

# Incident Response

LimaCharlie provides incident response teams with a powerful, centralized solution that unifies threat visibility across diverse data sources, streamlines detection capabilities, enhances threat hunting and analysis, and enables instant deployment. IR teams can respond to incidents with unparalleled speed, accuracy, and effectiveness.

## Incident response problems

* **Limited visibility and data correlation:** Incident response teams often face incomplete or fragmented data from clients, hindering their ability to grasp the full extent of an incident and make informed analysis.
* **Time-consuming manual analysis:** Manually sifting through large volumes of logs, alerts, and endpoint data can be time-consuming and prone to human error, delaying incident response and remediation.
* **Lack of centralized threat hunting capabilities:** Traditional IR toolkits often lack advanced analytics and threat hunting features, making it challenging to proactively uncover hidden threats or investigate complex attack patterns.
* **Slow infrastructure deployment:** Setting up traditional incident response infrastructure can take hours or even days, leaving critical time gaps where attackers have the upper hand.

## LimaCharlie's solution

* **Unified Threat Visibility:** LimaCharlie aggregates data from diverse sources (endpoints, networks, cloud environments, security tools), providing incident response teams with centralized visibility and context for swift analysis.
* **Streamlined Detection Capabilities:** The platform's powerful detection and response capabilities allow IR teams to quickly pinpoint relevant data, identify patterns, and correlate events across multiple impacted systems.
* **Enhanced Threat Hunting and Analysis:** LimaCharlie's advanced analytics and threat intelligence feeds enable IR teams to proactively hunt for hidden threats, investigate attack chains, and attribute attacks with greater confidence.
* **Instant Deployment:** Launch LimaCharlie in seconds, not hours, gaining immediate visibility and control over the compromised environment to outpace attacker timelines and minimize damage.

---

# Tag: detection and response

## Articles

### Soteria AWS Rules
10 Oct 2025

### False Positive Rules
10 Oct 2025

### Soteria M365 Rules
09 Oct 2025

### Soteria EDR Rules
09 Oct 2025

### Create a D&R Rule Using a Threat Feed
08 Oct 2025

### Stateful Rules
29 Sep 2025

### Reference: EDR Events
22 Sep 2025

### Reference: Endpoint Agent Commands
07 Aug 2025

### Detection Logic Operators
07 Aug 2025

### Detection on Alternate Targets
04 Aug 2025

### Endpoint Detection and Response (EDR)
31 Jul 2025

### Response Actions
02 May 2025

### Template Strings and Transforms
25 Apr 2025

### Template Strings and Transforms
25 Apr 2025

### Template Strings and Transforms
25 Apr 2025

### Template Strings and Transforms
25 Apr 2025

### Writing and Testing Rules
02 Apr 2025

### Detection and Response Examples
03 Jan 2025

### Sigma Converter
12 Nov 2024

### Sysmon Comparison
12 Nov 2024

---

# Detection and Response

[No documentation content available for this tag page]

---

# Managed Rulesets

In addition to LimaCharlie's powerful custom detection & response capabilities, we also offer native integration with several managed rulesets. LimaCharlie currently offers:

* Sigma Rules
* SnapAttack Community Edition
* SOC Prime
* Soteria
  + AWS
  + EDR
  + Microsoft/Office 365

## A Word on Managed Rulesets

While managed rulesets can help your organizations achieve detection and response capabilities quickly, not all detections are suitable for every environment.

Ensure that you are fine-tuning managed rulesets within your environment via enabling/disabling rules or via False Positive controls.

Managed rulesets offer several advantages, such as:

* Providing out-of-the-box coverage for common threats, reducing the time and effort to develop in-house rules.
* Curated rulesets are maintained and updated by their respective parties, often covering the latest threats.
* A foundation for building complex detection logic utilizing managed rulesets as inspiration.

Every environment is unique, and we recommend choosing rulesets that benefit your need(s) and/or use case(s).

## What's the difference between Sigma and Soteria rules?

[Sigma](https://github.com/SigmaHQ/sigma) is an open source project that aims at creating a generic query language for security and rules. It looks up known anomalies and Common Vulnerabilities and Exposures (CVEs).

As Sigma is an open source project,

* applying the Sigma ruleset is free
* there will be a higher rate of false positives

[Soteria](https://soteria.io/) is a US-based MSSP that has been using LimaCharlie for a long time. They developed a corpus of hundreds of behavioral signatures for Windows / Mac / Linux (signature not in terms of a hash, but in terms of a rule that describes a behavior). With one click, you can apply their rules in a managed way. When Soteria updates the rules for their customers, you will get those updates in real time as well.

As Soteria is a managed ruleset,

* applying the Soteria ruleset costs $0.5 per endpoint per month
* the rate of false positives is much lower

---

# Replay

Replay allows you to run [Detection & Response (D&R) rules](/v2/docs/detection-and-response) against historical traffic. This can be done in a few combinations of sources:

Rule Source:

* Existing rule in the organization, by name.
* Rule in the replay request.

Traffic:

* Sensor historical traffic.
* Local events provided during request.

## Using

Using the Replay API requires the [API key](/v2/docs/api-keys) to have the following permissions:

* `insight.evt.get`

The returned data from the API contains the following:

* `responses`: a list of the actions that would have been taken by the rule (like `report`, `task`, etc).
* `num_evals`: a number of evaluation operations performed by the rule. This is a rough estimate of the performance of the rule.
* `num_events`: the number of events that were replayed.
* `eval_time`: the number of seconds it took to replay the data.

```
{
  "error": "",        // if an error occured.
  "stats": {
    "n_proc": 0,      // the number of events processed
    "n_shard": 0,     // the number of chunks the replay job was broken into
    "n_eval": 0,      // the number of operator evaluations performed
    "wall_time": 0    // the number of real-world seconds the job took
  },
  "did_match": false, // indicates if the rule matched any event at all
  "results": [],      // a list of dictionaries containing the details of actions the engine would have taken
  "traces": []        // a list of trace items to help you troubleshoot where a rule failed
}
```

### Query Language

To use Replay in [LCQL Mode](/v2/docs/lcql) (LimaCharlie Query Language), you can specify your query in the `query` parameter of the Replay Request (defined below) when using the REST interface, or you can use the LimaCharlie Python SDK/CLI's [query interface](https://github.com/refractionPOINT/python-limacharlie/blob/master/limacharlie/Query.py): `limacharlie query --help`.

### Python CLI

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a friendly way to replay data, and to do so across larger datasets by automatically splitting up your query into multiple queries that can run in parallel.

Sample command line to query one sensor:

```
limacharlie-replay --sid 9cbed57a-6d6a-4af0-b881-803a99b177d9 --start 1556568500 --end 1556568600 --rule-content ./test_rule.txt
```

Sample command line to query an entire organization:

```
limacharlie-replay --entire-org --start 1555359000 --end 1556568600 --rule-name my-rule-name
```

If specifying a rule as content with the `--rule-content`, the format should be in `JSON` or `YAML` like:

```
detect:
  event: DNS_REQUEST
  op: is
  path: event/DOMAIN_NAME
  value: www.dilbert.com
respond:
  - action: report
    name: dilbert-is-here
```

Instead of specifying the `--entire-org` or `--sid` flags, you may use events from a local file via the `--events` flag.

We invite you to look at the command line usage itself, as the tool evolves.

### REST API

The Replay API is available to all DataCenter locations using a per-location URL. To get the appropriate URL for your organization, use the REST endpoint to retrieve the URLs found [here](https://api.limacharlie.io/static/swagger/#/Organizations/getOrgURLs) named `replay`.

Having per-location URLs will allow us to guarantee that processing occurs within the geographical area you chose. Currently, some locations are NOT guaranteed to be in the same area due to the fact we are using the Google Cloud Run product which is not available globally. For these cases, processing is currently done in the United States, but as soon as it becomes available in your area, the processing will be moved transparently.

Authentication to this API works with the same JWTs as the main limacharlie.io API.

For this example, we will use the experimental datacenter's URL:

```
https://0651b4f82df0a29c.replay.limacharlie.io/
```

The API mainly works on a per-sensor basis, on a limited amount of time. Replaying for multiple sensors (or entire org), or longer time period is done through multiple parallel API calls. This multiplexing is taken care of by the Python CLI above.

To query Replay, do a `POST` with a `Content-Type` header of `application-json` and with a JSON body like:

```
{
  "oid": "",             // OID this query relates to
  "rule_source": {       // rule source information (use one of "rule_name" or "rule")
    "rule_name": "",     // pre-existing rule name to run
    "namespace": "", // default: general namespace, can also be "managed" and "service"
    "rule": {            // literal rule to run
      "detect": {},
      "respond": []
    }
  },
  "event_source": {      // event source information (use one of "sensor_events" or "events")
    "sensor_events": {   // use historical events from sensors
      "sid": "",         // sensor id to replay from, or entire org if empty
      "start_time": 0,   // start second epoch time to replay from
      "end_time": 0      // end second epoch time to replay to
    },
    "events": [{}]       // literal list of events to replay
    "stream": "" // defaults to events, can also be "audit" or "detect"
  },
  "limit_event": 0,      // optional approximate number of events to process
  "limit_eval": 0,       // optional approximate number of operator evaluations to perform
  "trace": false,        // optional, if true add trace information to response, VERY VERBOSE
  "is_dry_run": false,   // optional, if true, an estimate of the total cost of the query will be returned
  "query": ""            // optional alternative way to describe a replay query as a LimaCharlie Query Language (LCQL) query.
}
```

Like the other endpoints you can also submit a `rule_name` in the URL query if you want to use an existing organization rule.

You may also specify a `limit_event` and `limit_eval` parameter as integers. They will limit the number of events evaluated and the number of rule evaluations performed (approximately). If the limits are reached, the response will contain an item named `limit_eval_reached: true` and `limit_event_reached: true`.

Finally, you may also set `trace` to `true` in the request to receive a detailed trace of the rule evaluation. This is useful in the development of new rules to find where rules are failing.

## Billing

The Replay service is billed on a per event evaluated.

---

# Replay

Replay allows you to run [Detection & Response (D&R) rules](/v2/docs/detection-and-response) against historical traffic. This can be done in a few combinations of sources:

Rule Source:

* Existing rule in the organization, by name.
* Rule in the replay request.

Traffic:

* Sensor historical traffic.
* Local events provided during request.

## Using

Using the Replay API requires the [API key](/v2/docs/api-keys) to have the following permissions:

* `insight.evt.get`

The returned data from the API contains the following:

* `responses`: a list of the actions that would have been taken by the rule (like `report`, `task`, etc).
* `num_evals`: a number of evaluation operations performed by the rule. This is a rough estimate of the performance of the rule.
* `num_events`: the number of events that were replayed.
* `eval_time`: the number of seconds it took to replay the data.

```
{
  "error": "",        // if an error occured.
  "stats": {
    "n_proc": 0,      // the number of events processed
    "n_shard": 0,     // the number of chunks the replay job was broken into
    "n_eval": 0,      // the number of operator evaluations performed
    "wall_time": 0    // the number of real-world seconds the job took
  },
  "did_match": false, // indicates if the rule matched any event at all
  "results": [],      // a list of dictionaries containing the details of actions the engine would have taken
  "traces": []        // a list of trace items to help you troubleshoot where a rule failed
}
```

### Query Language

To use Replay in [LCQL Mode](/v2/docs/lcql) (LimaCharlie Query Language), you can specify your query in the `query` parameter of the Replay Request (defined below) when using the REST interface, or you can use the LimaCharlie Python SDK/CLI's [query interface](https://github.com/refractionPOINT/python-limacharlie/blob/master/limacharlie/Query.py): `limacharlie query --help`.

### Python CLI

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a friendly way to replay data, and to do so across larger datasets by automatically splitting up your query into multiple queries that can run in parallel.

Sample command line to query one sensor:

```
limacharlie-replay --sid 9cbed57a-6d6a-4af0-b881-803a99b177d9 --start 1556568500 --end 1556568600 --rule-content ./test_rule.txt
```

Sample command line to query an entire organization:

```
limacharlie-replay --entire-org --start 1555359000 --end 1556568600 --rule-name my-rule-name
```

If specifying a rule as content with the `--rule-content`, the format should be in `JSON` or `YAML` like:

```
detect:
  event: DNS_REQUEST
  op: is
  path: event/DOMAIN_NAME
  value: www.dilbert.com
respond:
  - action: report
    name: dilbert-is-here
```

Instead of specifying the `--entire-org` or `--sid` flags, you may use events from a local file via the `--events` flag.

We invite you to look at the command line usage itself, as the tool evolves.

### REST API

The Replay API is available to all DataCenter locations using a per-location URL. To get the appropriate URL for your organization, use the REST endpoint to retrieve the URLs found [here](https://api.limacharlie.io/static/swagger/#/Organizations/getOrgURLs) named `replay`.

Having per-location URLs will allow us to guarantee that processing occurs within the geographical area you chose. Currently, some locations are NOT guaranteed to be in the same area due to the fact we are using the Google Cloud Run product which is not available globally. For these cases, processing is currently done in the United States, but as soon as it becomes available in your area, the processing will be moved transparently.

Authentication to this API works with the same JWTs as the main limacharlie.io API.

For this example, we will use the experimental datacenter's URL:

```
https://0651b4f82df0a29c.replay.limacharlie.io/
```

The API mainly works on a per-sensor basis, on a limited amount of time. Replaying for multiple sensors (or entire org), or longer time period is done through multiple parallel API calls. This multiplexing is taken care of by the Python CLI above.

To query Replay, do a `POST` with a `Content-Type` header of `application-json` and with a JSON body like:

```
{
  "oid": "",             // OID this query relates to
  "rule_source": {       // rule source information (use one of "rule_name" or "rule")
    "rule_name": "",     // pre-existing rule name to run
    "namespace": "", // default: general namespace, can also be "managed" and "service"
    "rule": {            // literal rule to run
      "detect": {},
      "respond": []
    }
  },
  "event_source": {      // event source information (use one of "sensor_events" or "events")
    "sensor_events": {   // use historical events from sensors
      "sid": "",         // sensor id to replay from, or entire org if empty
      "start_time": 0,   // start second epoch time to replay from
      "end_time": 0      // end second epoch time to replay to
    },
    "events": [{}]       // literal list of events to replay
    "stream": "" // defaults to events, can also be "audit" or "detect"
  },
  "limit_event": 0,      // optional approximate number of events to process
  "limit_eval": 0,       // optional approximate number of operator evaluations to perform
  "trace": false,        // optional, if true add trace information to response, VERY VERBOSE
  "is_dry_run": false,   // optional, if true, an estimate of the total cost of the query will be returned
  "query": ""            // optional alternative way to describe a replay query as a LimaCharlie Query Language (LCQL) query.
}
```

Like the other endpoints you can also submit a `rule_name` in the URL query if you want to use an existing organization rule.

You may also specify a `limit_event` and `limit_eval` parameter as integers. They will limit the number of events evaluated and the number of rule evaluations performed (approximately). If the limits are reached, the response will contain an item named `limit_eval_reached: true` and `limit_event_reached: true`.

Finally, you may also set `trace` to `true` in the request to receive a detailed trace of the rule evaluation. This is useful in the development of new rules to find where rules are failing.

## Billing

The Replay service is billed on a per event evaluated.

---

# Response Actions

## Overview

Actions in LimaCharlie Detection & Response (D&R) rules define what happens after a detection is triggered. Common actions include generating reports, tagging sensors, isolating networks, and the frequently used `task` action, which sends commands to an Endpoint Agent to interrogate or take action on the endpoint. This is useful for tasks like gathering system information or isolating a compromised endpoint. Suppression settings manage repetitive alerts by limiting action frequency, ensuring efficient automation and response workflows.

> For more information on how to use Actions, read [Detection & Response rules](/v2/docs/detection-and-response).

## Suppression

Suppression is valuable to help manage repetitive or noisy alerts.

### Reduce Frequency

In some cases, you may want to limit the number of times a specific Action is executed over a certain period of time. You can achieve this through `suppression`. This feature is supported in every Actions.

A suppression descriptor can be added to an Action like:

```
- action: report
  name: evil-process-detected
  suppression:
    max_count: 1
    period: 1h
    is_global: true
    keys:
      - '{{ .event.FILE_PATH }}'
      - 'evil-process-detected'
```

The above example means that the `evil-process-detected` detection will be generated up to once per hour per `FILE_PATH`. Beyond the first `report` with a given `FILE_PATH`, during the one hour period, new `report` actions from this rule will be skipped.

The `is_global: true` means that the suppression should operate globally within the Org (tenant), if the value was `false`, the suppression would be scoped per-Sensor.

The `keys` parameter is a list of strings that support [templating](/v2/docs/template-strings-and-transforms). Together, the unique combination of values of all those strings (ANDed) will be the uniqueness key this suppression rule uses. By adding to the keys the `{{ .event.FILE_PATH }}` template, we indicate that the `FILE_PATH` of the event generating this `report` is part of the key, while the constant string `evil-process detected` is just a convenient way for us to specify a value related to this specific detection. If the `evil process-detected` component of the key was not specified, then *all* actions that also just specify the `{{ .event.FILE_PATH }}` would be contained in this suppression. This means that using `is_global: true` and a complex key set, it is possible to suppress some actions across multiple Actions across multiple D&R rules.

> Supported Time Period Formats
>
> LimaCharlie supports the following formats for time periods: **ns**, **us** (or **s**, both are accepted), **ms**, **s**, **m**, **h** (nanoseconds, microseconds, milliseconds, seconds, minutes, and hours, respectively)

### Threshold Activation

The other way to use suppression is using the `min_count` parameter. When set, the specific action will be suppressed until `min_count` number of activations have been received in that period.

Here's an example of this:

```
- action: report
  name: high-alerts
  suppression:
    min_count: 3
    max_count: 3
    period: 24h
```

The above example means the `high-alerts` detection will be generated once per hour but only after the rule the action belongs to has matched 3 times within that period.

This could be useful if you wanted to create higher order alerts that trigger a different type of detection, or send a page alert to a SOC, when more than X alerts occurred on a single host per period.

> Note: Both `min_count` and `max_count` must be specified when setting a threshold.

### Variable Count

It is also possible to increment a suppression by a value that's not one (`1`). This is achieved using the `count_path` parameter, which is a path (like `event/record/v`) pointing to an integer that should be used to increment the suppression counter.

This is useful for things like billing alerts, where we set a threshold activation (meaning "alert me if above X") where the threshold is reached by increments of billable values.

Here's an example of this:

```
detect:
    event: billing_record
    op: is
    path: event/record/k
    target: billing
    value: ext-strelka:bytes_scanned

respond:
    - action: report
      name: strelka-bytes-reached
      suppression:
        count_path: event/record/v
        is_global: true
        keys:
          - strelka-bytes-usage
        max_count: 1048576
        min_count: 1048576
        period: 24h
```

The above will alert (generate a detection in this case) when 1MB (1024 x 1024 x 1) of bytes have been billed by the Strelka Extension based on the `bytes_scanned` SKU, per 24h.

It does so by incrementing the suppression counter by the billed value (found in `event/record/v`), resetting after 24h, and if the value of 1MB is reached, alert once and only once.

## Available Actions

Actions allow you to specify "what" happens after a detection is found.

### add tag, remove tag

```
- action: add tag
  tag: vip
  entire_device: false # defaults to false
  ttl: 30 # optional
```

Adds or removes Tags on the sensor.

#### Optional Parameters

The `add tag` action can optionally take a `ttl` parameter that is a number of seconds the tag should remain applied to the sensor.

The `add tag` action can optionally have the `entire_device` parameter set to `true`. When enabled, the new tag will apply to the entire Device ID, meaning that every sensor that shares this Device ID will have the tag applied (and relevant TTL). If a Device ID is unavailable for the sensor, it will still be tagged.

This can be used as a mechanism to synchronize and operate changes across an entire device. A D&R rule could detect a behavior and then tag all sensors on the device so they may act accordingly, like start doing full pcap.

For example, this would apply the `full_pcap` to all sensors on the device for 5 minutes:

```
- action: add tag
  tag: full_pcap
  ttl: 300
  entire_device: true
```

### add var, del var

Add or remove a value from the variables associated with a sensor.

```
- action: add var
  name: my-variable
  value: <<event/VOLUME_PATH>>
  ttl: 30 # optional
```

The `add var` action can optionally take a `ttl` parameter that is a number of seconds the variable should remain in state for the sensor.

### extension request

Perform an asynchronous request to an extension the Organization is subscribed to.

```
- action: extension request
  extension name: dumper # name of the extension
  extension action: dump # action to trigger
  extension request:     # request parameters
    sid: '{{ .routing.sid }}'
    pid: event.PROCESS_ID
```

The `extension request` parameters will vary depending on the extension (see the relevant extension's schema). The `extension request` parameter is a [transform](/v2/docs/template-strings-and-transforms).

You can also specify a `based on report: true` parameter. When true (defaults to false), the transform for the `extension request` will be based on the latest `report` action's report instead of the original event. This means you MUST have a `report` action *before* the `extension request`.

### isolate network

Isolates the sensor from the network in a persistent fashion (if the sensor/host reboots, it will remain isolated). Only works on platforms supporting the `segregate_network` [sensor command](/v2/docs/reference-endpoint-agent-commands#segregatenetwork).

```
- action: isolate network
```

When the network isolation feature is used, LimaCharlie will block connections to all destinations other than the LimaCharlie cloud (so that you can perform an investigation, take remediation actions, and then ultimately remove the isolation to resume normal network operation). The host will maintain internet connectivity to allow for you to perform those actions.

> The `segregate_network` command is stateless, so if the endpoint reboots, it will not be in effect. The isolate network command in D&R rules is stateful, so it sets a flag in the cloud to make sure the endpoint remains isolated even after reboots.

### seal

Seals the sensor in a persistent fashion (if the sensor/host reboots, it will remain sealed). Only works on platforms supporting the `seal` [sensor command](/v2/docs/reference-endpoint-agent-commands#seal).

```
- action: seal
```

Sealing a sensor enables tamper resistance, preventing direct modifications to the installed EDR.

> The `seal` command is stateless, so if the endpoint reboots, it will not be in effect. The seal command in D&R rules is stateful, so it sets a flag in the cloud to make sure the endpoint remains sealed even after reboots.

### unseal

Removes the seal status of a sensor that had it set using `seal`.

```
- action: unseal
```

### output

Forwards the matched event to an Output identified by `name` in the `tailored` [stream](/v2/docs/outputs).

This allows you to create highly granular Outputs for specific events.

The `name` parameter is the name of the Output.

Example:

```
- action: output
  name: my-output
```

### rejoin network

Removes the isolation status of a sensor that had it set using `isolate network`.

```
- action: rejoin network
```

### report

```
- action: report
  name: my-detection-name
  publish: true # defaults to true
  priority: 3   # optional
  metadata:     # optional & free-form
    author: Alice (alice@wonderland.com)
  detect_data:  # additional free-form field that can be used for extraction of specific elements
```

Reports the match as a detection. Think of it as an alert. Detections go a few places:

* The `detection` Output stream
* The organization's Detections page (if `insight` is enabled)
* The D&R rule engine, for chaining detections

The `name`, `metadata` and `detect_data` parameters support [string templates](/v2/docs/template-strings-and-transforms) like `detected {{ .cat }} on {{ .routing.hostname }}`, note that the context of the transform is the detection itself and not the original event, so you would refer to `.detect.event.USER_NAME` and not `.event.USER_NAME` for example.

The `metadata` is generally used to populate information about the rule, its author, remediation etc.

The `detect_data` is generally used to extract specific parts of the detected event into a known format that can be common across multiple detection, like extracting the `domain` or `hash` field for example.

#### Limiting Scope

There is a mechanism for limiting scope of a `report`, prefixing `name` with `__` (double underscore). This will cause the detection generated to be visible to chained D&R rules and Services, but the detection will *not* be sent to the Outputs for storage.

This is a useful mechanism to automate behavior using D&R rules without generating extra traffic that is not useful.

#### Optional Parameters

The `priority` parameter, if set, should be an integer. It will be added to the root of the detection report as `priority`.

The `metadata` parameter, if set, can include any data. It will be added to the root of the detection report as `detect_mtd`. This can be used to include information for internal use like reference numbers or URLs.

### task

```
- action: task
  command: history_dump
  investigation: susp-process-inv
```

Sends a task in the `command` parameter to the sensor that the event under evaluation is from.

An optional `investigation` parameter can be given to create a unique identifier for the task and any events emitted from the sensor as a result of the task.

The `command` parameter supports [string templates](/v2/docs/template-strings-and-transforms) like `artifact_get {{ .event.FILE_PATH }}`.

> To view all possible commands, see [Endpoint Agent Commands](/v2/docs/reference-endpoint-agent-commands)

### undelete sensor

Un-deletes a sensor that was previously deleted.

```
detect:
    target: deployment
    event: deleted_sensor
    op: is
    path: routing/event_type
    value: deleted_sensor
respond:
    - action: undelete sensor
```

This can be used in conjunction with the `deleted_sensor` event to allow sensors to rejoin the fleet.

### wait

Adds a delay (up to 1 minute) before running the next response action.

This can be useful if a previous response action needs to finish running (i.e. a command or payload run via `task`) before you can execute the next action.

> The `wait` action will block processing any events from that sensor for the specified duration of time. This is because D&R rules are run at wire-speed and in-order.

The `duration` parameter supports two types of values:

* A string describing a duration, like `5s` for 5 seconds or `10ms` for 10 milliseconds, as defined by [this function call](https://pkg.go.dev/time#ParseDuration).
* An integer representing a number of seconds.

Example:

```
- action: wait
  duration: 10s
```

and

```
- action: wait
  duration: 5
```

### add hive tag

Adds a tag to a Hive record. This can be used to mark some Hive records like D&R rules automatically.

```
- action: add hive tag
  hive name: dr-general
  record name: my-rule
  tag: high-hit
```

Unless the rule is not expected to hit often, you likely want to couple this with a `suppression` statement to avoid doing a lot of tagging of the same rules like:

```
- action: add hive tag
  hive name: dr-general
  record name: my-rule
  tag: high-hit
  suppression:
    max_count: 1
    period: 1h
    is_global: true
    keys:
      - 'high-hit'
      - 'hive-tag'
```

### remove hive tag

Removes a tag from a Hive record.

```
- action: remove hive tag
  hive name: dr-general
  record name: my-rule
  tag: high-hit
```

---

# SOC Prime Rules

To use SOC Prime rules in LimaCharlie, start by configuring lists in [SOC Prime](https://socprime.com/). You can learn how to do it [here](https://socprime.com/blog/enable-continuous-content-management-with-the-soc-prime-platform/).

After the lists have been configured, you can finish the configuration in LimaCharlie. Note that currently the SOC Prime API is not available for free users. It is available only for paid users or if they requested a trial.

First, enable the `socprime` add-on on the LimaCharlie marketplace.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2855%29.png)

Then, navigate to the Integrations page in your Organization, enter the SOC Prime Key & click `Update`.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2856%29.png)

When the Key is saved, you will get the ability to select the SOC Prime content lists you want to have populated in LimaCharlie as detection & response rules. After selecting the lists & clicking `Update`, you are all set to start receiving detections based on the SOC Prime lists.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2857%29.png)

A detection that comes from the SOC Prime Lists, will have `socprime` listed as a detection author.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2858%29.png)

Note that adding a new rule to a SOC Prime content list that is enabled in LC will see the new rule be applied during next sync (LimaCharlie syncs the SOC Prime rules every 3 hours).

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Soteria AWS Rules

Soteria's AWS ruleset provides coverage across multiple AWS telemetry streams, including:

* [AWS CloudTrail](https://aws.amazon.com/cloudtrail/)
* [AWS GuardDuty](https://aws.amazon.com/guardduty/)

> **Data access**
>
> Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

To leverage detection logic provided by the ruleset:

1. Subscribe your tenant to the Soteria AWS [ruleset extension](https://app.limacharlie.io/add-ons/extension-detail/soteria-rules-aws).
2. Subscribe your tenant to [tor](/v2/docs/ext-lookup-manager) lookup (provided at no cost).
3. Configure [AWS CloudTrail](/v2/docs/adapter-types-aws-cloudtrail) and [AWS GuardDuty](/v2/docs/adapter-types-aws-guardduty) adapters to start collecting AWS audit logs.

## Enabling Soteria's AWS Rules

Soteria's AWS rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's AWS ruleset, navigate to the **Extensions** section of the **Add-On Marketplace** and search for Soteria. You can also directly select `soteria-rules-aws`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to **soteria-rules-aws** and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

## Amazon Web Services

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Soteria AWS Rules

Soteria's AWS ruleset provides coverage across multiple AWS telemetry streams, including:

* [AWS CloudTrail](https://aws.amazon.com/cloudtrail/)
* [AWS GuardDuty](https://aws.amazon.com/guardduty/)

> **Data access**
>
> Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

To leverage detection logic provided by the ruleset:

1. Subscribe your tenant to the Soteria AWS [ruleset extension](https://app.limacharlie.io/add-ons/extension-detail/soteria-rules-aws).
2. Subscribe your tenant to [tor](/v2/docs/ext-lookup-manager) lookup (provided at no cost).
3. Configure [AWS CloudTrail](/v2/docs/adapter-types-aws-cloudtrail) and [AWS GuardDuty](/v2/docs/adapter-types-aws-guardduty) adapters to start collecting AWS audit logs.

## Enabling Soteria's AWS Rules

Soteria's AWS rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's AWS ruleset, navigate to the **Extensions** section of the **Add-On Marketplace** and search for Soteria. You can also directly select `soteria-rules-aws`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to **soteria-rules-aws** and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-aws-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

## Amazon Web Services

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Soteria M365 Rules

Soteria's O365 ruleset provides coverage across O365 (aka M365) telemetry streams. The ruleset is designed for in-depth analysis of the Office 365 ecosystem which includes:

* Teams
* Word
* Excel
* PowerPoint
* Outlook
* OneDrive
* ...and other productivity applications.

## Data access

Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

To leverage detection logic provided by the ruleset:

1. Subscribe your tenant to the [Soteria Office 365 ruleset extension](https://app.limacharlie.io/add-ons/extension-detail/soteria-rules-o365)
2. Subscribe your tenant to [tor](https://app.limacharlie.io/add-ons/detail/tor-ips) lookup (provided at no cost).
3. Configure Office 365 Sensor to start collecting [Office 365 audit logs](/v2/docs/adapter-types-microsoft-365).

## Enabling Soteria's O365 Rules

Soteria's O365 rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's O365 ruleset, navigate to the Extensions section of the Add-On Marketplace and search for Soteria. You can also directly select `soteria-rules-o365`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to Soteria O365 rules and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

---

# Soteria M365 Rules

Soteria's O365 ruleset provides coverage across O365 (aka M365) telemetry streams. The ruleset is designed for in-depth analysis of the Office 365 ecosystem which includes:

* Teams
* Word
* Excel
* PowerPoint
* Outlook
* OneDrive
* ...and other productivity applications.

## Data access

Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

To leverage detection logic provided by the ruleset:

1. Subscribe your tenant to the [Soteria Office 365 ruleset extension](https://app.limacharlie.io/add-ons/extension-detail/soteria-rules-o365)
2. Subscribe your tenant to [tor](https://app.limacharlie.io/add-ons/detail/tor-ips) lookup (provided at no cost).
3. Configure Office 365 Sensor to start collecting [Office 365 audit logs](/v2/docs/adapter-types-microsoft-365).

## Enabling Soteria's O365 Rules

Soteria's O365 rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's O365 ruleset, navigate to the Extensions section of the Add-On Marketplace and search for Soteria. You can also directly select `soteria-rules-o365`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to Soteria O365 rules and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-o365-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Soteria Rules

[Soteria](https://soteria.io/) is a US-based MSSP and longtime MSSP, and has built a wealth of experience writing and managing LimaCharlie [detection & response](/v2/docs/detection-and-response) rules. With one click, you can apply their rules to your environment. When Soteria updates the rules for their customers, you will get those updates in real time as well. Soteria provides their rules in the form of managed rulesets, available via the [Add-ons Marketplace](https://app.limacharlie.io/add-ons/category/rulesets).

> **Note:** Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

Soteria provides the following rulesets:

* [AWS Rules](/v2/docs/soteria-aws-rules)
* [EDR Rules](/v2/docs/soteria-edr-rules)
* [M365/O365 Rules](/v2/docs/soteria-m365-rules)

Soteria rules are available for a fee, either per-Sensor or per-Organization, which can be found on their respective pages within the Add-ons Marketplace.

## Definitions

**Managed Security Services Provider**

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Stateful Rules

## Overview

> It's recommended to first read [Detection & Response rules](/v2/docs/detection-and-response) before diving into stateful rules.

In LimaCharlie, a Stateful Rule tracks and remembers the state of past events to make decisions based on historical context. Unlike stateless rules, which evaluate events in isolation, stateful rules can detect patterns over time, such as multiple failed logins within an hour. This enables more complex and accurate detection, allowing users to trigger actions only when specific conditions are met across multiple events or timeframes.

Events in LimaCharlie have well-defined relationships to one another using `routing/this`, `routing/parent`, `routing/target`, and can even be implicitly related by occurring in a similar timeframe. The relation context can be useful for writing more complex rules.

These are called "stateful" rules.

## Detecting Children / Descendants

To detect events in a tree you can use the following parameters:

* `with child`: matches children of the initial event
* `with descendant`: matches descendants (children, grandchildren, etc.) of the initial event

Aside from how deep they match, the `with child` and `with descendant` parameters operate identically: they declare a nested stateful rule.

For example, let's detect a `cmd.exe` process spawning a `calc.exe` process:

```
# Detect initial event
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: cmd.exe
case sensitive: false
with child: # Wait for child matching this nested rule
  op: ends with
  event: NEW_PROCESS
  path: event/FILE_PATH
  value: calc.exe
  case sensitive: false
```

Simply put, this will detect:

```
cmd.exe --> calc.exe
```

Because it uses `with child` it will not detect:

```
cmd.exe --> firefox.exe --> calc.exe
```

To do that, we could use `with descendant` instead.

## Detecting Proximal Events

To detect repetition of events close together on the same Sensor, we can use `with events`.

The `with events` parameter functions very similarly to `with child` and `with descendant`: it declares a nested stateful rule.

For example, let's detect a scenario where `5` bad login attempts occur within `60` seconds.

```
event: WEL
op: is windows
with events:
  event: WEL
  op: is
  path: event/EVENT/System/EventID
  value: '4625'
  count: 5
  within: 60
```

The top-level rule filters down meaningful events to `WEL` ones sent from Windows sensors using the `is windows` operator, and then it declares a stateful rule inside `with events`. It uses `count` and `within` to declare a suitable timespan to evaluate matching events.

## Stateful Rules

Stateful rules  the rules declared within `with child`, `with descendant` or `with events`  have full range. They can do anything a normal rule might do, including declaring nested stateful rules or using `and`/`or` operators to write more complex rules.

Here's a stateful rule that uses `and` to detect a specific combination of child events:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false
```

The above example is looking for an `outlook.exe` process that spawns a `chrome.exe` process and drops a `.ps1` (powershell) file to disk. Like this:

```
outlook.exe
|--+--> chrome.exe
|--+--> .ps1 file
```

### Counting Events

Rules declared using `with child` or `with descendant` also have the ability to use `count` and `within` to help scope the events it will statefully match.

For example, a rule that matches on Outlook writing 5 new `.ps1` documents within 60 seconds:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: ends with
  event: NEW_DOCUMENT
  path: event/FILE_PATH
  value: .ps1
  case sensitive: false
  count: 5
  within: 60
```

### Choosing Event to Report

A reported detection will include a copy of the event that was detected. When writing detections that match multiple events, the default behavior will be to include a copy of the initial parent event.

In many cases it's more desirable to get the latest event in the chain instead. For this, there's a `report latest event: true` flag that can be set. Piggy-backing on the earlier example:

```
# Detection
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
report latest event: true
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false

# Response
- action: report
  name: Outlook Spawning Chrome & Powershell
```

The event returned in the detection will be either the `chrome.exe` `NEW_PROCESS` event or the `.ps1` `NEW_DOCUMENT` event, whichever was last. Without `report latest event: true` being set, it would default to including the `outlook.exe` `NEW PROCESS` event.

### Flipping back to stateless

Since all operators under the `with child` and `with descentant` are operating in stateful mode (meaning all the nodes don't have to match a single event, but can match over multiple events), sometimes you want a operator and the operators underneath to flip back to stateless mode where they must match a single event. You can achieve this by setting `is stateless: true` in the operator like:

```
# Detection
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
report latest event: true
with child:
  op: and
  is stateless: true
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: evil.exe
      case sensitive: false
    - op: contains
      event: COMMAND_LINE
      path: event/FILE_PATH
      value: something-else
      case sensitive: false
```

## Caveats

### Testing Stateful Rules

Stateful rules are forward-looking only and changing a rule will reset its state.

Practically speaking, this means that if you change a rule that detects `excel.exe -> cmd.exe`, `excel.exe` will need to be relaunched while the updated rule is running for it to then begin watching for `cmd.exe`.

### Using Events in Actions

Using `report` to report a detection works according to the [Choosing Event to Report](#choosing-event-to-report) section earlier. Other actions have a subtle difference: they will *always* observe the latest event in the chain.

Consider the `excel.exe -> cmd.exe` example. The `cmd.exe` event will be referenced inside the response action if using lookbacks (i.e. `<<routing/this>>`). If we wanted to end the `excel.exe` process (and its descendants), we would write a `task` that references the parent of the current event (`cmd.exe`):

```
- action: task
  command: deny_tree <<routing/parent>>
```

---

# Stateful Rules

## Overview

> It's recommended to first read [Detection & Response rules](/v2/docs/detection-and-response) before diving into stateful rules.

In LimaCharlie, a Stateful Rule tracks and remembers the state of past events to make decisions based on historical context. Unlike stateless rules, which evaluate events in isolation, stateful rules can detect patterns over time, such as multiple failed logins within an hour. This enables more complex and accurate detection, allowing users to trigger actions only when specific conditions are met across multiple events or timeframes.

Events in LimaCharlie have well-defined relationships to one another using `routing/this`, `routing/parent`, `routing/target`, and can even be implicitly related by occurring in a similar timeframe. The relation context can be useful for writing more complex rules.

These are called "stateful" rules.

## Detecting Children / Descendants

To detect events in a tree you can use the following parameters:

* `with child`: matches children of the initial event
* `with descendant`: matches descendants (children, grandchildren, etc.) of the initial event

Aside from how deep they match, the `with child` and `with descendant` parameters operate identically: they declare a nested stateful rule.

For example, let's detect a `cmd.exe` process spawning a `calc.exe` process:

```
# Detect initial event
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: cmd.exe
case sensitive: false
with child: # Wait for child matching this nested rule
  op: ends with
  event: NEW_PROCESS
  path: event/FILE_PATH
  value: calc.exe
  case sensitive: false
```

Simply put, this will detect:

```
cmd.exe --> calc.exe
```

Because it uses `with child` it will not detect:

```
cmd.exe --> firefox.exe --> calc.exe
```

To do that, we could use `with descendant` instead.

## Detecting Proximal Events

To detect repetition of events close together on the same Sensor, we can use `with events`.

The `with events` parameter functions very similarly to `with child` and `with descendant`: it declares a nested stateful rule.

For example, let's detect a scenario where `5` bad login attempts occur within `60` seconds.

```
event: WEL
op: is windows
with events:
  event: WEL
  op: is
  path: event/EVENT/System/EventID
  value: '4625'
  count: 5
  within: 60
```

The top-level rule filters down meaningful events to `WEL` ones sent from Windows sensors using the `is windows` operator, and then it declares a stateful rule inside `with events`. It uses `count` and `within` to declare a suitable timespan to evaluate matching events.

## Stateful Rules

Stateful rules  the rules declared within `with child`, `with descendant` or `with events`  have full range. They can do anything a normal rule might do, including declaring nested stateful rules or using `and`/`or` operators to write more complex rules.

Here's a stateful rule that uses `and` to detect a specific combination of child events:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false
```

The above example is looking for an `outlook.exe` process that spawns a `chrome.exe` process and drops a `.ps1` (powershell) file to disk. Like this:

```
outlook.exe
|--+--> chrome.exe
|--+--> .ps1 file
```

### Counting Events

Rules declared using `with child` or `with descendant` also have the ability to use `count` and `within` to help scope the events it will statefully match.

For example, a rule that matches on Outlook writing 5 new `.ps1` documents within 60 seconds:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: ends with
  event: NEW_DOCUMENT
  path: event/FILE_PATH
  value: .ps1
  case sensitive: false
  count: 5
  within: 60
```

### Choosing Event to Report

A reported detection will include a copy of the event that was detected. When writing detections that match multiple events, the default behavior will be to include a copy of the initial parent event.

In many cases it's more desirable to get the latest event in the chain instead. For this, there's a `report latest event: true` flag that can be set. Piggy-backing on the earlier example:

```
# Detection
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
report latest event: true
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false

# Response
- action: report
  name: Outlook Spawning Chrome & Powershell
```

The event returned in the detection will be either the `chrome.exe` `NEW_PROCESS` event or the `.ps1` `NEW_DOCUMENT` event, whichever was last. Without `report latest event: true` being set, it would default to including the `outlook.exe` `NEW PROCESS` event.

### Flipping back to stateless

Since all operators under the `with child` and `with descendant` are operating in stateful mode (meaning all the nodes don't have to match a single event, but can match over multiple events), sometimes you want an operator and the operators underneath to flip back to stateless mode where they must match a single event. You can achieve this by setting `is stateless: true` in the operator like:

```
# Detection
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
report latest event: true
with child:
  op: and
  is stateless: true
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: evil.exe
      case sensitive: false
    - op: contains
      event: COMMAND_LINE
      path: event/FILE_PATH
      value: something-else
      case sensitive: false
```

## Caveats

### Testing Stateful Rules

Stateful rules are forward-looking only and changing a rule will reset its state.

Practically speaking, this means that if you change a rule that detects `excel.exe -> cmd.exe`, `excel.exe` will need to be relaunched while the updated rule is running for it to then begin watching for `cmd.exe`.

### Using Events in Actions

Using `report` to report a detection works according to the [Choosing Event to Report](#choosing-event-to-report) section earlier. Other actions have a subtle difference: they will *always* observe the latest event in the chain.

Consider the `excel.exe -> cmd.exe` example. The `cmd.exe` event will be referenced inside the response action if using lookbacks (i.e. `<<routing/this>>`). If we wanted to end the `excel.exe` process (and its descendants), we would write a `task` that references the parent of the current event (`cmd.exe`):

```
- action: task
  command: deny_tree <<routing/parent>>
```

---

# Writing and Testing Rules

Detection & Response (D&R) Rules are similar to Google Cloud Functions or AWS Lambda. They allow you to push D&R rules to the LimaCharlie cloud where the rules will be applied in real-time to data coming from the sensors.

D&R rules can also be applied to Artifact Collection, but for now we will focus on the simple case where it is applied to Sensor events.

For a full list of all rule operators and detailed documentation see the Detection and Response section.

## Life of a Rule

D&R rules are generally applied on a per-event basis. When the rule is applied, the "detection" component of the rule is processed to determine if it matches. If there is a match, the "response" component is applied.

The detection is processed one step at a time, starting at the root of the detection. If the root matches, the rule is considered to be matching.

The detection component is composed of "nodes", where each node has an operator describing the logical evaluation. Most operators are simple, like `is`, `starts with` etc. These simple nodes can be combined with Boolean (true/false) logic using the `and` and `or` operators, which themselves reference a series of nodes. The `and` node matches if all the sub-nodes match, while the `or` node matches if any one of the sub-nodes matches.

When evaluating an `or`, as soon as the first matching sub-node is found, the rest of the sub-nodes are skipped since they will have no impact on the final matching state of the "or". Similarly, failure of a sub-node in an "and" node will immediately terminate its evaluation.

If the "detection" component is matched, then the "response" evaluation begins.

The "response" component is a list of actions that should be taken. When an action refers to a sensor, that sensor is assumed to be the sensor the event being evaluated is coming from.

The best general strategy for D&R rules is to put the parts of the rule most likely to eliminate the event at the beginning of the rule, so that LC may move on to the next event as quickly as possible.

## Introduction

### Goal

The goal of this code lab will be to create a D&R rule to detect the MITRE ATT&CK framework Control Panel Items execution.

### Services Used

This code lab will use the Replay service to validate and test the rule prior to pushing it to production.

## Setup and Requirements

This code lab assumes you have access to a Linux host (MacOS terminal with `brew`). This code lab also assumes you have "owner" access to an LC Organization. If you don't have one already, create one, this code lab is compatible with the free tier that comes with all organizations.

### Install CLI

Interacting with LC can always be done via the web app but day to day operations and automation can be done via the Command Line Interface (CLI). This will make following this code lab easier.

Install the CLI: `pip install limacharlie --user`. If you don't have `pip` installed, install it, the exact instructions will depend on your Linux distribution.

### Create REST API Key

We need to create an API key we can use in the CLI to authenticate with LC. To do so, go to the REST API section of the web app.

1. In the REST API section, click the "+" button in the top right of the page.
2. Give your key a name.
3. For simplicity, click the "Select All" button to enable all permissions. Obviously this would not be a recommended in a production environment.
4. Click the copy-to-clipboard button for the new key and take note of it (pasting it in a temporary text note for example).
5. Back on the REST API page, copy the "Organization ID" at the top of the page and keep note of it like the API key in the previous step.

The Organization ID (OID) identifies uniquely your organization while the API key grants specific permissions to this organization.

### Login to the CLI

Back in your terminal, log in with your credentials: `limacharlie login`.

1. When asked for the Organization ID, paste your OID from the previous step.
2. When asked for a name for this access, you can leave it blank to set the default credentials.
3. When asked for the secret API key, enter the key you got from the previous step.

You're done! If you issue a `limacharlie dr list` you should not get any errors.

## Draft Rule

To draft our rule, open your preferred text editor and save the rule to a file, we'll call it `T1196.rule`. The format of a rule is YAML, if you are unfamiliar with it, there is benefit to spending a few minutes getting familiar. It won't take long as it is not overly complex.

For our rules based on the T1196 technique, we need to apply the following constraints:

1. It only applies to Windows.
2. The event is a module (DLL for example on Windows) loading.
3. The module loading ends with `.cpl` (control panel extension).
4. The module is loading from outside of the `C:\windows\` directory.

LC supports a lot of different event types, this means that the first thing we should strive to do to try to make the rule fail as quickly as possible is to filter all events we don't care about.

In this case, we only care about CODE_IDENTITY events. We also know that our rule will use more than one criteria, and those criteria will be AND-ed together because we only want to match when they all match.

```yaml
op: and
event: CODE_IDENTITY
rules:
  -
```

The above sets up the criteria #2 preceding it, with the AND-ing that will follow. Since the AND is at the top of our rule, and it has an `event:` clause, it will ensure that any event processed by this rule but is NOT a `CODE_IDENTITY` event will be skipped over right away.

Next, we should look at the other criteria, and add them to the `rules:` list, which are all the sub-nodes that will be AND-ed together.

Criteria #1 was to limit to Windows, that's easy:

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  -
```

Next up is criteria #3 and #4. Both of those can be determined using the `FILE_PATH` component of the `CODE_IDENTITY` event. If you are unsure what those events look like, the best way to get a positive confirmation of the structure is simply to open the Historic View, start a new process on that specific host and look for the relevant event. If we were to do this on a Windows host, we'd get an example like this one:

```json
{
    "routing": {
        "parent": "...",
        "this": "...",
        "hostname": "WIN-...",
        "event_type": "CODE_IDENTITY",
        "event_time": 1567438408423,
        "ext_ip": "XXX.176.XX.148",
        "event_id": "11111111-1111-1111-1111-111111111111",
        "oid": "11111111-1111-1111-1111-111111111111",
        "plat": 268435456,
        "iid": "11111111-1111-1111-1111-111111111111",
        "sid": "11111111-1111-1111-1111-111111111111",
        "int_ip": "172.XX.223.XXX",
        "arch": 2,
        "tags": [
            "..."
        ],
        "moduleid": 2
    },
    "ts": "2019-09-02 15:33:28",
    "event": {
        "HASH_MD5": "7812c2c0a46d1f0a1cf8f2b23cd67341",
        "HASH": "d1d59eefe1aeea20d25a848c2c4ee4ffa93becaa3089745253f9131aedc48515",
        "ERROR": 0,
        "FILE_INFO": "10.0.17134.1",
        "HASH_SHA1": "000067ac70f0e38f46ce7f93923c6f5f06ecef7b",
        "SIGNATURE": {
            "FILE_CERT_IS_VERIFIED_LOCAL": 1,
            "CERT_SUBJECT": "C=US, S=Washington, L=Redmond, O=Microsoft Corporation, CN=Microsoft Windows",
            "FILE_PATH": "C:\\Windows\\System32\\setupcln.dll",
            "FILE_IS_SIGNED": 1,
            "CERT_ISSUER": "C=US, S=Washington, L=Redmond, O=Microsoft Corporation, CN=Microsoft Windows Production PCA 2011"
        },
        "FILE_PATH": "C:\\Windows\\System32\\setupcln.dll"
    }
}
```

This means what we want is to apply rules to the `event/FILE_PATH`. First part, #3 is easy, we just want to test for the `event/FILE_PATH` ends in `.cpl`, we can do this using the `ends with` operator.

Most operators will use a `path` and a `value`. General convention is the `path` describes how to get to a value we want to compare within the event. So `event/FILE_PATH` says "starting in the `event` then get the `FILE_PATH`. The `value` generally represents a value we want to compare to the element found in the `path`. How it is compared depends on the operator.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
```

That was easy, but we're missing a critical component! By default, D&R rules operate in a case sensitive mode. This means that the above node we added will match `.cpl` but will NOT match `.cPl`. To fix this, we just add the `case sensitive: false` statement.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
    case sensitive: false
  -
```

Finally, we want to make sure the `event/FILE_PATH` is NOT in the `windows` directory. To do this, we will use a regular expression with a `matches` operator. But in this case, we want to EXCLUDE the paths that include the `windows` directory, so we want to "invert" the match. We can do this with the `not: true` statement.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
    case sensitive: false
  - op: matches
    path: event/FILE_PATH
    re: ^.\:\\windows\\
    case sensitive: false
    not: true
```

Here we go, we're done drafting our first rule.

## Validate Rule

What we want to do now is validate the rule. If the rule validates, it doesn't mean it's correct, it just means that the structure is correct, the operators we use are known, etc. It's the first pass at detecting possible formatting issues or typos.

To validate, we will simply leverage the Replay service. This service can be used to test rules or replay historical events against a rule. In this case however, we just want to start by validating.

Up until now we focused on the "detection" part of the rule. But a full rule also contains a "response" component. So before we proceed, we'll add this structure. For a response, we will use a simple `action: report`. The `report` creates a "detection" (alert).

```yaml
detect:
  op: and
  event: CODE_IDENTITY
  rules:
    - op: is windows
    - op: ends with
      path: event/FILE_PATH
      value: .cpl
      case sensitive: false
    - op: matches
      path: event/FILE_PATH
      re: ^.\:\\windows\\
      case sensitive: false
      not: true
respond:
  - action: report
    name: T1196
```

Now validate: `limacharlie replay --validate --rule-content T1196.rule`

After a few seconds, you should see a response with `success: true` if the rule validates properly.

## Test rule

### Test Plan

Now that we know our rule is generally sound, we need to test it against some events.

Our test plan will take the following approach:

1. Test a positive (a `.cpl` loading outside of `windows`).
2. Test a negative for the major criteria:
   1. Test a non-`.cpl` loading outside of `windows` does not match.
   2. Test a `.cpl` loading within `windows` does not match.
3. Test on historical data.

With this plan, #1 and #2 lend themselves well to unit tests while #3 can be done more holistically by using Replay to run historical events through the rule and evaluate if there are any false positives.

This may be excessive for you, or for certain rules which are very simple, we leave that evaluation to you. For the sake of this code lab, we will do a light version to demonstrate how to do tests.

### Testing a Single Event

To test #1 and #2, let's just create some synthetic events. It's always better to use real-world samples, but we'll leave that up to you.

Take the event sample we had in the "Draft Rule" section and copy it to two new files we will name `positive.json`, `negative-1.json` and `negative-2.json`.

Modify the `positive.json` file by renaming the `FILE_PATH` at the bottom from `"C:\\Windows\\System32\\setupcln.dll"` to `"C:\\temp\\System32\\setupcln.cpl"` so that the event now describes a `.cpl` loading in the `temp` directory, which we should detect.

Then modify the `negative-1.json` file by changing the same `.dll` to `.cpl`. This should NOT match because the path is still in the `windows` directory.

Then modify the `negative-2.json` file by changing the `windows` directory to `temp`. This should still NOT match because it's not a `.cpl`.

Now we can run our 3 samples against the rule using Replay,

`limacharlie replay --rule-content T1196.rule --events positive.json` should output a result indicating the event matched (by actioning the `report`) like:

```json
{
  "num_evals": 4,
  "eval_time": 0.00020599365234375,
  "num_events": 1,
  "responses": [
    {
      "report": {
        "source": "11111111-1111-1111-1111-111111111111.11111111-1111-1111-1111-111111111111.11111111-1111-1111-1111-111111111111.10000000.2",
        "routing": {
...
```

`limacharlie replay --rule-content T1196.rule --events negative-1.json` should output a result indicating the event did NOT match like:

```json
{
  "num_evals": 4,
  "eval_time": 0.00011777877807617188,
  "num_events": 1,
  "responses": [],
  "errors": []
}
```

`limacharlie replay --rule-content T1196.rule --events negative-2.json` be the same as `negative-1.json`.

### Testing Historical Data

The final test is to run the rule against historical data. If you are not using an organization on the free tier, note that the Replay API is billed on usage. In the following step we will run against all historical data from the organization, so if your organization is not on the free tier and it is large, there may be non-trivial costs associated.

Running our rule against the last week of data is simple:

`limacharlie replay --rule-content T1196.rule --entire-org --last-seconds 604800`

No matches should look like that:

```json
{
  "num_evals": 67354,
  "eval_time": 1107.2150619029999,
  "num_events": 222938,
  "responses": [],
  "errors": []
}
```

### Moving to Unit Tests

Once your rule is done and you've evaluated various events for matches, you can move these to D&R Rules Unit Tests so that the tests are run during rule update.

## Publish Rule

Now is the time to push the new rule to production, the easy part.

Simply run `limacharlie dr add --rule-name T1196 --rule-file T1196.rule` and confirm it is operational by running `limacharlie dr list`.

---

# Writing and Testing Rules

Detection & Response (D&R) Rules are similar to Google Cloud Functions or AWS Lambda. They allow you to push D&R rules to the LimaCharlie cloud where the rules will be applied in real-time to data coming from the sensors.

D&R rules can also be applied to Artifact Collection, but for now we will focus on the simple case where it is applied to Sensor events.

For a full list of all rule operators and detailed documentation see the Detection and Response section.

## Life of a Rule

D&R rules are generally applied on a per-event basis. When the rule is applied, the "detection" component of the rule is processed to determine if it matches. If there is a match, the "response" component is applied.

The detection is processed one step at a time, starting at the root of the detection. If the root matches, the rule is considered to be matching.

The detection component is composed of "nodes", where each node has an operator describing the logical evaluation. Most operators are simple, like `is`, `starts with` etc. These simple nodes can be combined with Boolean (true/false) logic using the `and` and `or` operators, which themselves reference a series of nodes. The `and` node matches if all the sub-nodes match, while the `or` node matches if any one of the sub-nodes matches.

When evaluating an `or`, as soon as the first matching sub-node is found, the rest of the sub-nodes are skipped since they will have no impact on the final matching state of the "or". Similarly, failure of a sub-node in an "and" node will immediately terminate its evaluation.

If the "detection" component is matched, then the "response" evaluation begins.

The "response" component is a list of actions that should be taken. When an action refers to a sensor, that sensor is assumed to be the sensor the event being evaluated is coming from.

The best general strategy for D&R rules is to put the parts of the rule most likely to eliminate the event at the beginning of the rule, so that LC may move on to the next event as quickly as possible.

## Introduction

### Goal

The goal of this code lab will be to create a D&R rule to detect the MITRE ATT&CK framework Control Panel Items execution.

### Services Used

This code lab will use the Replay service to validate and test the rule prior to pushing it to production.

## Setup and Requirements

This code lab assumes you have access to a Linux host (MacOS terminal with `brew`). This code lab also assumes you have "owner" access to an LC Organization. If you don't have one already, create one, this code lab is compatible with the free tier that comes with all organizations.

### Install CLI

Interacting with LC can always be done via the web app but day to day operations and automation can be done via the Command Line Interface (CLI). This will make following this code lab easier.

Install the CLI: `pip install limacharlie --user`. If you don't have `pip` installed, install it, the exact instructions will depend on your Linux distribution.

### Create REST API Key

We need to create an API key we can use in the CLI to authenticate with LC. To do so, go to the REST API section of the web app.

1. In the REST API section, click the "+" button in the top right of the page.
2. Give your key a name.
3. For simplicity, click the "Select All" button to enable all permissions. Obviously this would not be recommended in a production environment.
4. Click the copy-to-clipboard button for the new key and take note of it (pasting it in a temporary text note for example).
5. Back on the REST API page, copy the "Organization ID" at the top of the page and keep note of it like the API key in the previous step.

The Organization ID (OID) identifies uniquely your organization while the API key grants specific permissions to this organization.

### Login to the CLI

Back in your terminal, log in with your credentials: `limacharlie login`.

1. When asked for the Organization ID, paste your OID from the previous step.
2. When asked for a name for this access, you can leave it blank to set the default credentials.
3. When asked for the secret API key, enter the key you got from the previous step.

You're done! If you issue a `limacharlie dr list` you should not get any errors.

## Draft Rule

To draft our rule, open your preferred text editor and save the rule to a file, we'll call it `T1196.rule`. The format of a rule is YAML, if you are unfamiliar with it, there is benefit to spending a few minutes getting familiar. It won't take long as it is not overly complex.

For our rules based on the T1196 technique, we need to apply the following constraints:

1. It only applies to Windows.
2. The event is a module (DLL for example on Windows) loading.
3. The module loading ends with `.cpl` (control panel extension).
4. The module is loading from outside of the `C:\windows\` directory.

LC supports a lot of different event types, this means that the first thing we should strive to do to try to make the rule fail as quickly as possible is to filter all events we don't care about.

In this case, we only care about CODE_IDENTITY events. We also know that our rule will use more than one criteria, and those criteria will be AND-ed together because we only want to match when they all match.

```yaml
op: and
event: CODE_IDENTITY
rules:
  -
```

The above sets up the criteria #2 preceding it, with the AND-ing that will follow. Since the AND is at the top of our rule, and it has an `event:` clause, it will ensure that any event processed by this rule but is NOT a `CODE_IDENTITY` event will be skipped over right away.

Next, we should look at the other criteria, and add them to the `rules:` list, which are all the sub-nodes that will be AND-ed together.

Criteria #1 was to limit to Windows, that's easy:

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  -
```

Next up is criteria #3 and #4. Both of those can be determined using the `FILE_PATH` component of the `CODE_IDENTITY` event. If you are unsure what those events look like, the best way to get a positive confirmation of the structure is simply to open the Historic View, start a new process on that specific host and look for the relevant event. If we were to do this on a Windows host, we'd get an example like this one:

```json
{
    "routing": {
        "parent": "...",
        "this": "...",
        "hostname": "WIN-...",
        "event_type": "CODE_IDENTITY",
        "event_time": 1567438408423,
        "ext_ip": "XXX.176.XX.148",
        "event_id": "11111111-1111-1111-1111-111111111111",
        "oid": "11111111-1111-1111-1111-111111111111",
        "plat": 268435456,
        "iid": "11111111-1111-1111-1111-111111111111",
        "sid": "11111111-1111-1111-1111-111111111111",
        "int_ip": "172.XX.223.XXX",
        "arch": 2,
        "tags": [
            "..."
        ],
        "moduleid": 2
    },
    "ts": "2019-09-02 15:33:28",
    "event": {
        "HASH_MD5": "7812c2c0a46d1f0a1cf8f2b23cd67341",
        "HASH": "d1d59eefe1aeea20d25a848c2c4ee4ffa93becaa3089745253f9131aedc48515",
        "ERROR": 0,
        "FILE_INFO": "10.0.17134.1",
        "HASH_SHA1": "000067ac70f0e38f46ce7f93923c6f5f06ecef7b",
        "SIGNATURE": {
            "FILE_CERT_IS_VERIFIED_LOCAL": 1,
            "CERT_SUBJECT": "C=US, S=Washington, L=Redmond, O=Microsoft Corporation, CN=Microsoft Windows",
            "FILE_PATH": "C:\\Windows\\System32\\setupcln.dll",
            "FILE_IS_SIGNED": 1,
            "CERT_ISSUER": "C=US, S=Washington, L=Redmond, O=Microsoft Corporation, CN=Microsoft Windows Production PCA 2011"
        },
        "FILE_PATH": "C:\\Windows\\System32\\setupcln.dll"
    }
}
```

This means what we want is to apply rules to the `event/FILE_PATH`. First part, #3 is easy, we just want to test for the `event/FILE_PATH` ends in `.cpl`, we can do this using the `ends with` operator.

Most operators will use a `path` and a `value`. General convention is the `path` describes how to get to a value we want to compare within the event. So `event/FILE_PATH` says "starting in the `event` then get the `FILE_PATH`. The `value` generally represents a value we want to compare to the element found in the `path`. How it is compared depends on the operator.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
```

That was easy, but we're missing a critical component! By default, D&R rules operate in a case sensitive mode. This means that the above node we added will match `.cpl` but will NOT match `.cPl`. To fix this, we just add the `case sensitive: false` statement.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
    case sensitive: false
  -
```

Finally, we want to make sure the `event/FILE_PATH` is NOT in the `windows` directory. To do this, we will use a regular expression with a `matches` operator. But in this case, we want to EXCLUDE the paths that include the `windows` directory, so we want to "invert" the match. We can do this with the `not: true` statement.

```yaml
op: and
event: CODE_IDENTITY
rules:
  - op: is windows
  - op: ends with
    path: event/FILE_PATH
    value: .cpl
    case sensitive: false
  - op: matches
    path: event/FILE_PATH
    re: ^.\:\\windows\\
    case sensitive: false
    not: true
```

Here we go, we're done drafting our first rule.

## Validate Rule

What we want to do now is validate the rule. If the rule validates, it doesn't mean it's correct, it just means that the structure is correct, the operators we use are known, etc. It's the first pass at detecting possible formatting issues or typos.

To validate, we will simply leverage the Replay service. This service can be used to test rules or replay historical events against a rule. In this case however, we just want to start by validating.

Up until now we focused on the "detection" part of the rule. But a full rule also contains a "response" component. So before we proceed, we'll add this structure. For a response, we will use a simple `action: report`. The `report` creates a "detection" (alert).

```yaml
detect:
  op: and
  event: CODE_IDENTITY
  rules:
    - op: is windows
    - op: ends with
      path: event/FILE_PATH
      value: .cpl
      case sensitive: false
    - op: matches
      path: event/FILE_PATH
      re: ^.\:\\windows\\
      case sensitive: false
      not: true
respond:
  - action: report
    name: T1196
```

Now validate: `limacharlie replay --validate --rule-content T1196.rule`

After a few seconds, you should see a response with `success: true` if the rule validates properly.

## Test rule

### Test Plan

Now that we know our rule is generally sound, we need to test it against some events.

Our test plan will take the following approach:

1. Test a positive (a `.cpl` loading outside of `windows`).
2. Test a negative for the major criteria:
   1. Test a non-`.cpl` loading outside of `windows` does not match.
   2. Test a `.cpl` loading within `windows` does not match.
3. Test on historical data.

With this plan, #1 and #2 lend themselves well to unit tests while #3 can be done more holistically by using Replay to run historical events through the rule and evaluate if there are any false positives.

This may be excessive for you, or for certain rules which are very simple, we leave that evaluation to you. For the sake of this code lab, we will do a light version to demonstrate how to do tests.

### Testing a Single Event

To test #1 and #2, let's just create some synthetic events. It's always better to use real-world samples, but we'll leave that up to you.

Take the event sample we had in the "Draft Rule" section and copy it to two new files we will name `positive.json`, `negative-1.json` and `negative-2.json`.

Modify the `positive.json` file by renaming the `FILE_PATH` at the bottom from `"C:\\Windows\\System32\\setupcln.dll"` to `"C:\\temp\\System32\\setupcln.cpl"` so that the event now describes a `.cpl` loading in the `temp` directory, which we should detect.

Then modify the `negative-1.json` file by changing the same `.dll` to `.cpl`. This should NOT match because the path is still in the `windows` directory.

Then modify the `negative-2.json` file by changing the `windows` directory to `temp`. This should still NOT match because it's not a `.cpl`.

Now we can run our 3 samples against the rule using Replay,

`limacharlie replay --rule-content T1196.rule --events positive.json` should output a result indicating the event matched (by actioning the `report`) like:

```json
{
  "num_evals": 4,
  "eval_time": 0.00020599365234375,
  "num_events": 1,
  "responses": [
    {
      "report": {
        "source": "11111111-1111-1111-1111-111111111111.11111111-1111-1111-1111-111111111111.11111111-1111-1111-1111-111111111111.10000000.2",
        "routing": {
...
```

`limacharlie replay --rule-content T1196.rule --events negative-1.json` should output a result indicating the event did NOT match like:

```json
{
  "num_evals": 4,
  "eval_time": 0.00011777877807617188,
  "num_events": 1,
  "responses": [],
  "errors": []
}
```

`limacharlie replay --rule-content T1196.rule --events negative-2.json` be the same as `negative-1.json`.

### Testing Historical Data

The final test is to run the rule against historical data. If you are not using an organization on the free tier, note that the Replay API is billed on usage. In the following step we will run against all historical data from the organization, so if your organization is not on the free tier and it is large, there may be non-trivial costs associated.

Running our rule against the last week of data is simple:

`limacharlie replay --rule-content T1196.rule --entire-org --last-seconds 604800`

No matches should look like that:

```json
{
  "num_evals": 67354,
  "eval_time": 1107.2150619029999,
  "num_events": 222938,
  "responses": [],
  "errors": []
}
```

### Moving to Unit Tests

Once your rule is done and you've evaluated various events for matches, you can move these to D&R Rules Unit Tests so that the tests are run during rule update.

## Publish Rule

Now is the time to push the new rule to production, the easy part.

Simply run `limacharlie dr add --rule-name T1196 --rule-file T1196.rule` and confirm it is operational by running `limacharlie dr list`.

---

# Events & Telemetry

# Anomalies

## HIDDEN_MODULE_DETECTED

Generated when a [`hidden_module_scan`](/v1/docs/sensor-commands-anomalies#hiddenmodulescan) command is issued.

Note that the name of the event does not confirm the presence of a hidden module. Please check the output to confirm whether a hidden module was detected.

**Sample Event:**

```
{
  "ERROR": 0,
  "ERROR_MESSAGE": "done"
}
```

---

# Artifacts

The Artifact Collection system allows you to ingest artifact types like:

* Plain text logs (syslog for example)
* Windows Event Logs
* PCAPs
* Windows Prefetch files
* Windows PE (executables) files
* [Zeek](https://zeek.org) (previously Bro)
* Full memory dumps
* Generic JSON
* OLE (MS Word, Excel etc)
* Windows MFT CSV Listing
* Apple Binary/XML plists

Those artifacts can be ingested from hosts running a LimaCharlie sensor, or they can be pushed to the LimaCharlie platform via a REST interface.

Once ingested, the artifacts are retained and made available to you with a custom retention period. Ingested artifacts are also indexed similarly to LimaCharlie events. This means that you can search all of your artifact for the last year for Indicators like IP Addresses, Domain Names, User Names, Hashes etc.

This in turn makes it possible for you to be looking at sensor data, identify an IP of interest, and launch a quick search to see if this IP has been observed in any artifacts over the past year. If it has, with one click you can visualize the relevant artifact entries to assist you in your investigation.

We call this "artifact operationalization". It is not meant to be a general viewing and querying tool like Splunk, but as a tactical tool providing you with critical answers as you need them during security operations.

Note that Artifact Collection configurations are synchronized with sensors every few minutes.

## Ingestion

There are multiple ways to ingest artifacts within LimaCharlie, depending on the need of the user.

**Please note, if you are looking for real-time streaming of a file (such as a system log), we'd recommend using the `file` adapter instead of Artifact Collection.**

### Using LC Sensors

The LimaCharlie sensor can be used to retrieve artifact files directly from hosts.

#### Manually

To instruct the ingestion of an artifact file located on a host where LC is installed, simply issue the `artifact_get` command. You should receive two events in response to this command: a general receipt indicating the sensor received the command, and a response with a status code indicating whether the ingestion was successful (an error code of `200` (as in HTTP) indicates success).

```
artifact_get --type pcap --file /path/to/file.pcap --days-retention $days
```

#### Using the Extension

**File Collection - Artifact Collection Rules**

With the Artifact Collection Extension enabled, a new section should be open in the web interface. It will allow you to manage the automatic collection of files from your fleet without manual input or configuration.

The extension manages this through the use of Rules that specify a set of Platforms (like Windows), Tags (sensor tags), a retention time and file patterns.

Rules define which file path patterns should be monitored for changes and ingested for specific sets of hosts.

Filter tags are tags that must ALL be present on a sensor for it to match (ANDed), while the platform of the sensor much match one of the platforms in the filter (ORed).

Patterns are file path where the file expression at the end of the path can contain patterns line (`*`, `?`, `+`).

These wildcards are NOT supported in the path portion of the pattern. Windows directory separators (backslash, `\`) must be escaped like `\\`.

* Good example: `/var/log/*.1`
* Bad example: `/var/*/syslog`

Note that matching files are watched for changes. When a change is detected, the entire file is ingested. *This means you usually want to target logs that get rolled over after a certain time.*

For example syslog is rolled from `syslog` to `syslog.1` after a day, you want to target `syslog.1` to avoid duplicating records from a file being appended to.

Rules may also specify special accesses to log. For example, specifying a rule with a file path of `wel://Security:*` will begin collection of the Windows Event Logs (`wel`) in real-time directly from the sensor. See the Windows Event Logs section below.

**Network Capture - PCAP Capture Rules**

The extension also offers a rule system to do network capture from the host. **This feature is currently only available on Linux.**

To see the network interfaces available for capture, issue the `pcap_ifaces` command to the sensor.

Each capture rule filters a set of sensors per tag. The second part of the rule is the list of patterns to capture from. Each pattern defines a network interface to use and a [tcpdump-like](https://www.tcpdump.org/manpages/pcap-filter.7.html) filter expression to select traffic from that interface.

The filter part of the capture pattern will automatically receive an additional "filter out" expression that removes traffic related to LimaCharlie itself (to avoid a feedback loop of traffic).

For example, you could specify the filter:

```
tcp port 80
```

which would automatically be expanded for you as

```
tcp port 80 and not lc.aaa.limacharlie.io and not ...
```

These rules get synced with agents every 10 minutes. Once a capture on the agent reaches a certain threshold (about 30MB), the capture will get automatically sent to the LimaCharlie cloud as an artifact with the retention specified in the rule. From there you can specify D&R rules to process further the pcap data automatically, like using the Zeek service.

### Using the CLI

To simplify the task on ingesting via the REST API, you can use the LC CLI tool (`pip install limacharlie`). Using this tool, you can use the `limacharlie artifacts --help` command it installs. This is the recommended way of ingesting logs from external systems where LC is not installed.

### Using the REST API

When the sensor is tasked to ingest an artifacts file, it itself uses the REST API.

The REST API uses Ingestion Keys, which can be managed through the REST API section of the LC web interface. Access to manage these Ingestion Keys requires the `ingestkey.ctrl` permission.

The REST endpoint is located at a per-datacenter URL. You can query the relevant URL for your organization using [this REST call](https://api.limacharlie.io/static/swagger/#/orgs/get_orgs__oid__url).

The endpoint is authenticated using Basic Authentication with the user name being the Organization ID (OID) and the password the Ingestion Key, via a POST.

The body of the POST contains the artifact file to ingest. Additional metadata is provided using the following Header fields:

* `lc-source` is a free form string used as an identifier of the origin of the artifact. When an artifact is ingested from a LC sensor, this value is the Sensor ID (SID) of the sensor.
* `lc-hint` if present, this indicates to the backend how the file should be interpreted. It default to `auto` which results in the backend auto-detecting the formal. Currently supported hints include `wel` (Windows Events Log), `prefetch` (Windows prefetch file), `pcap`, `txt`, `pe`, `zeek`, `json`.
* `lc-payload-id` if present, this is a globally unique identifier for the artifact file. It can be used to ingest artifacts in an idempotent way, meaning a second file ingested with this same value will be ignored.
* `lc-path` if present, should be a base-64 encoded string representing the original file path of the artifact on the source system.
* `lc-part` if present, is used to track multi-part artifact uploads. If set, it should be an integer starting at `0` and incrementing for every part with the last part being set to `done`. The `lc-payload-id` MUST be set and constant across all parts.

## Accessing Artifacts

The left navigation contains a link to "Artifacts" which displays all artifacts ingested across all sensors in the organization. From there you can select a specific artifact and view it, or choose to download the original and/or parsed version.

You can also see the artifacts collected for a particular sensor by going to the "Sensors" section, clicking into a sensor, and then clicking on "Artifacts".

## Windows Event Logs

### From Real-Time Events

*Only supported on Windows 2008 and up*

It is possible to subscribe to receive Windows Event Logs in real-time from the sensor. By doing this, the targeted Windows Events will be sent to the cloud as normal LimaCharlie telemetry events encapsulated in an event type of `WEL`. The Windows Events in those cases will be structured as JSON similarly to other LimaCharlie telemetry. This means you can create detection and response rules that operate directly on Windows Events, or even correlate between Windows Events and native LimaCharlie telemetry events.

To configure this collection, you need to specify a special kind of log path as a collection pattern. The format is as follows:

```
wel://EventSource:FilterExpression
```

The `wel://` prefix tells LimaCharlie this is not a file at rest, but a live API request from the sensor. The `EventSource` part of the expression refers to the `ChannelPath` described in the Windows documentation here: https://docs.microsoft.com/en-us/windows/win32/api/winevt/nf-winevt-evtsubscribe. The `FilterExpression` component refers to the `Query` parameter described in the same documentation. Additional documentation on the filter format can also be found here: https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events.

Examples of supported patterns:

* `wel://Security:*`
* `wel://Application:*`
* `wel://System:*`
* `wel://Microsoft-Windows-Windows Defender/Operational:*`
* `wel://Microsoft-Windows-PowerShell/Operational:*`
* `wel://Microsoft-Windows-Sysmon/Operational:*`
* `wel://System:Event[System[EventID=4624]]`
* `wel://System:*[System[(EventID='7040')]]`

These WEL events will come in to the sensor's Timeline as `WEL` events like:

```json
{
  "EVENT": {
    "EventData": {
      "AuthenticationPackageName": "NTLM",
      "FailureReason": "%%2313",
      "IpAddress": "185.198.69.35",
      "IpPort": "0",
      "KeyLength": "0",
      "LmPackageName": "-",
      "LogonProcessName": "NtLmSsp",
      "LogonType": "3",
      "ProcessId": "0x0",
      "ProcessName": "-",
      "Status": "0xc000006d",
      "SubStatus": "0xc0000064",
      "SubjectDomainName": "-",
      "SubjectLogonId": "0x0",
      "SubjectUserName": "-",
      "SubjectUserSid": "S-1-0-0",
      "TargetDomainName": "",
      "TargetUserName": "USER",
      "TargetUserSid": "S-1-0-0",
      "TransmittedServices": "-",
      "WorkstationName": "-"
    },
    "System": {
      "Channel": "Security",
      "Computer": "demo-win-2016",
      "Correlation": {
        "ActivityID": "{F207C050-075F-0001-952F-331047A7DA01}"
      },
      "EventID": "4625",
      "EventRecordID": "29089367",
      "Execution": {
        "ProcessID": "568",
        "ThreadID": "3456"
      },
      "Keywords": "0x8010000000000000",
      "Level": "0",
      "Opcode": "0",
      "Provider": {
        "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
        "Name": "Microsoft-Windows-Security-Auditing"
      },
      "Security": "",
      "Task": "12544",
      "TimeCreated": {
        "SystemTime": "2024-05-30T22:17:45.516965200Z"
      },
      "Version": "0",
      "_event_id": "4625"
    }
  }
}
```

### From Files at Rest

When running D&R rules against Windows Event Logs (`target: artifact` and `artifact type: wel`) that were acquired from files at rest, although the Artifact Collection Service may ingest the same Windows Event Log file that contains some records that have already been processed by the rules, the LimaCharlie platform will keep track of the processed `EventRecordID` and therefore will NOT run the same D&R rule over the same record multiple times.

This means you can safely set the Artifact Collection Service to collect various Windows Event Logs from your hosts and run D&R rules over them without risking producing the same alert multiple times.

For most Windows Event Logs available, see `c:\windows\system32\winevt\logs\`.

## Mac Unified Logs

Like with Windows Event Logs (WEL), it is possible to collect Mac Unified Logs (MUL) in real-time from the endpoint.

The mechanism used is very similar to WEL: specify a collection rule with a log path like:

```
mul://<Predicate>
```

where `<Predicate>` is a [predicate filter](https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/Predicates/Articles/pSyntax.html) supplied to the API on the endpoint, selecting a subset of the local Mac Unified Logs to be streamed to the cloud.

Example: `mul://process == "Safari"`

*Note that predicates are case sensitive.*

These selected MUL events will come in to the sensor's Timeline as an event of type `MUL` like:

```json
{
  "activityIdentifier": 0,
  "category": "entry",
  "date": 1717107501.628475,
  "level": 2,
  "nessage": "CopyData('DefaultAsciiKeyboardLayoutPasteboard' (<CFUUID 0x600001d47ac0> 425712C6-DAB1-497D-A573-168ECB75AF4A) gen: -1 item: 1264739405 flavor: 'DefaultAsciiKeyboardLayoutFlavor')",
  "process": "Safari",
  "processIdentifier": 77998,
  "sender": "CoreFoundation",
  "storeCategory": 0,
  "subsystem": "com.apple.CFPasteboard",
  "threadIdentifier": 20382671,
  "type": "log"
}
```

---

# Azure Event Hub

## Overview

This Adapter allows you to connect to an Azure Event Hub to fetch structured data stored there.

[Azure Event Hubs](https://azure.microsoft.com/en-us/products/event-hubs) are fully managed, real-time data ingestion services that allow for event streaming from various Microsoft Azure services. LimaCharlie can ingest either structured known data (such as JSON or XML) *or* known Microsoft data types, including:

* Azure Monitor (Platform: `azure_monitor`)
* Entra ID [formerly Azure AD] (Platform: `azure_ad`)
* Microsoft Defender (Platform: `msdefender`)

Documentation for creating an event hub can be found here [here](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

* If using a binary Adapter, `azure_event_hub` will be the ingestion type.
* `connection_string` - The connection string provided in Azure for connecting to the Azure Event Hub, including the `EntityPath=...` at the end which identifies the Hub Name (this component is sometimes now shown in the connection string provided by Azure).

## Guided Deployment

Azure Event Hub data can be pulled via either a cloud or binary Adapter.

### Cloud-to-Cloud

LimaCharlie offers several helpers within the webapp that allow you to ingest Microsoft data, such as Entra ID or Microsoft Defender, from Azure Event Hubs.

### CLI Deployment

The following example configures a binary Adapter to collect Microsoft Defender data from an Azure Event Hub:

```
./lc_adapter azure_event_hub client_options.identity.installation_key=<INSTALLATION_KEY> \
client_options.identity.oid=<OID> \
client_options.platform=msdefender \
client_options.sensor_seed_key=<SENSOR_SEED_KEY> \
client_options.hostname=<HOSTNAME> \
"connection_string=Endpoint=sb://mynamespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fnaaaaaaaaaaaaaaak0g54alYbbbbbbbbbbbbbbbALQ=;EntityPath=lc-stream"
```

### Infrastructure as Code Deployment

```
# Azure Event Hub Specific Docs: https://docs.limacharlie.io/docs/adapter-types-azure-event-hub

sensor_type: "azure_event_hub"
  azure_event_hub:
    connection_string: "Endpoint=sb://your-eventhub-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=YOUR_EVENT_HUB_SHARED_ACCESS_K
  EY_HERE;EntityPath=your-actual-event-hub-name"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_FOR_AZURE"
      hostname: "azure-eventhub-adapter"
      platform: "json"
      sensor_seed_key: "azure-eventhub-prod-sensor"
      indexing: []
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Command-line Interface

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Detecting Related Events

## Overview

> It's recommended to first read [Detection & Response rules](/v1/docs/detection-and-response) before diving into detecting related events.

Events in LimaCharlie have well-defined relationships to one another using `routing/this`, `routing/parent`, `routing/target`, and can even be implicitly related by occurring in a similar timeframe. The relation context can be useful for writing more complex rules.

These are called "stateful" rules.

## Detecting Children / Descendants

To detect events in a tree you can use the following parameters:

* `with child`: matches children of the initial event
* `with descendant`: matches descendants (children, grandchildren, etc.) of the initial event

Aside from how deep they match, the `with child` and `with descendant` parameters operate identically: they declare a nested stateful rule.

For example, let's detect a `cmd.exe` process spawning a `calc.exe` process:

```
# Detect initial event
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: cmd.exe
case sensitive: false
with child: # Wait for child matching this nested rule
  op: ends with
  event: NEW_PROCESS
  path: event/FILE_PATH
  value: calc.exe
  case sensitive: false
```

Simply put, this will detect:

```
cmd.exe --> calc.exe
```

Because it uses `with child` it will not detect:

```
cmd.exe --> firefox.exe --> calc.exe
```

To do that, we could use `with descendant` instead.

## Detecting Proximal Events

To detect repetition of events close together on the same sensor, we can use `with events`.

The `with events` parameter functions very similarly to `with child` and `with descendant`: it declares a nested stateful rule.

For example, let's detect a scenario where `5` bad login attempts occur within `60` seconds.

```
event: WEL
op: is windows
with events:
  event: WEL
  op: is
  path: event/EVENT/System/EventID
  value: '4625'
  count: 5
  within: 60
```

The top-level rule filters down meaningful events to [`WEL`](/v1/docs/reference-events#WEL) ones sent from Windows sensors using the `is windows` operator, and then it declares a stateful rule inside `with events`. It uses `count` and `within` to declare a suitable timespan to evaluate matching events.

## Stateful Rules

Stateful rules  the rules declared within `with child`, `with descendant` or `with events`  have full range. They can do anything a normal rule might do, including declaring nested stateful rules or using `and`/`or` operators to write more complex rules.

Here's a stateful rule that uses `and` to detect a specific combination of child events:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false
```

The above example is looking for an `outlook.exe` process that spawns a `chrome.exe` process and drops a `.ps1` (powershell) file to disk. Like this:

```
outlook.exe
|--+--> chrome.exe
|--+--> .ps1 file
```

### Counting Events

Rules declared using `with child` or `with descendant` also have the ability to use `count` and `within` to help scope the events it will statefully match.

For example, a rule that matches on Outlook writing 5 new .ps1 documents within 60 seconds:

```
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
with child:
  op: ends with
  event: NEW_DOCUMENT
  path: event/FILE_PATH
  value: .ps1
  case sensitive: false
  count: 5
  within: 60
```

### Choosing Event to Report

A reported detection will include a copy of the event that was detected. When writing detections that match multiple events, the default behavior will be to include a copy of the initial parent event.

In many cases it's more desirable to get the latest event in the chain instead. For this, there's a `report latest event: true` flag that can be set. Piggy-backing on the earlier example:

```
# Detection
event: NEW_PROCESS
op: ends with
path: event/FILE_PATH
value: outlook.exe
case sensitive: false
report latest event: true
with child:
  op: and
  rules:
    - op: ends with
      event: NEW_PROCESS
      path: event/FILE_PATH
      value: chrome.exe
      case sensitive: false
    - op: ends with
      event: NEW_DOCUMENT
      path: event/FILE_PATH
      value: .ps1
      case sensitive: false

# Response
- action: report
  name: Outlook Spawning Chrome & Powershell
```

The event returned in the detection will be either the `chrome.exe` `NEW_PROCESS` event or the `.ps1` `NEW_DOCUMENT` event, whichever was last. Without `report latest event: true` being set, it would default to including the `outlook.exe` `NEW PROCESS` event.

## Caveats

### Testing Stateful Rules

Stateful rules are forward-looking only and changing a rule will reset its state.

Practically speaking, this means that if you change a rule that detects `excel.exe -> cmd.exe`, `excel.exe` will need to be relaunched while the updated rule is running for it to then begin watching for `cmd.exe`.

### Using Events in Actions

Using `report` to report a detection works according to the [Choosing Event to Report](#choosing-event-to-report) section earlier. Other actions have a subtle difference: they will *always* observe the latest event in the chain.

Consider the `excel.exe -> cmd.exe` example. The `cmd.exe` event will be referenced inside the response action if using lookbacks (i.e. `<<routing/this>>`). If we wanted to end the `excel.exe` process (and its descendants), we would write a `task` that references the parent of the current event (`cmd.exe`):

```
- action: task
  command: deny_tree <<routing/parent>>
```

---

# Endpoint Detection and Response (EDR)

The SecOps Cloud Platform, revolutionizes endpoint security by providing true real-time visibility, versatile detection capabilities, integration with open-source and managed rulesets, and vendor-agnostic telemetry ingestion. By leveraging LimaCharlie's API-first approach, flexible billing model, and seamless integration with your existing security stack, security teams can effectively detect, investigate, and respond to threats while avoiding the limitations of traditional EDR solutions.

## EDR problems

Endpoint Detection and Response (EDR) solutions are crucial for organizations to detect, investigate, and respond to threats on endpoints. However, traditional EDR solutions often present several challenges:

* **Lack of real-time visibility:** Many EDR solutions rely on periodic scans or delayed data collection, making it difficult to detect and respond to threats in real-time.
* **Limited customization and flexibility:** Traditional EDRs often use proprietary detection languages or rulesets, limiting the ability of security teams to create custom detections tailored to their unique environments.
* **Vendor lock-in and high costs:** Legacy EDR solutions often require long-term contracts, have high minimum commitments, and can be expensive to scale, leading to vendor lock-in and budget constraints.

## LimaCharlie's solution

* **True real-time EDR:** LimaCharlie provides true real-time visibility by streaming verbose telemetry from the endpoint sensor to the cloud over a semi-persistent TLS connection. This enables response actions to be taken on the endpoint within 100ms of the triggering action or behavior, drastically reducing the time to detect and respond to threats.
* **Versatile detection syntax:** LimaCharlie uses a YAML-based detection syntax that allows security teams to create highly sophisticated detections, including the ability to track state and build multi-step detection logic. This versatile syntax empowers security teams to create custom detections tailored to their specific needs and environment.
* **Integration with open-source and managed rulesets:** Leverage detections created by best-in-class security professionals using managed and open-source rulesets. With one-click access to sources like SOC Prime, Soteria, Sigma, and YARA, teams can gain unparalleled cost efficiencies and stay ahead of emerging threats.
* **Reduced mean time to respond (MTTR):** LimaCharlie allows security teams to execute a full suite of remediation responses, such as triggering memory dumps or killing process trees. By simplifying the process of activating rulesets and building custom rules, LimaCharlie significantly reduces MTTR.
* **Vendor-agnostic telemetry ingestion:** Ingest data from any source, including existing EDR solutions, in real-time. This allows security teams to avoid vendor lock-in and leverage the SCP's powerful Detection, Automation, and Response Engine on all of their telemetry, regardless of the source.

---

# Endpoint Detection and Response (EDR)

The SecOps Cloud Platform, revolutionizes endpoint security by providing true real-time visibility, versatile detection capabilities, integration with open-source and managed rulesets, and vendor-agnostic telemetry ingestion. By leveraging LimaCharlie's API-first approach, flexible billing model, and seamless integration with your existing security stack, security teams can effectively detect, investigate, and respond to threats while avoiding the limitations of traditional EDR solutions.

## EDR problems

Endpoint Detection and Response (EDR) solutions are crucial for organizations to detect, investigate, and respond to threats on endpoints. However, traditional EDR solutions often present several challenges:

* **Lack of real-time visibility:** Many EDR solutions rely on periodic scans or delayed data collection, making it difficult to detect and respond to threats in real-time.
* **Limited customization and flexibility:** Traditional EDRs often use proprietary detection languages or rulesets, limiting the ability of security teams to create custom detections tailored to their unique environments.
* **Vendor lock-in and high costs:** Legacy EDR solutions often require long-term contracts, have high minimum commitments, and can be expensive to scale, leading to vendor lock-in and budget constraints.

## LimaCharlie's solution

* **True real-time EDR:** LimaCharlie provides true real-time visibility by streaming verbose telemetry from the endpoint sensor to the cloud over a semi-persistent TLS connection. This enables response actions to be taken on the endpoint within 100ms of the triggering action or behavior, drastically reducing the time to detect and respond to threats.
* **Versatile detection syntax:** LimaCharlie uses a YAML-based detection syntax that allows security teams to create highly sophisticated detections, including the ability to track state and build multi-step detection logic. This versatile syntax empowers security teams to create custom detections tailored to their specific needs and environment.
* **Integration with open-source and managed rulesets:** Leverage detections created by best-in-class security professionals using managed and open-source rulesets. With one-click access to sources like SOC Prime, Soteria, Sigma, and YARA, teams can gain unparalleled cost efficiencies and stay ahead of emerging threats.
* **Reduced mean time to respond (MTTR):** LimaCharlie allows security teams to execute a full suite of remediation responses, such as triggering memory dumps or killing process trees. By simplifying the process of activating rulesets and building custom rules, LimaCharlie significantly reduces MTTR.
* **Vendor-agnostic telemetry ingestion:** Ingest data from any source, including existing EDR solutions, in real-time. This allows security teams to avoid vendor lock-in and leverage the SCP's powerful Detection, Automation, and Response Engine on all of their telemetry, regardless of the source.

---

# Event Schemas

Since LimaCharlie standardizes on JSON, including arbitrary sources of data, it means that Schema in LimaCharlie is generally dynamic.

To enable users to create schemas in external systems that expect more strictly typed data, LimaCharlie makes a Schema API available.

This Schema API exposes the "learned" schema from specific event types. As data comes into LimaCharlie, the Schema API will accumulate the list of fields and types observed for those specific events. In turn, the API allows you to retrieve this learned schema.

## API

### Listing Schemas

The list of all available schemas can get retrieved by doing a `GET` to `api.limacharlie.io/v1/orgs/YOUR-OID/schema`.

The returned data looks like:

```
{
  "event_types": [
    "evt:New-ExchangeAssistanceConfig",
    "det:00285-WIN-RDP_Connection_From_Non-RFC-1918_Address",
    "det:VirusTotal hit on DNS request",
    "evt:WEL",
    "evt:SHUTTING_DOWN",
    "evt:NETSTAT_REP",
    "evt:AdvancedHunting-DeviceEvents",
    "evt:NEW_DOCUMENT",
    "sched:12h_per_cloud_adapter",
    "sched:1h_per_sensor",
    "sched:3h_per_sensor",
    ...
}
```

Each element in the list of schema is composed of a prefix and a value.

Prefixes can be:

* `evt` for an Event.
* `dep` for a Deployment Event.
* `det` for a Detection.
* `art` for an Artifact Event.
* `sched` for Scheduling Events.

The value is generally the Event Type except for Detections where it is the `cat` (detection name).

### Retrieveing Schema Definition

Retrieving a specific schema definition can be done by doing a `GET` on `api.limacharlie.io/v1/orgs/YOUR-OID/schema/EVENT-TYPE`, where the `EVENT-TYPE` is one of the exact keys returned by the listing API above.

The returned data looks like:

```
{
  "schema": {
    "elements": [
      "i:routing/event_time",
      "s:routing/sid",
      "i:routing/moduleid",
      "i:event/PROCESS_ID",
      "s:routing/this",
      "i:event/DNS_TYPE",
      "s:routing/iid",
      "s:routing/did",
      "i:event/DNS_FLAGS",
      "i:routing/tags",
      "s:event/IP_ADDRESS",
      "s:routing/event_type",
      "i:event/MESSAGE_ID",
      "s:event/CNAME",
      "s:event/DOMAIN_NAME",
      "s:routing/ext_ip",
      "s:routing/parent",
      "s:routing/hostname",
      "s:routing/int_ip",
      "i:routing/plat",
      "s:routing/oid",
      "i:routing/arch",
      "s:routing/event_id"
    ],
    "event_type": "evt:DNS_REQUEST"
  }
}
```

The `schema.elements` data returned is composed of a prefix and a value.

The prefix is one of:

* `i` indicating the element is an Integer.
* `s` indicating the element is a String.
* `b` indicating the element is a Boolean.

The value is a path within the JSON. For example, the schema above would represent the following event:

```
{
  "event": {
    "CNAME": "cs9.wac.phicdn.net",
    "DNS_TYPE": 5,
    "DOMAIN_NAME": "ocsp.digicert.com",
    "MESSAGE_ID": 19099,
    "PROCESS_ID": 1224
  },
  "routing": {
    "arch": 2,
    "did": "b97e9d00-aaaa-aaaa-aaaa-27c3468d5901",
    "event_id": "8cec565d-14bd-4639-a1af-4fc8d5420b0c",
    "event_time": 1656959942437,
    "event_type": "DNS_REQUEST",
    "ext_ip": "35.1.1.1",
    "hostname": "demo-win-2016.c.lc-demo-infra.internal",
    "iid": "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
    "int_ip": "10.1.1.1",
    "moduleid": 2,
    "oid": "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
    "parent": "42217cb0326ca254999554a862c3298e",
    "plat": 268435456,
    "sid": "bb4b30af-aaaa-aaaa-aaaa-f014ada33345",
    "tags": [
      "edr"
    ],
    "this": "a443f9c48bef700740ef27e062c333c6"
  }
}
```

---

# Event Schemas

Since LimaCharlie standardizes on JSON, including arbitrary sources of data, it means that Schema in LimaCharlie is generally dynamic.

To enable users to create schemas in external systems that expect more strictly typed data, LimaCharlie makes a Schema API available.

This Schema API exposes the "learned" schema from specific event types. As data comes into LimaCharlie, the Schema API will accumulate the list of fields and types observed for those specific events. In turn, the API allows you to retrieve this learned schema.

## API

### Listing Schemas

The list of all available schemas can get retrieved by doing a `GET` to `api.limacharlie.io/v1/orgs/YOUR-OID/schema`.

The returned data looks like:

```
{
  "event_types": [
    "evt:New-ExchangeAssistanceConfig",
    "det:00285-WIN-RDP_Connection_From_Non-RFC-1918_Address",
    "det:VirusTotal hit on DNS request",
    "evt:WEL",
    "evt:SHUTTING_DOWN",
    "evt:NETSTAT_REP",
    "evt:AdvancedHunting-DeviceEvents",
    "evt:NEW_DOCUMENT",
    "sched:12h_per_cloud_adapter",
    "sched:1h_per_sensor",
    "sched:3h_per_sensor",
    ...
}
```

Each element in the list of schema is composed of a prefix and a value.

Prefixes can be:

* `evt` for an Event.
* `dep` for a Deployment Event.
* `det` for a Detection.
* `art` for an Artifact Event.
* `sched` for Scheduling Events.

The value is generally the Event Type except for Detections where it is the `cat` (detection name).

### Retrieveing Schema Definition

Retrieving a specific schema definition can be done by doing a `GET` on `api.limacharlie.io/v1/orgs/YOUR-OID/schema/EVENT-TYPE`, where the `EVENT-TYPE` is one of the exact keys returned by the listing API above.

The returned data looks like:

```
{
  "schema": {
    "elements": [
      "i:routing/event_time",
      "s:routing/sid",
      "i:routing/moduleid",
      "i:event/PROCESS_ID",
      "s:routing/this",
      "i:event/DNS_TYPE",
      "s:routing/iid",
      "s:routing/did",
      "i:event/DNS_FLAGS",
      "i:routing/tags",
      "s:event/IP_ADDRESS",
      "s:routing/event_type",
      "i:event/MESSAGE_ID",
      "s:event/CNAME",
      "s:event/DOMAIN_NAME",
      "s:routing/ext_ip",
      "s:routing/parent",
      "s:routing/hostname",
      "s:routing/int_ip",
      "i:routing/plat",
      "s:routing/oid",
      "i:routing/arch",
      "s:routing/event_id"
    ],
    "event_type": "evt:DNS_REQUEST"
  }
}
```

The `schema.elements` data returned is composed of a prefix and a value.

The prefix is one of:

* `i` indicating the element is an Integer.
* `s` indicating the element is a String.
* `b` indicating the element is a Boolean.

The value is a path within the JSON. For example, the schema above would represent the following event:

```
{
  "event": {
    "CNAME": "cs9.wac.phicdn.net",
    "DNS_TYPE": 5,
    "DOMAIN_NAME": "ocsp.digicert.com",
    "MESSAGE_ID": 19099,
    "PROCESS_ID": 1224
  },
  "routing": {
    "arch": 2,
    "did": "b97e9d00-aaaa-aaaa-aaaa-27c3468d5901",
    "event_id": "8cec565d-14bd-4639-a1af-4fc8d5420b0c",
    "event_time": 1656959942437,
    "event_type": "DNS_REQUEST",
    "ext_ip": "35.1.1.1",
    "hostname": "demo-win-2016.c.lc-demo-infra.internal",
    "iid": "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
    "int_ip": "10.1.1.1",
    "moduleid": 2,
    "oid": "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
    "parent": "42217cb0326ca254999554a862c3298e",
    "plat": 268435456,
    "sid": "bb4b30af-aaaa-aaaa-aaaa-f014ada33345",
    "tags": [
      "edr"
    ],
    "this": "a443f9c48bef700740ef27e062c333c6"
  }
}
```

---

# Events

## Events Overview

LimaCharlie provides a multitude of events based on actions generated by sensors, systems, services, artifacts, and other key functions of the platform. The following pages provide details on structured events available in LimaCharlie. Note, this section only describes events generated by the LimaCharlie Endpoint Agent Sensor or the LimaCharlie platform. Events generated by third-party sources (i.e., ingested via an [Adapter](/v2/docs/adapters)) will be provided in their raw format, and can be addressed as such within [Detection & Response rules](/v2/docs/detection-and-response).

> **Missing events on a sensor timeline?**
> 
> Not seeing an expected event in your timeline? Be sure that you included all events of interest in your [Exfil Control](/v2/docs/ext-exfil).

## Operationalizing Events

Events can be observed and matched by [Detection & Response rules](/v2/docs/detection-and-response) to automate behavior and can also be streamed via [Outputs](/v2/docs/outputs) to the destination of your choice.

## Schema

Specific Event schemas are learned and available through the Schema API, learn more [here](/v2/docs/event-schemas).

## Streams

There are 6 different event streams moving through LimaCharlie:

| Name | Description | D&R Target | Output |
| --- | --- | --- | --- |
| Events | Events sent from sensors | <default> |  |
| Deployment | Lifecycle events sent from sensors | `deployment` |  |
| Detections | Detections reported from D&R rules | `detection` |  |
| Artifacts | Artifacts sent from sensors (or API) | `artifact` |  |
| Artifact Events | Lifecycle events for artifacts | `artifact_event` |  |
| Audit | Audit logs for management activity within LimaCharlie | `audit` |  |
| Billing | Billing activity within LimaCharlie | `billing` |  |

## Formatting

At a high level, events in LimaCharlie are in standard formatted JSON.

```json
{
  "type": "object",
  "properties": {
    "event": {
      "type": "any",
      "description": "Schema is determined by the routing/event_type"
    },
    "routing": {
      "type": "object",
      "properties": {
        "this": {
          "type": "string",
          "description": "GUID (i.e. 1e9e242a512d9a9b16d326ac30229e7b) - see 'Atoms' section for more detail",
          "format": "Atom"
        },
        "event_type": {
          "type": "string",
          "description": "The event type (e.g. NEW_PROCESS, NETWORK_SUMMARY) dictates the 'event' schema"
        },
        "event_time": {
          "type": "integer",
          "description": "The time the event was observed on the host"
        },
        "latency": {
          "type": "integer",
          "description": "The time difference between event time and event arrival, in milliseconds"
        },
        "event_id": {
          "type": "string",
          "format": "UUID"
        },
        "oid": {
          "type": "string",
          "format": "UUID",
          "description": "Organization ID"
        },
        "sid": {
          "type": ["string", "null"],
          "format": "UUID",
          "description": "Sensor ID"
        },
        "did": {
          "type": ["string", "null"],
          "format": "UUID",
          "description": "Device ID"
        },
        "iid": {
          "type": ["string", "null"],
          "format": "UUID",
          "description": "Installer Key ID"
        },
        "investigation_id": {
          "type": ["string", "null"],
          "format": "string",
          "description": "Events responding to a command will include this if it was provided along with the command"
        },
        "parent": {
          "type": ["string", "null"],
          "description": "Atom of possible parent event",
          "format": "Atom"
        },
        "target": {
          "type": ["string", "null"],
          "description": "Atom of possible target event",
          "format": "Atom"
        },
        "hostname": {
          "type": ["string", "null"],
        },
        "arch": {
          "type": ["integer", "null"],
          "description": "Integer corresponds with sensor architecture"
        },
        "plat": {
          "type": ["integer", "null"],
          "description": "Integer corresponds with sensor platform"
        },
        "tags": {
          "type": ["array"],
          "format": "string",
          "description": "Tags applied to sensor at the time the event was sent"
        },
      }
    }
  }
}
```

The following is a sample event utilizing the above schema:

```json
{
  "event": {
    "BASE_ADDRESS": 140702709383168,
    "COMMAND_LINE": "C:\\\\Windows\\\\System32\\\\evil.exe -Embedding",
    "FILE_IS_SIGNED": 1,
    "FILE_PATH": "C:\\\\Windows\\\\System32\\\\evil.exe",
    "HASH": "5ef1322b96f176c4ea4b8304caf8b45e2e42c3188aa52ed1fd6196afc04b7297",
    "MEMORY_USAGE": 9515008,
    "PARENT": {
      "BASE_ADDRESS": 140697905135616,
      "COMMAND_LINE": "C:\\\\Windows\\\\system32\\\\unknown.exe -k Launch",
      "CREATION_TIME": 1625797634428,
      "FILE_IS_SIGNED": 1,
      "FILE_PATH": "C:\\\\Windows\\\\system32\\\\unknown.exe",
      "HASH": "438b6ccd84f4dd32d9684ed7d58fd7d1e5a75fe3f3d14ab6c788e6bb0ffad5e7",
      "MEMORY_USAGE": 19070976,
      "PARENT_ATOM": "ebf1884039c7650401b2198f60f89d2d",
      "PARENT_PROCESS_ID": 123,
      "PROCESS_ID": 1234,
      "THIS_ATOM": "ad48d1f14a8e5a114e85f79b60f89d2d",
      "THREADS": 14,
      "TIMESTAMP": 1626905901981,
      "USER_NAME": "NT AUTHORITY\\\\SYSTEM"
    },
    "PARENT_PROCESS_ID": 580,
    "PROCESS_ID": 5096,
    "THREADS": 6,
    "USER_NAME": "BUILTIN\\\\Administrators"
  },
  "routing": {
    "this": "655c970d2052b9f1c365839b611baf96",
    "parent": "ad48d1f14a3e5a114e85f79b60f89d2d",
    "arch": 2,
    "did": "3ef599f3-64dc-51f5-8322-62b0a6b8eef7",
    "event_id": "bdf6df69-b72c-470a-994b-216f1cdde9a7",
    "event_time": 1629204374140,
    "latency": 78,
    "event_type": "NEW_PROCESS",
    "ext_ip": "123.456.78.901",
    "hostname": "test-host-123",
    "iid": "e22638c9-44a6-455a-83e2-a689ac9868a7",
    "int_ip": "10.4.34.227",
    "moduleid": 2,
    "oid": "8cbe27f4-agh1-4afb-ba19-138cd51389cd",
    "plat": 268435456,
    "sid": "d3d17f12-eecf-5287-b3a1-bf267aabb3cf",
    "tags": ["server"],
  },
}
```

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Ingesting Defender Event Logs

The Windows Sensor can listen, alert, and automate based on various Defender events.

This is done by ingesting [artifacts from the Defender Event Log Source](/v2/docs/artifacts) and using [Detection & Response rules](/v2/docs/detection-and-response) to take the appropriate action.

A config template to alert on the common Defender events of interest is available [here](https://github.com/refractionPOINT/templates/blob/master/anti-virus/windows-defender.yaml). The template can be used in conjunction with [Infrastructure Extension](/v2/docs/ext-infrastructure) or its user interface in the [web app](https://app.limacharlie.io).

Specifically, the template alerts on the following Defender events:

* windows-defender-malware-detected (`event ID 1006`)
* windows-defender-history-deleted (`event ID 1013`)
* windows-defender-behavior-detected (`event ID 1015`)
* windows-defender-activity-detected (`event ID 1116`)

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Ingesting Sysmon Event Logs

Sysmon can be a valuable addition to any defender's toolkit, given it's verbosity and generous log data. It's worth noting that LimaCharlie's native EDR capabilities mirror much of the same telemetry. However, Sysmon and LimaCharlie can be combined to provide granular coverage across Windows systems.

With Sysmon deployed, you can utilize LimaCharlie's native Windows Event Log (WEL) streaming capabilities to bring logs into the Sensor timeline.

1. Install [Sysmon](https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon) on the endpoint.

   * This can easily be done via LimaCharlie's Payload functionality, with a rule, or manually.
   * Please note that the LimaCharlie agent must be restarted in order for Sysmon data to show up in the timeline.
   * Example rule to deploy Sysmon via payloads on Windows systems tagged with `deploy-sysmon`:

     ```
     detect:
       events:
         - CONNECTED
       op: and
       rules:
         - op: is platform
           name: windows
         - op: is tagged
           tag: deploy-sysmon
     respond:
     - action: task
       command: put --payload-name sysmon.exe --payload-path "C:\Windows\Temp\sysmon.exe"
     - action: wait
       duration: 10s
     - action: task
       command: put --payload-name sysmon-config.xml --payload-path "C:\Windows\Temp\sysmon-config.xml"
     - action: wait
       duration: 10s
     - action: task
       command: run --shell-command "C:\Windows\Temp\sysmon.exe -accepteula -i C:\Windows\Temp\sysmon-config.xml"
     - action: wait
       duration: 10s
     - action: task
       command: file_del "C:\Windows\Temp\sysmon.exe"
     - action: task
       command: file_del "C:\Windows\Temp\sysmon-config.xml"
     - action: remove tag
       tag: deploy-sysmon
     - action: task
       command: restart
     ```

2. Within the Organization where you wish to collect Sysmon data, go to the `Event Collection > Event Collection Rules` section.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-sysmon-1.png)

3. Ensure that for Windows systems, `WEL` events are collected.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-sysmon-2.png)

4. Go to the `Artifact Collection` section and add a new collection rule with the following path to bring in all Sysmon events:

`wel://Microsoft-Windows-Sysmon/Operational:*`

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-sysmon-3.png)

> **Note:** You can use tagging or other filters to narrow down the systems that logs are collected from.

## Event Filtering

You can filter events by event ID to import select events. For example:

`wel://Microsoft-Windows-Sysmon/Operational:16`

`wel://Microsoft-Windows-Sysmon/Operational:25`

5. Allow up to 10 minutes for data to come into LimaCharlie after setting up a new Artifact Collection rule. Data will flow in real-time after that point.

6. Navigate to the Timeline view of a Sensor to confirm that Sysmon logs are present. You can search for Event Type `WEL` and Search for `Microsoft-Windows-Sysmon` to validate the telemetry.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2896%29.png)

---

# Ingesting Windows Event Logs

You can enable real-time Windows Event Log (WEL) ingestion using the LimaCharlie EDR Sensor.

First, navigate to the Exfil Control section of LimaCharlie and ensure that `WEL` events are enabled for your Windows rules.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-wel-1.png)

Next, navigate to the `Artifact Collection` section and set up an artifact collection rule for the Windows Event Log(s) of interest.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-wel-2.png)

To ingest WEL real-time events in the timeline, use the `wel://[Log Name]` format. For example, to ingest the System event log, you'd use the following pattern:

`wel://system:*`

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-wel-3.png)

## Difference between `.evtx` versus `wel://` ingestion

If you specify the file on disk, via the `evtx` file extension (as seen in the image above), LimaCharlie will upload the entire Windows Event Log file from disk. This will be represented as a collected artifact, not as real-time events on the sensor's timeline. This method incurs regular artifact ingestion costs for "Telemetry Sources" as seen on our [pricing](https://limacharlie.io/pricing) page.

If you ingest Windows Event Logs with a `wel://` pattern, they are streamed in real-time as first-class telemetry alongside the native EDR events, and are included in the flat rate price of the sensor.

After you apply those, you should start seeing your Windows Event Log data coming through for your endpoints. You can verify this by going into the Timeline view and choosing `WEL` event type.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ingest-wel-4.png)

---

# Endpoint Detection and Response (EDR)

Endpoint Detection and Response (EDR) is a comprehensive security solution that provides continuous monitoring, threat detection, investigation, and response capabilities for endpoint devices (workstations, servers, mobile devices, etc.).

## Overview

LimaCharlie's EDR platform combines advanced threat detection with rapid incident response capabilities. It enables security teams to:

- **Monitor** endpoints in real-time for suspicious activities
- **Detect** known and unknown threats using behavioral analysis
- **Investigate** security incidents with deep forensic data
- **Respond** to threats automatically or manually with flexible actions

## Key Capabilities

### Real-Time Monitoring

Continuous collection of telemetry data from all monitored endpoints, including:

- Process execution and command lines
- Network connections
- File system changes
- Registry modifications
- User authentication events
- System configuration changes

### Threat Detection

Multiple detection mechanisms working in concert:

- **Signature-based detection**: Matching known threat indicators
- **Behavioral detection**: Identifying suspicious patterns and anomalies
- **YARA rules**: Custom pattern matching for files and memory
- **Detection & Response (D&R) rules**: Flexible custom detection logic

### Investigation & Forensics

Deep visibility into security incidents:

- Historical telemetry data for retrospective analysis
- Event timeline reconstruction
- Process tree visualization
- Network connection analysis
- File and memory analysis

### Automated Response

Configurable automated actions when threats are detected:

- Process termination
- Network isolation
- File quarantine
- Custom remediation scripts
- Alert generation and escalation

## Core Components

### Sensors

Lightweight agents deployed on endpoints that:

- Collect security telemetry
- Execute response actions
- Maintain secure communication with the cloud platform
- Operate with minimal performance impact

### Detection Rules

Flexible rule system for defining threat detection logic:

```yaml
event: NEW_PROCESS
op: is
path: event/FILE_PATH
value: '*/powershell.exe'
op: and
op: contains
path: event/COMMAND_LINE
value: '-encodedcommand'
```

### Response Actions

Automated or manual actions to contain and remediate threats:

- `deny_tree`: Kill a process and all its children
- `segregate_network`: Isolate endpoint from network
- `delete_file`: Remove malicious files
- `quarantine`: Move file to secure location
- Custom actions via extensions

## Integration Architecture

LimaCharlie's EDR integrates with broader security infrastructure:

- **SIEM integration**: Forward events to security information and event management systems
- **Threat intelligence**: Enrich detection with external threat feeds
- **Orchestration**: Integrate with SOAR platforms
- **Cloud services**: Connect with AWS, Azure, GCP security services

## Use Cases

### Threat Hunting

Proactively search for threats using:

- Historical telemetry queries
- Custom detection rules
- Behavioral analytics
- Threat intelligence correlation

### Incident Response

Rapid response to security incidents:

1. Alert triage and validation
2. Scope determination across endpoints
3. Evidence collection and preservation
4. Threat containment and eradication
5. Recovery and remediation

### Compliance & Auditing

Support regulatory compliance requirements:

- Continuous monitoring and logging
- Evidence retention
- Audit trail maintenance
- Reporting and documentation

### Malware Analysis

Analyze suspicious files and behaviors:

- Dynamic analysis in isolated environments
- Static file analysis
- Memory analysis
- Network behavior analysis

## Best Practices

### Deployment

- Start with monitoring mode before enabling automated responses
- Deploy to test group before organization-wide rollout
- Configure appropriate retention policies
- Establish baseline normal behavior

### Detection Tuning

- Begin with conservative detection rules
- Monitor false positive rates
- Iteratively refine detection logic
- Document rule changes and rationale

### Response Planning

- Define clear response procedures for different threat types
- Test automated response actions in safe environment
- Maintain manual override capabilities
- Document incident response workflows

### Operational Excellence

- Regular review of detection effectiveness
- Continuous threat intelligence updates
- Team training on investigation techniques
- Performance monitoring and optimization

## Getting Started

1. **Deploy sensors** to your endpoints
2. **Configure detection rules** based on your security requirements
3. **Set up response actions** for automated threat containment
4. **Integrate with existing tools** (SIEM, threat intelligence, etc.)
5. **Train your team** on investigation and response procedures

For detailed implementation guidance, refer to the specific documentation sections for sensors, detection rules, and response actions.

---

# Reference: Platform Events

Platform events are generated by the LimaCharlie cloud infrastructure itself, rather than by endpoint agents. These events capture system-level activities, infrastructure changes, and platform operations.

## Platform Event Types

### Cloud Events
Events generated by cloud infrastructure components and services.

### System Events
Internal platform operations and state changes.

### Infrastructure Events
Changes to infrastructure configuration and status.

## Event Structure

Platform events follow a consistent schema with the following common fields:

- **event_type**: The type of platform event
- **timestamp**: When the event occurred
- **routing**: Event routing information
- **event**: Event-specific payload

## Common Platform Events

### Audit Events
Track administrative actions and configuration changes across the platform.

### Billing Events
Monitor usage and billing-related activities.

### Integration Events
Events from third-party integrations and external services.

### Service Events
Platform service status changes and health monitoring.

## Accessing Platform Events

Platform events are available through:

- REST API queries
- Event streaming
- Detection & Response rules
- Artifact collection

## Event Retention

Platform events follow the same retention policies as endpoint telemetry, based on your subscription tier.

---

# Sysmon Comparison

LimaCharlie provides comprehensive endpoint telemetry that covers and extends beyond Windows Sysmon capabilities.

## Event Coverage Comparison

### Process Events

**Sysmon Event ID 1: Process Creation**
- **LimaCharlie Equivalent**: `NEW_PROCESS`
- **Advantages**: Cross-platform (Windows, Linux, macOS, Chrome), includes parent process chain, network context

**Sysmon Event ID 5: Process Terminated**
- **LimaCharlie Equivalent**: `TERMINATE_PROCESS`
- **Advantages**: Exit codes, resource usage statistics

**Sysmon Event ID 10: Process Access**
- **LimaCharlie Equivalent**: Not directly mapped
- **Note**: LimaCharlie focuses on behavior-based detection rather than low-level access events

### File Events

**Sysmon Event ID 11: File Created**
- **LimaCharlie Equivalent**: `NEW_DOCUMENT`, `FILE_CREATE`
- **Advantages**: Cross-platform, content hashing, automatic threat intelligence lookups

**Sysmon Event ID 23: File Deleted**
- **LimaCharlie Equivalent**: `FILE_DELETE`
- **Advantages**: Tracks deletion context and actor

**Sysmon Event ID 26: File Delete Logged**
- **LimaCharlie Equivalent**: `FILE_DELETE`

### Network Events

**Sysmon Event ID 3: Network Connection**
- **LimaCharlie Equivalent**: `NETWORK_CONNECTIONS`, `NEW_TCP4_CONNECTION`, `NEW_TCP6_CONNECTION`, `NEW_UDP4_CONNECTION`, `NEW_UDP6_CONNECTION`
- **Advantages**: Automatic GeoIP, threat intelligence integration, DNS correlation

**Sysmon Event ID 22: DNS Query**
- **LimaCharlie Equivalent**: `DNS_REQUEST`
- **Advantages**: Query and response correlation, threat intelligence

### Registry Events (Windows)

**Sysmon Event ID 12: Registry Object Added or Deleted**
**Sysmon Event ID 13: Registry Value Set**
**Sysmon Event ID 14: Registry Object Renamed**
- **LimaCharlie Equivalent**: `REGISTRY_CREATE`, `REGISTRY_DELETE`, `REGISTRY_WRITE`
- **Advantages**: Pattern-based collection, reduced noise

### Driver/Image Events

**Sysmon Event ID 6: Driver Loaded**
- **LimaCharlie Equivalent**: `DRIVER_LOAD`

**Sysmon Event ID 7: Image Loaded**
- **LimaCharlie Equivalent**: `MODULE_LOAD`
- **Advantages**: Hash-based tracking, signature verification

### Additional Sysmon Events

**Sysmon Event ID 8: CreateRemoteThread**
- **LimaCharlie Equivalent**: `REMOTE_THREAD_INJECTION`
- **Advantages**: Behavior detection, code injection detection

**Sysmon Event ID 15: File Stream Created (ADS)**
- **LimaCharlie Equivalent**: Covered by file monitoring
- **Note**: Alternate data streams tracked automatically

**Sysmon Event ID 17/18: Pipe Events**
- **LimaCharlie Equivalent**: `NAMED_PIPE_CREATE`, `NAMED_PIPE_CONNECT`

**Sysmon Event ID 19/20/21: WMI Events**
- **LimaCharlie Equivalent**: `WMI_QUERY`, `WMI_CREATE`

## LimaCharlie-Exclusive Capabilities

Events and features available in LimaCharlie but not in Sysmon:

### Cross-Platform Coverage
- Linux events (process, file, network)
- macOS events
- Chrome OS events

### Cloud-Native Features
- Automatic threat intelligence enrichment
- Real-time Detection & Response
- Artifact collection
- Memory analysis
- Live investigation commands

### Advanced Telemetry
- `CODE_IDENTITY`: Code signing and identity verification
- `GET_DOCUMENT`: On-demand file retrieval
- `CONNECTED`: Sensor heartbeat and connectivity
- `USER_OBSERVED`: User session tracking
- Service-specific events (SSH, HTTP, etc.)

### Behavioral Detection
- Process relationship tracking
- Automated behavior analysis
- Privilege escalation detection
- Lateral movement detection

## Migration Considerations

When migrating from Sysmon to LimaCharlie:

1. **Event Mapping**: Use the comparison above to map existing Sysmon-based detections
2. **Configuration**: LimaCharlie uses exfil rules and D&R rules instead of Sysmon XML configuration
3. **Performance**: LimaCharlie's cloud-native architecture reduces endpoint performance impact
4. **Flexibility**: D&R rules provide more powerful detection logic than Sysmon event filtering

---

# Event Schemas

LimaCharlie events follow structured schemas that define the fields and data types for each event type.

## Schema Structure

All events share common base fields:

```json
{
  "routing": {
    "oid": "organization_id",
    "sid": "sensor_id",
    "iid": "installer_id",
    "tags": ["tag1", "tag2"],
    "arch": "x64|x86|arm64",
    "plat": "windows|linux|macos|chrome"
  },
  "event_type": "EVENT_TYPE",
  "event_time": 1234567890,
  "event_id": "unique_event_identifier",
  "event": {
    // Event-specific fields
  }
}
```

## Common Field Types

### Routing Information
- **oid**: Organization ID (UUID)
- **sid**: Sensor/Agent ID (UUID)
- **iid**: Installer ID (UUID)
- **tags**: Array of sensor tags
- **arch**: Architecture (x64, x86, arm64)
- **plat**: Platform (windows, linux, macos, chrome)

### Time Fields
- **event_time**: Unix timestamp (seconds since epoch)
- Timestamps use UTC

### Identifiers
- **event_id**: Unique event identifier (UUID)
- **pid**: Process ID
- **ppid**: Parent process ID
- **tid**: Thread ID

### File Information
- **file_path**: Full path to file
- **file_name**: File name only
- **hash**: File hash (usually SHA256)
- **size**: File size in bytes

### Network Information
- **ip**: IP address (IPv4 or IPv6)
- **port**: Port number
- **protocol**: Network protocol
- **domain**: DNS domain name

### Process Information
- **process_path**: Full executable path
- **command_line**: Complete command line
- **user**: Username
- **integrity_level**: Process integrity level (Windows)

## Event-Specific Schemas

### NEW_PROCESS

```json
{
  "event_type": "NEW_PROCESS",
  "event": {
    "PROCESS_ID": 1234,
    "PARENT_PROCESS_ID": 5678,
    "FILE_PATH": "C:\\Windows\\System32\\cmd.exe",
    "COMMAND_LINE": "cmd.exe /c whoami",
    "USER_NAME": "SYSTEM",
    "HASH": "sha256_hash",
    "PARENT": {
      "PROCESS_ID": 5678,
      "FILE_PATH": "C:\\Windows\\System32\\services.exe"
    }
  }
}
```

### NETWORK_CONNECTIONS

```json
{
  "event_type": "NETWORK_CONNECTIONS",
  "event": {
    "PROCESS_ID": 1234,
    "STATE": "ESTABLISHED",
    "IP_ADDRESS": "1.2.3.4",
    "PORT": 443,
    "PROTOCOL": "tcp"
  }
}
```

### DNS_REQUEST

```json
{
  "event_type": "DNS_REQUEST",
  "event": {
    "DOMAIN_NAME": "example.com",
    "MESSAGE_TYPE": "query",
    "PROCESS_ID": 1234
  }
}
```

### FILE_CREATE

```json
{
  "event_type": "FILE_CREATE",
  "event": {
    "FILE_PATH": "C:\\Users\\user\\file.txt",
    "PROCESS_ID": 1234,
    "HASH": "sha256_hash",
    "SIZE": 12345
  }
}
```

## Schema Validation

Event schemas are validated by the LimaCharlie platform. Invalid events are rejected with error codes.

For complete event schema documentation, see the endpoint agent events reference.

---

# Reference: Schedule Events

Schedule events are time-based triggers in the LimaCharlie platform that execute actions on a defined schedule.

## Schedule Event Types

### Periodic Schedules
Execute actions at regular intervals:
- Every N minutes
- Hourly
- Daily
- Weekly
- Monthly

### Cron-Style Schedules
Use cron expressions for complex scheduling patterns.

### One-Time Schedules
Execute once at a specific time.

## Creating Scheduled Events

Scheduled events are configured through:
- REST API
- Web interface
- Infrastructure as Code

## Schedule Configuration

### Required Fields
- **name**: Schedule identifier
- **interval**: Execution frequency
- **action**: Action to execute

### Optional Fields
- **enabled**: Enable/disable schedule
- **timezone**: Timezone for execution (default: UTC)
- **max_executions**: Limit number of executions

## Actions

Scheduled events can trigger:
- Detection & Response rules
- Artifact collection
- External webhooks
- Service requests

## Example Schedule Configurations

### Daily Report
```json
{
  "name": "daily_report",
  "interval": "24h",
  "action": "generate_report",
  "enabled": true
}
```

### Hourly Scan
```json
{
  "name": "hourly_scan",
  "interval": "1h",
  "action": "run_scan",
  "timezone": "America/New_York"
}
```

## Schedule Management

- View active schedules
- Modify schedule parameters
- Enable/disable schedules
- Delete schedules
- View execution history

## Execution Logging

All scheduled executions are logged with:
- Execution timestamp
- Success/failure status
- Action output
- Error messages (if any)

---

# Reference: Error Codes

LimaCharlie error codes and their meanings.

## HTTP Error Codes

Standard HTTP status codes used by the REST API.

### 2xx Success
- **200 OK**: Request succeeded
- **201 Created**: Resource created successfully
- **204 No Content**: Success with no response body

### 4xx Client Errors
- **400 Bad Request**: Invalid request format or parameters
- **401 Unauthorized**: Authentication required or failed
- **403 Forbidden**: Insufficient permissions
- **404 Not Found**: Resource does not exist
- **409 Conflict**: Resource conflict (e.g., duplicate)
- **429 Too Many Requests**: Rate limit exceeded

### 5xx Server Errors
- **500 Internal Server Error**: Unexpected server error
- **503 Service Unavailable**: Service temporarily unavailable

## Platform Error Codes

LimaCharlie-specific error codes.

### Authentication Errors
- **INVALID_API_KEY**: API key is invalid or expired
- **INVALID_JWT**: JWT token is invalid
- **ORG_NOT_FOUND**: Organization does not exist

### Permission Errors
- **INSUFFICIENT_PERMISSIONS**: User lacks required permissions
- **RESOURCE_ACCESS_DENIED**: Access to resource denied

### Resource Errors
- **SENSOR_NOT_FOUND**: Sensor ID does not exist
- **RULE_NOT_FOUND**: Detection rule not found
- **INVALID_RULE_SYNTAX**: D&R rule syntax error

### Data Errors
- **INVALID_EVENT_FORMAT**: Event format validation failed
- **SCHEMA_VALIDATION_FAILED**: Data does not match required schema
- **INVALID_TIMESTAMP**: Timestamp format invalid

### Rate Limiting
- **RATE_LIMIT_EXCEEDED**: API rate limit exceeded
- **QUOTA_EXCEEDED**: Account quota limit reached

### Service Errors
- **SERVICE_UNAVAILABLE**: Service temporarily unavailable
- **TIMEOUT**: Request timeout
- **INTERNAL_ERROR**: Unexpected internal error

## Error Response Format

```json
{
  "error": "ERROR_CODE",
  "message": "Human-readable error description",
  "details": {
    "field": "Additional context"
  }
}
```

## Handling Errors

Best practices for error handling:

1. **Check HTTP status code** first
2. **Parse error response** for details
3. **Implement retry logic** for 5xx errors
4. **Respect rate limits** (429 responses)
5. **Log errors** for debugging

## Common Error Scenarios

### Invalid API Key
```json
{
  "error": "INVALID_API_KEY",
  "message": "The provided API key is not valid"
}
```

### Rate Limit Exceeded
```json
{
  "error": "RATE_LIMIT_EXCEEDED",
  "message": "Rate limit exceeded, retry after 60 seconds",
  "details": {
    "retry_after": 60
  }
}
```

### Rule Syntax Error
```json
{
  "error": "INVALID_RULE_SYNTAX",
  "message": "Detection rule contains syntax errors",
  "details": {
    "line": 5,
    "error": "Unexpected token"
  }
}
```

---

# Events

Events are the core telemetry data collected by LimaCharlie. They represent activities, state changes, and observations from endpoints and the platform.

## Event Categories

### Endpoint Agent Events
Telemetry collected from endpoint sensors:
- Process events
- File system events
- Network events
- Registry events (Windows)
- Authentication events
- System events

### Platform Events
Events generated by LimaCharlie infrastructure:
- Cloud events
- Service events
- Audit events
- Integration events

## Event Flow

1. **Collection**: Events are generated by endpoint agents or platform services
2. **Routing**: Events are routed based on organization, sensor, and tags
3. **Processing**: Events are processed by Detection & Response rules
4. **Storage**: Events are stored according to retention policies
5. **Access**: Events are accessible via REST API, streaming, or web interface

## Event Structure

All events follow a common structure:

- **Routing metadata**: Organization, sensor, tags, platform
- **Event type**: Identifier for the event category
- **Timestamp**: When the event occurred
- **Event payload**: Event-specific data

## Working with Events

### Querying Events
- REST API queries
- Timeline search
- Historical queries
- Real-time streaming

### Filtering Events
- By event type
- By sensor or tags
- By time range
- By custom fields

### Event Retention
Retention periods vary by subscription tier:
- Free tier: 7 days
- Professional: 30 days
- Enterprise: Custom retention

## Event-Driven Actions

Events can trigger automated responses:
- Detection & Response rules
- Artifacts collection
- External webhooks
- Service integrations

## Event Analysis

Tools for analyzing events:
- Timeline visualization
- Search and filtering
- Aggregation and statistics
- Export and reporting

## Performance Considerations

- Events are processed in real-time
- Historical queries may take longer for large datasets
- Use specific filters to improve query performance
- Consider retention policies for long-term storage

## Best Practices

1. **Use appropriate event types** for detection rules
2. **Filter events** to reduce noise
3. **Leverage tags** for event routing
4. **Monitor event volume** for anomalies
5. **Archive important events** before retention expiration

---

# Endpoint Agent Events Overview

Endpoint agents collect telemetry from monitored systems, generating events that capture process execution, file activity, network connections, and system changes.

## Event Categories

### Process Events
Track process lifecycle and execution:
- **NEW_PROCESS**: Process creation with full command line and parent chain
- **TERMINATE_PROCESS**: Process termination with exit code
- **MODULE_LOAD**: DLL/library loading
- **REMOTE_THREAD_INJECTION**: Code injection detection

### File Events
Monitor file system activity:
- **FILE_CREATE**: New file creation
- **FILE_DELETE**: File deletion
- **FILE_MODIFY**: File modification
- **NEW_DOCUMENT**: Document file creation (filtered by extension)
- **GET_DOCUMENT**: File content retrieval

### Network Events
Track network connections and DNS:
- **NETWORK_CONNECTIONS**: Active network connections snapshot
- **NEW_TCP4_CONNECTION**: New IPv4 TCP connection
- **NEW_TCP6_CONNECTION**: New IPv6 TCP connection
- **NEW_UDP4_CONNECTION**: New IPv4 UDP connection
- **NEW_UDP6_CONNECTION**: New IPv6 UDP connection
- **DNS_REQUEST**: DNS query and response

### Registry Events (Windows)
Monitor Windows Registry changes:
- **REGISTRY_CREATE**: Registry key creation
- **REGISTRY_DELETE**: Registry key deletion
- **REGISTRY_WRITE**: Registry value modification

### Authentication Events
Track user authentication and sessions:
- **USER_OBSERVED**: User session detection
- **USER_LOGIN**: User login event
- **USER_LOGOUT**: User logout event

### System Events
System-level events and changes:
- **DRIVER_LOAD**: Kernel driver loading (Windows)
- **SYSTEM_STARTUP**: System boot
- **SYSTEM_SHUTDOWN**: System shutdown
- **CONNECTED**: Sensor heartbeat and connectivity
- **CODE_IDENTITY**: Code signing and verification

### Detection Events
Events generated by detections:
- **DETECTION**: D&R rule match
- **ALERT**: Triggered alert

## Platform-Specific Events

### Windows-Only Events
- Registry events
- Driver loading
- WMI events
- Named pipe events

### Linux-Only Events
- Process credentials
- File permissions
- SELinux events

### macOS-Only Events
- LaunchAgent/LaunchDaemon monitoring
- Kernel extension events

## Event Collection Control

### Exfil Rules
Control which events are sent to the cloud:
- Include/exclude event types
- Filter by process, path, or network
- Reduce event volume
- Focus on relevant telemetry

### Event Sampling
Sample high-volume events to reduce data:
- Network connections
- DNS queries
- File system events

## Event Enrichment

Events are automatically enriched with:
- File hashes (SHA256)
- Code signatures
- Threat intelligence lookups
- GeoIP data
- Parent process chain

## Event Volume

Typical event volumes vary by:
- System activity level
- Exfil rule configuration
- Event types enabled
- Workload characteristics

## Common Use Cases

### Threat Hunting
Query historical events to find indicators of compromise.

### Incident Response
Investigate security incidents using event timeline.

### Compliance
Monitor and audit system activity for compliance requirements.

### Detection Engineering
Build Detection & Response rules based on event patterns.

## Performance Impact

LimaCharlie endpoint agents are designed for minimal performance impact:
- Kernel-level event collection
- Efficient filtering at source
- Cloud-based processing
- Adaptive sampling

## Best Practices

1. **Use exfil rules** to control event volume
2. **Enable relevant events** for your use cases
3. **Leverage enrichment** (hashes, signatures)
4. **Monitor sensor performance** metrics
5. **Archive critical events** before retention expiration

For detailed event schemas, see the Event Schemas reference.

---

# Platform Events Overview

Platform events are generated by LimaCharlie cloud infrastructure rather than endpoint agents. They capture system-level activities, infrastructure changes, and platform operations.

## Platform Event Categories

### Audit Events
Track administrative actions and configuration changes:
- User management (create, modify, delete)
- API key operations
- Organization settings changes
- Detection rule modifications
- Service configuration updates

### Service Events
Monitor platform service status:
- Service health changes
- Integration status
- Webhook delivery status
- Processing errors
- Queue metrics

### Billing Events
Track usage and billing activities:
- Subscription changes
- Usage thresholds
- Invoice generation
- Payment events

### Integration Events
Events from third-party integrations:
- External service callbacks
- Webhook deliveries
- API integrations
- Data exports

### Infrastructure Events
Platform infrastructure changes:
- Sensor registration
- Sensor decommissioning
- Tag modifications
- Installer creation

## Platform Event Structure

Platform events follow the standard event schema with routing information specific to the platform context:

```json
{
  "routing": {
    "oid": "organization_id",
    "event_source": "platform"
  },
  "event_type": "PLATFORM_EVENT_TYPE",
  "event_time": 1234567890,
  "event": {
    // Platform-specific fields
  }
}
```

## Common Platform Events

### USER_CREATED
```json
{
  "event_type": "USER_CREATED",
  "event": {
    "user_email": "user@example.com",
    "user_role": "analyst",
    "created_by": "admin@example.com"
  }
}
```

### API_KEY_GENERATED
```json
{
  "event_type": "API_KEY_GENERATED",
  "event": {
    "key_name": "automation_key",
    "permissions": ["read", "write"],
    "created_by": "admin@example.com"
  }
}
```

### RULE_MODIFIED
```json
{
  "event_type": "RULE_MODIFIED",
  "event": {
    "rule_name": "detect_malware",
    "changes": ["enabled", "action"],
    "modified_by": "analyst@example.com"
  }
}
```

### WEBHOOK_DELIVERY_FAILED
```json
{
  "event_type": "WEBHOOK_DELIVERY_FAILED",
  "event": {
    "webhook_url": "https://example.com/webhook",
    "error": "Connection timeout",
    "retry_count": 3
  }
}
```

## Accessing Platform Events

Platform events are available through:

### REST API
Query platform events using the events API with platform-specific filters.

### Web Interface
View platform events in the audit log section.

### Detection & Response
Create D&R rules that trigger on platform events.

### Webhooks
Forward platform events to external systems.

## Platform Event Retention

Platform events follow organizational retention policies:
- Audit events: Typically retained longer for compliance
- Service events: Standard retention
- Integration events: Standard retention

## Use Cases

### Security Auditing
Monitor administrative actions and configuration changes for security compliance.

### Operations Monitoring
Track service health and integration status for operational awareness.

### Compliance Reporting
Generate audit trails of user actions and system changes.

### Automation
Trigger automated workflows based on platform events.

## Best Practices

1. **Enable audit logging** for compliance requirements
2. **Monitor webhook failures** for integration health
3. **Set up alerts** for critical platform events
4. **Review audit logs** regularly
5. **Archive important events** for long-term compliance

For event schemas and field definitions, see the Event Schemas reference.

---

# Ingesting Linux Audit Logs

The LimaCharlie sensor can ingest Linux audit logs and forward them as telemetry events. This guide shows you how to configure audit log ingestion.

## Prerequisites

- LimaCharlie sensor installed on Linux system
- Root/sudo access to configure audit rules
- Linux audit daemon (auditd) installed

## Configuration

### 1. Enable Audit Log Collection

Configure the sensor to monitor audit logs by adding the audit configuration to your sensor's configuration file or through the LimaCharlie web interface.

```yaml
audit:
  enabled: true
  rules:
    - path: /var/log/audit/audit.log
      format: auditd
```

### 2. Configure Audit Rules

Add audit rules to track specific system events. Example audit rules:

```bash
# Monitor file access
auditctl -w /etc/passwd -p wa -k passwd_changes
auditctl -w /etc/shadow -p wa -k shadow_changes

# Monitor system calls
auditctl -a always,exit -F arch=b64 -S execve -k exec_tracking

# Monitor network connections
auditctl -a always,exit -F arch=b64 -S socket -S connect -k network_tracking
```

### 3. Verify Configuration

Check that audit rules are active:

```bash
auditctl -l
```

Verify the sensor is reading audit logs:

```bash
journalctl -u limacharlie -f
```

## Event Format

Ingested audit logs will appear in LimaCharlie with the event type `AUDIT_LOG` and contain:

- `audit_type`: The audit event type (e.g., SYSCALL, EXECVE, PATH)
- `audit_msg`: The raw audit message
- `timestamp`: Event timestamp
- `node`: Node identifier
- Additional parsed fields depending on the audit event type

## Example Events

### File Access Event

```json
{
  "event_type": "AUDIT_LOG",
  "audit_type": "PATH",
  "path": "/etc/passwd",
  "operation": "write",
  "user": "root",
  "timestamp": 1696867200
}
```

### Process Execution Event

```json
{
  "event_type": "AUDIT_LOG",
  "audit_type": "EXECVE",
  "command": "/bin/bash",
  "arguments": ["-c", "whoami"],
  "user": "ubuntu",
  "timestamp": 1696867200
}
```

## Filtering and Rules

You can create D&R rules to detect specific audit events:

```yaml
detect:
  event: AUDIT_LOG
  op: and
  rules:
    - op: is
      path: audit_type
      value: EXECVE
    - op: contains
      path: command
      value: suspicious_binary

respond:
  - action: report
    name: suspicious_execution_detected
```

## Performance Considerations

- Audit logs can be high volume on busy systems
- Consider filtering rules to focus on security-relevant events
- Monitor sensor resource usage when enabling audit ingestion
- Use audit rules judiciously to avoid performance impact

## Troubleshooting

### Logs Not Appearing

1. Check sensor configuration is correct
2. Verify audit daemon is running: `systemctl status auditd`
3. Check file permissions on audit.log
4. Review sensor logs for errors

### High Volume

1. Refine audit rules to reduce noise
2. Use exclusion patterns in sensor configuration
3. Consider sampling for high-frequency events

## Related Documentation

- [Reference: EDR Events](/docs/en/reference-edr-events)
- [Reference: Platform Events](/docs/en/reference-platform-events)
- [Tutorial: Ingesting Telemetry from Cloud-Based External Sources](/docs/en/tutorial-ingesting-telemetry-from-cloud-based-external-sources)

---

# Reference: Platform Events

## Event Details

### ACK_MESSAGES

Acknowledge messages event is used by some LimaCharlie Sensors (e.g. USP). It is not used by the EDR.

---

### BACKOFF

Used for flow control. Provides a number of seconds that the Sensor should wait before sending events to the cloud.

---

### billing_record

This event is emitted for all kinds of billable records for the Organization.

**Sample Event:**

```json
{
  "record": {
    "cat": "extension",
    "k": "ext-strelka:bytes_scanned",
    "oid": "8cbe27f4-aaaa-bbbb-cccc-138cd51389cd",
    "record_id": "3bbbe4d9-925b-4538-bcad-e2e1ba2be923-0",
    "ts": "2024-05-30 00:44:37",
    "v": 2797
  }
}
```

---

### CLOUD_ADAPTER_DISABLED

This event is emitted when a Cloud Adapter gets disabled because it has been erroring for a long period of time.

**Sample Event:**

```json
{
  "event":{
    "error": "invalid api key"
  },
  "routing": {
    "event_time": 1644444297696,
    "event_type": "cloud_adapter_disabled",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd"
  }
}
```

---

### DATA_DROPPED

This event is generated by the Sensor when it has been offline and the events generated overflowed its internal buffer before they could be sent to the cloud, resulting in dropped events.

---

### DELETED_SENSOR

Deleted Sensor deployment events are produced when a sensor that was previously deleted from an Org attempts to connect to the LimaCharlie cloud.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "ext_ip": "104.196.34.101",
    "int_ip": "172.17.0.2",
    "hostname": "linux-server-1",
    "event_type": "deleted_sensor",
    "event_time": 1561741553230
  },
  "event": {
    "denied_for": "720h0m0s"
  }
}
```

---

### ENROLLMENT

Enrollment deployment events are produced when a sensor enrolls into the Organization for the first time.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "enrollment",
    "event_time": 1561741553230
  },
  "event": {
    "public_ip": "104.196.34.101",
    "internal_ip": "172.17.0.2",
    "host_name": "linux-server-1"
  }
}
```

---

### EXPORT_COMPLETE

An export of artifact data is completed and ready for download.

**Sample Event:**

```json
{
  "routing" : {
    "log_id" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_type" : "export_complete",
    "log_type" : "pcap",
    "oid" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_time" : 1561741553230
  },
  "event" : {
    "size" : 2048,
    "source" : "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "original_path" : "/data/pcap/dat.pcap",
    "export_id" : "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca"
  }
}
```

---

### INGEST

A new artifact has been ingested.

**Sample Event:**

```json
{
  "routing" : {
    "log_id" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_type" : "ingest",
    "log_type" : "pcap",
    "oid" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_time" : 1561741553230
  },
  "event" : {
    "size" : 2048,
    "source" : "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "original_path" : "/data/pcap/dat.pcap",
    "original_md5" : "adjfnwonefowrnfowef"
  }
}
```

---

### QUOTA_CHANGED

Quota changed events are emitted when the quota for an Organization changes.

**Sample Event:**

```json
{
  "event":{
    "new_quota": 30,
    "old_quota": 25
  },
  "routing": {
    "event_time": 1644444297696,
    "event_type": "quota_changed",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd"
  }
}
```

---

### RUN

Emitted after a run command has been issued (e.g. to run a payload, shell command, etc.).

---

### SELF_TEST_RESULT

Internal event used during a power-on-self-test (POST) of the sensor.

---

### SENSOR_CLONE

Sensor clone events are generated when the LimaCharlie Cloud detects that a specific Sensor ID may have been cloned.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "sensor_clone",
    "event_time": 1561741553230
  },
  "event": {
    "previous_hostname" : "server-1",
    "new_hostname" : "server-2"
  }
}
```

---

### SENSOR_CRASH

This event is generated when a Sensor has crashed. It will include some telemetry useful to help LimaCharlie troubleshoot the crash.

**Sample Event:**

```json
{
  "routing": {
    "arch": 2,
    "event_time": 1670861698000,
    "event_type": "sensor_crash",
    "hostname": "linux-server-1",
    "ext_ip": "104.196.34.101",
    "int_ip": "172.17.0.2",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd",
    "plat": 268435456,
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81"
  },
  "event": {
    "crash_context": {
      "FILE_ID": 63,
      "LINE_NUMBER": 1216,
      "THREAD_ID": 7808
    }
  }
}
```

---

### SENSOR_OVER_QUOTA

Over quota deployment events are produced when a Sensor tries to connect but the Organization quota is already reached.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "sensor_over_quota",
    "event_time": 1561741553230
  },
  "event": {
    "public_ip": "104.196.34.101",
    "internal_ip": "172.17.0.2",
    "host_name": "linux-server-1"
  }
}
```

---

### SET_PERFORMANCE_MODE

Enables performance mode in the kernel (e.g., disables file tracking on Windows).

---

### SYNC

Internal event used as a heartbeat to the cloud. Sent by default every 10 minutes.

---

### UNLOAD_KERNEL

Allows manual unloading of kernel component.

---

### UPDATE

Internal event used to update the configuration of a specific collector within the endpoint.

---

### *_per_cloud_adapter

Events that are emitted once per period per cloud adapter. See [Schedule Events Reference](/v2/docs/reference-schedule-events) for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 1800,
    "adapter_name": "office-audit",
    "runtime_mtd": {
      "entity_name": "81c72a07-9540-4341-9c35-66f6cfe1b9d7",
      "entity_type": "adapter",
      "mtd": {
        "platform": "office365",
        "hostname": "office-365-audit",
        "adapter_type": "office365"
      },
      "published_at": 1689858693935
    }
  }
}
```

---

### *_per_org

Events that are emitted once per period per org. See [Schedule Events Reference](/v2/docs/reference-schedule-events) for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 86400
  },
  "routing": {
    "event_id": "0f236fbb-31df-4d11-b6ab-c6b71a63a072",
    "event_time": 1673298756512,
    "event_type": "1h_per_org",
    "oid": "8cbe27f4-bfa1-4afb-ba19-138cd51389cd",
    "sid": "00000000-0000-0000-0000-000000000000",
    "tags": []
  }
}
```

---

### *_per_sensor

Events that are emitted once per period per Sensor. See [Schedule Events Reference](/v2/docs/reference-schedule-events) for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 1800,
    "runtime_mtd": {
      "entity_name": "81c72a07-9540-4341-9c35-66f6cfe1b9d7",
      "entity_type": "sensor",
      "mtd": {
        "bytes_recv": 6202524,
        "conn_at": 1689819872,
        "eps_in": 1,
        "eps_out": 0,
        "q_size": 0
      },
      "published_at": 1689858693935
    }
  }
}
```

---

# Reference: Platform Events

## Event Details

### ACK_MESSAGES

Acknowledge messages event is used by some LimaCharlie Sensors (e.g. USP). It is not used by the EDR.

### BACKOFF

Used for flow control. Provides a number of seconds that the Sensor should wait before sending events to the cloud.

### billing_record

This event is emitted for all kinds of billable records for the Organization.

**Sample Event:**

```json
{
  "record": {
    "cat": "extension",
    "k": "ext-strelka:bytes_scanned",
    "oid": "8cbe27f4-aaaa-bbbb-cccc-138cd51389cd",
    "record_id": "3bbbe4d9-925b-4538-bcad-e2e1ba2be923-0",
    "ts": "2024-05-30 00:44:37",
    "v": 2797
  }
}
```

### CLOUD_ADAPTER_DISABLED

This event is emitted when a Cloud Adapter gets disabled because it has been erroring for a long period of time.

**Sample Event:**

```json
{
  "event":{
    "error": "invalid api key"
  },
  "routing": {
    "event_time": 1644444297696,
    "event_type": "cloud_adapter_disabled",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd"
  }
}
```

### DATA_DROPPED

This event is generated by the Sensor when it has been offline and the events generated overflowed its internal buffer before they could be sent to the cloud, resulting in dropped events.

### DELETED_SENSOR

Deleted Sensor deployment events are produced when a sensor that was previously deleted from an Org attempts to connect to the LimaCharlie cloud.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "ext_ip": "104.196.34.101",
    "int_ip": "172.17.0.2",
    "hostname": "linux-server-1",
    "event_type": "deleted_sensor",
    "event_time": 1561741553230
  },
  "event": {
    "denied_for": "720h0m0s"
  }
}
```

### ENROLLMENT

Enrollment deployment events are produced when a sensor enrolls into the Organization for the first time.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "enrollment",
    "event_time": 1561741553230
  },
  "event": {
    "public_ip": "104.196.34.101",
    "internal_ip": "172.17.0.2",
    "host_name": "linux-server-1"
  }
}
```

### EXPORT_COMPLETE

An export of artifact data is completed and ready for download.

**Sample Event:**

```json
{
  "routing" : {
    "log_id" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_type" : "export_complete",
    "log_type" : "pcap",
    "oid" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_time" : 1561741553230
  },
  "event" : {
    "size" : 2048,
    "source" : "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "original_path" : "/data/pcap/dat.pcap",
    "export_id" : "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca"
  }
}
```

### INGEST

A new artifact has been ingested.

**Sample Event:**

```json
{
  "routing" : {
    "log_id" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_type" : "ingest",
    "log_type" : "pcap",
    "oid" : "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "event_time" : 1561741553230
  },
  "event" : {
    "size" : 2048,
    "source" : "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "original_path" : "/data/pcap/dat.pcap",
    "original_md5" : "adjfnwonefowrnfowef"
  }
}
```

### QUOTA_CHANGED

Quota changed events are emitted when the quota for an Organization changes.

**Sample Event:**

```json
{
  "event":{
    "new_quota": 30,
    "old_quota": 25
  },
  "routing": {
    "event_time": 1644444297696,
    "event_type": "quota_changed",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd"
  }
}
```

### RUN

Emitted after a run command has been issued (e.g. to run a payload, shell command, etc.).

### SELF_TEST_RESULT

Internal event used during a power-on-self-test (POST) of the sensor.

### SENSOR_CLONE

Sensor clone events are generated when the LimaCharlie Cloud detects that a specific Sensor ID may have been cloned.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "sensor_clone",
    "event_time": 1561741553230
  },
  "event": {
    "previous_hostname" : "server-1",
    "new_hostname" : "server-2"
  }
}
```

### SENSOR_CRASH

This event is generated when a Sensor has crashed. It will include some telemetry useful to help LimaCharlie troubleshoot the crash.

**Sample Event:**

```json
{
  "routing": {
    "arch": 2,
    "event_time": 1670861698000,
    "event_type": "sensor_crash",
    "hostname": "linux-server-1",
    "ext_ip": "104.196.34.101",
    "int_ip": "172.17.0.2",
    "oid": "8cbe27f4-aaaa-cccc-bbbb-138cd51389cd",
    "plat": 268435456,
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81"
  },
  "event": {
    "crash_context": {
      "FILE_ID": 63,
      "LINE_NUMBER": 1216,
      "THREAD_ID": 7808
    }
  }
}
```

### SENSOR_OVER_QUOTA

Over quota deployment events are produced when a Sensor tries to connect but the Organization quota is already reached.

**Sample Event:**

```json
{
  "routing": {
    "oid": "d9ae5c17-d519-4ef5-a4ac-c454a95d31ca",
    "iid": "ca812425-5a36-4c73-a0a0-935a8ace6451",
    "sid": "a75cc927-bf28-4178-a42d-25ecc8a6be81",
    "plat": 536870912,
    "arch": 2,
    "event_type": "sensor_over_quota",
    "event_time": 1561741553230
  },
  "event": {
    "public_ip": "104.196.34.101",
    "internal_ip": "172.17.0.2",
    "host_name": "linux-server-1"
  }
}
```

### SET_PERFORMANCE_MODE

Enables performance mode in the kernel (e.g., disables file tracking on Windows).

### SYNC

Internal event used as a heartbeat to the cloud. Sent by default every 10 minutes.

### UNLOAD_KERNEL

Allows manual unloading of kernel component.

### UPDATE

Internal event used to update the configuration of a specific collector within the endpoint.

### *_per_cloud_adapter

Events that are emitted once per period per cloud adapter. See Schedule Events Reference for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 1800,
    "adapter_name": "office-audit",
    "runtime_mtd": {
      "entity_name": "81c72a07-9540-4341-9c35-66f6cfe1b9d7",
      "entity_type": "adapter",
      "mtd": {
        "platform": "office365",
        "hostname": "office-365-audit",
        "adapter_type": "office365"
      },
      "published_at": 1689858693935
    }
  }
}
```

### *_per_org

Events that are emitted once per period per org. See Schedule Events Reference for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 86400
  },
  "routing": {
    "event_id": "0f236fbb-31df-4d11-b6ab-c6b71a63a072",
    "event_time": 1673298756512,
    "event_type": "1h_per_org",
    "oid": "8cbe27f4-bfa1-4afb-ba19-138cd51389cd",
    "sid": "00000000-0000-0000-0000-000000000000",
    "tags": []
  }
}
```

### *_per_sensor

Events that are emitted once per period per Sensor. See Schedule Events Reference for more details.

**Sample Event:**

```json
{
  "event": {
    "frequency": 1800,
    "runtime_mtd": {
      "entity_name": "81c72a07-9540-4341-9c35-66f6cfe1b9d7",
      "entity_type": "sensor",
      "mtd": {
        "bytes_recv": 6202524,
        "conn_at": 1689819872,
        "eps_in": 1,
        "eps_out": 0,
        "q_size": 0
      },
      "published_at": 1689858693935
    }
  }
}
```

---

# Reference: Schedule Events

Schedule events are triggered automatically at various intervals per Organization or per Sensor, observable in rules via the `schedule` target.

Scheduling events have a very similar structure whether they are per-sensor or per-org.

The `event` component contains a single key, `frequency` which is the number of seconds frequency this scheduling event is for. The event type also contains the human readable version of the frequency.

The following frequencies are currently emitted:

* `30m`: `30m_per_org` and `30m_per_sensor`
* `1h`: `1h_per_org` and `1h_per_sensor`
* `3h`: `3h_per_org` and `3h_per_sensor`
* `6h`: `6h_per_org` and `6h_per_sensor`
* `12h`: `12h_per_org` and `12h_per_sensor`
* `24h`: `24h_per_org` and `24h_per_sensor`
* `168h` (7 days): `168h_per_org` and `168h_per_sensor`

Scheduling events are generated for each org that meets the following criteria:

* Has had at least 1 sensor online in the last 7 days.

Scheduling events are generated for each sensor that meets the following criteria:

* Has been online at least once in the last 30 days.

Scheduling events are not retained as part of the year retention in LimaCharlie. To leverage them, create D&R rules that target the `schedule` target and take the relevant `action` when matched. For example to issue an `os_packages` once per week on Windows hosts:

```
detect:
  target: schedule
  event: 168h_per_sensor
  op: is platform
  name: windows
respond:
  - action: task
    command: os_packages
    investigation: weekly-package-list
```

## Glossary

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Sensor**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Soteria EDR Rules

Soteria's EDR ruleset provides coverage across Windows, Linux, and macOS. You can check the dynamic MITRE ATT&CK mapping here:

* [All rules](https://mitre-attack.github.io/attack-navigator/#layerURL=https%3A%2F%2Fstorage.googleapis.com%2Fsoteria-detector-mapping%2F%2Fall.json)
* [Windows](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//windows.json)
* [Linux](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//linux.json)
* [macOS](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//mac.json)

## Data Access

Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

The following events are utilized by Soteria rules. Please ensure that they are configured within your Organization:

* `CODE_IDENTITY`
* `DNS_REQUEST`
* `EXISTING_PROCESS`
* `FILE_CREATE`
* `FILE_MODIFIED`
* `MODULE_LOAD`
* `NETWORK_CONNECTIONS`
* `NEW_DOCUMENT`
* `NEW_NAMED_PIPE`
* `NEW_PROCESS`
* `REGISTRY_WRITE`
* `REGISTRY_CREATE`
* `SENSITIVE_PROCESS_ACCESS`
* `THREAD_INJECTION`

This can also be done in the Add-ons Marketplace.

## Enabling Soteria's EDR Rules

Soteria's EDR rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's EDR ruleset, navigate to the **Extensions** section of the Add-On Marketplace and search for Soteria. You can also directly select `soteria-rules-edr`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to Soteria rules and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

## Endpoint Detection & Response

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Soteria EDR Rules

Soteria's EDR ruleset provides coverage across Windows, Linux, and macOS. You can check the dynamic MITRE ATT&CK mapping here:

* [All rules](https://mitre-attack.github.io/attack-navigator/#layerURL=https%3A%2F%2Fstorage.googleapis.com%2Fsoteria-detector-mapping%2F%2Fall.json)
* [Windows](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//windows.json)
* [Linux](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//linux.json)
* [macOS](https://mitre-attack.github.io/attack-navigator/#layerURL=https://storage.googleapis.com/soteria-detector-mapping//mac.json)

## Data Access

Please note that Soteria won't get access to your data, and you won't be able to see or edit their rules - LimaCharlie acts as a broker between the two parties.

The following events are utilized by Soteria rules. Please ensure that they are configured within your Organization:

* `CODE_IDENTITY`
* `DNS_REQUEST`
* `EXISTING_PROCESS`
* `FILE_CREATE`
* `FILE_MODIFIED`
* `MODULE_LOAD`
* `NETWORK_CONNECTIONS`
* `NEW_DOCUMENT`
* `NEW_NAMED_PIPE`
* `NEW_PROCESS`
* `REGISTRY_WRITE`
* `REGISTRY_CREATE`
* `SENSITIVE_PROCESS_ACCESS`
* `THREAD_INJECTION`

This can also be done in the Add-ons Marketplace.

## Enabling Soteria's EDR Rules

Soteria's EDR rules can be activated via two means.

### Activating via the Web UI

To enable Soteria's EDR ruleset, navigate to the **Extensions** section of the Add-On Marketplace and search for Soteria. You can also directly select `soteria-rules-edr`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to Soteria rules and click **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-2.png)

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/soteria-edr-3.png)

### Infrastructure as Code

Alternatively, to manage tenants and LimaCharlie functionality at scale, you can leverage our Infrastructure as Code functionality.

> **Endpoint Detection & Response**
> 
> In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Tutorial: Ingesting Telemetry from Cloud-Based External Sources

LimaCharlie allows for ingestion of logs or telemetry from any external source in real-time. It includes built-in parsing for popular formats, with the option to define your own for custom sources.

There are two ways to ingest logs or telemetry from external sources:

* Run the [LimaCharlie Adapter](/v2/docs/adapters) on premises or on your cloud
* Provide credentials for the destination and allow LimaCharlie cloud to connect directly (available for cloud-based Adapters)

To connect with the cloud-based external source, first ensure you have the appropriate `cloudsensor.*` permissions.

After the permissions have been enabled, navigate to the `Sensors` page of the web app and click `Add Sensor`.

Choose an external source you would like to ingest logs or telemetry from, or filter the list to only include `Cloud & External Sources` to see available options.

If there is an external source you wish to connect that is not listed, you can still ingest via the LimaCharlie Adapter with self-defined parsing. Alternatively, please contact us to discuss adding this source in LimaCharlie.

After selecting the Sensor type, choose or create an [Installation Key](/v2/docs/installation-keys). Then, enter the name for the sensor and provide method-specific credentials for connection.

If the sensor you selected is cloud-based, you will see the call to action `Complete Cloud Installation`.

*Note: Sensors that support cloud to cloud communication, can also be installed by running an adapter on-prem or on cloud hosted by the customer. While it is a rare scenario, some customers might prefer that option when they do not want to share the sensor's API credentials with LimaCharlie.*

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Windows Event Log

## Overview

This Adapter allows you to connect to the local Windows Event Logs API on Windows. This means this Adapter is only available from Windows builds and only works locally (will not connect to remote Windows instances).

## Configurations

Adapter Type: `wel`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `evt_sources`: a comma separated list of elements in the format `SOURCE:FILTER`, where `SOURCE` is an Event Source name like `Application`, `System` or `Security` and `FILTER` is an `XPath` filter value as described in the documentation linked below.

### Infrastructure as Code Deployment

```
# Windows Event Log (WEL) Specific Docs: https://docs.limacharlie.io/docs/adapter-types-windows-event-log

# Basic Event Sources:
# evt_sources: "Security,System,Application"

# With XPath Filters:
# evt_sources: "Security:'*[System[(Level=1 or Level=2 or Level=3)]]',System:'*[System[Provider[@Name=\"Microsoft-Windows-Kernel-General\"]]]'"

# File-Based Sources:
# evt_sources: "C:\\Windows\\System32\\winevt\\Logs\\Security.evtx:'*[System[(EventID=4624)]]'"

  wel:
    evt_sources: "Security:'*[System[(Level=1 or Level=2 or Level=3)]]',System,Application"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_WEL"
      hostname: "prod-dc01.example.local"
      platform: "windows"
      sensor_seed_key: "wel-collector"
    write_timeout_sec: 30
```

### XPath Filter Examples

Security Events (High Priority):

```
  Security:'*[System[(Level=1 or Level=2 or Level=3)]]'
```

Logon Events Only:

```
  Security:'*[System[(EventID=4624 or EventID=4625 or EventID=4634)]]'
```

System Errors:

```
  System:'*[System[(Level=1 or Level=2)]]'
```

Specific Provider:

```
  Application:'*[System[Provider[@Name="Microsoft-Windows-ApplicationError"]]]'
```

## API Doc

See the [official documentation](https://learn.microsoft.com/en-us/windows/win32/wes/consuming-events).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Windows Event Log

## Overview

This Adapter allows you to connect to the local Windows Event Logs API on Windows. This means this Adapter is only available from Windows builds and only works locally (will not connect to remote Windows instances).

## Configurations

Adapter Type: `wel`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `evt_sources`: a comma separated list of elements in the format `SOURCE:FILTER`, where `SOURCE` is an Event Source name like `Application`, `System` or `Security` and `FILTER` is an `XPath` filter value as described in the documentation linked below.

### Infrastructure as Code Deployment

```
# Windows Event Log (WEL) Specific Docs: https://docs.limacharlie.io/docs/adapter-types-windows-event-log

# Basic Event Sources:
# evt_sources: "Security,System,Application"

# With XPath Filters:
# evt_sources: "Security:'*[System[(Level=1 or Level=2 or Level=3)]]',System:'*[System[Provider[@Name=\"Microsoft-Windows-Kernel-General\"]]]'"

# File-Based Sources:
# evt_sources: "C:\\Windows\\System32\\winevt\\Logs\\Security.evtx:'*[System[(EventID=4624)]]'"

  wel:
    evt_sources: "Security:'*[System[(Level=1 or Level=2 or Level=3)]]',System,Application"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_WEL"
      hostname: "prod-dc01.example.local"
      platform: "windows"
      sensor_seed_key: "wel-collector"
    write_timeout_sec: 30
```

### XPath Filter Examples

Security Events (High Priority):

```
  Security:'*[System[(Level=1 or Level=2 or Level=3)]]'
```

Logon Events Only:

```
  Security:'*[System[(EventID=4624 or EventID=4625 or EventID=4634)]]'
```

System Errors:

```
  System:'*[System[(Level=1 or Level=2)]]'
```

Specific Provider:

```
  Application:'*[System[Provider[@Name="Microsoft-Windows-ApplicationError"]]]'
```

## API Doc

See the [official documentation](https://learn.microsoft.com/en-us/windows/win32/wes/consuming-events).

---

# Windows Event Logs

This example shows collecting Windows Event Logs (`wel`) from a Windows box natively (and therefore is only available using the Windows Adapter). This is useful for cases where you'd like to collect WEL without running the LimaCharlie Windows Agent.

```
./lc_adapter wel client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d `
    client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd `
    client_options.sensor_seed_key=domain-controller1 `
    client_options.platform=wel `
    evt_sources=security:*,application:*,system:*,Microsoft-Windows-Windows Defender/Operational:*
```

Here's a breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `wel`: the method the Adapter should use to collect data locally. The `wel` value will use a native local Windows Event Logs subscription.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=wel`: this indicates the type of data that will be received from this adapter. In this case it's `wel` events.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `evt_sources=....`: a comma separated list of event channel to collect along with a XPath filter expression for each. The format is `CHANNEL_NAME:FILTER_EXPRESSION` where a filter of `*` means all events. Common channels: `security`, `system` and `application`.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Windows Event Logs

This example shows collecting Windows Event Logs (`wel`) from a Windows box natively (and therefore is only available using the Windows Adapter). This is useful for cases where you'd like to collect WEL without running the LimaCharlie Windows Agent.

```
./lc_adapter wel client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d `
    client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd `
    client_options.sensor_seed_key=domain-controller1 `
    client_options.platform=wel `
    evt_sources=security:*,application:*,system:*,Microsoft-Windows-Windows Defender/Operational:*
```

Here's a breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `wel`: the method the Adapter should use to collect data locally. The `wel` value will use a native local Windows Event Logs subscription.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=wel`: this indicates the type of data that will be received from this adapter. In this case it's `wel` events.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `evt_sources=....`: a comma separated list of event channel to collect along with a XPath filter expression for each. The format is `CHANNEL_NAME:FILTER_EXPRESSION` where a filter of `*` means all events. Common channels: `security`, `system` and `application`.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# FAQ

# FAQ - Account Management

## How Can I Create More Than Two Organizations?

By default, LimaCharlie has a limit of two organizations. If you need to create more organizations, please reach out to the support team and we will change this limit.

## How Do I Delete an Organization?

Please navigate to the bottom of the Billing & Usage section of the organization you want to delete, and click Delete Organization button. Note that this action is final and cannot be undone.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/account-3.png)

## Is There a Way to Wipe an Organization?

You can wipe the data retention by disabling the `Insight` add on on the marketplace and re-enabling it again. Please note that unsubscribing from `Insight` will delete all telemetry stored for a selected organization, and this action cannot be undone.

To wipe the configuration, you can use Templates / Infrastructure as Code functionality with the `is_force` flag to remove everything. To learn more about the infrastructure as code, visit [Infrastructure Extension](/v2/docs/ext-infrastructure).

## Can I Transfer Ownership of an Organization?

You can transfer ownership of an organization to any other entity. The request needs to be initiated by the current owner (billing or legal contact) of the organization. To do so, contact support@limacharlie.io.

## I Created an Account and Have Been Given Access, but I Do Not Seem to Have Access to Other Organizations.

With LimaCharlie's granular role-based access control you can be granted access in one of two ways:

* On a per-organization basis
* To a set of organizations using [Organization Groups](/v2/docs/user-access)

You'll want to ask the person who granted access if they added you to the individual organizations, or if they'd set up an organization group. Either method works, but they'll have to ensure that either you're added to each organization individually, or that they set up a group.

## How Can I Update My Time Zone?

All dates and times displayed in the web app follow the user preferred time zone.

To set your time zone, navigate to the settings icon in the right hand corner and select `Manage User Settings`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/account-1.png)

You can set your preferred time zone under `Display` section of the `User Settings`; all changes are saved automatically.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/account-2.png)

## How Can I Unsubscribe/Cancel/Delete My Limacharlie Account?

You can unsubscribe / cancel your subscription from app.limacharlie.io by logging in and going to the Billing & Usage under the Billing section. Click the Delete Organization button at the bottom of the page and follow the instructions on screen.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/account-3.png)

## Why Didn't I Receive My Account Activation Email?

Account activation emails are sent when you sign up for a new LimaCharlie account. If you do not see the activation email in your inbox, it can typically be found in a spam / junk folder. If you're a user of Microsoft Office 365, or similar service that has server-side filtering, you may wish to check your online Quarantine (or equivalent). See the [Microsoft instructions](https://docs.microsoft.com/en-us/microsoft-365/security/office-365-security/quarantine-email-messages?view=o365-worldwide) for details.

Please reach out to our support team and we can verify if a successful delivery response message was received from your mail server.

---

# FAQ - Billing

This page contains frequently asked questions about billing within LimaCharlie.

Pricing Details

Please note that our pricing is transparent, and is available via our [Pricing webpage](https://limacharlie.io/pricing).

## How Can I Change My Quota/Upgrade to the Paid Tier?

When you sign up for the LimaCharlie account, you will automatically be on a free tier, allowing you to create two organizations with two sensors each. All add-ons and additional services are free on this tier.

To upgrade to paid tier, simply navigate to the Setup section of the Organization you are looking to upgrade and perform the following actions:

1. Ensure you have a payment method on file by clicking the **Billing & Usage** tab.
2. In the **Billing & Usage** tab, set the quota number you would like and click **Update Quota**. Quota is the number of sensors concurrently online you would like to support.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/billing-1.png)

## What is the Cost of Deploying Payloads via LimaCharlie?

Payload pricing is provided via our [pricing page](https://limacharlie.io/pricing). For example, assume deploying Payloads via LimaCharlie costs $0.19 per 1 GB of data sent. A 1GB payload sent to 10 endpoints will cost $1.9 (10GBs x  $0.19).

This only impacts organizations that leverage Payloads functionality, as well as Atomic Red Team and Dumper services (they are running as Payloads in LC).

To understand the impact on your organization, check the **Metered Usage** section of the **Billing** page. You will notice the new **Payload Data Sent** metric along with the size of payloads deployed and price.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/billing-2.png)

## What is Usage-Based Billing?

Along with our predictable per endpoint pricing model, LimaCharlie offers a pure usage-based billing model for our Endpoint Detection & Response (EDR) capability. Pricing within this model is calculated solely on the time the Sensor is connected, events processed, and events stored. You can find more information about our billing options [here](/v2/docs/billing).

We acknowledge that some might not need the entirety of available components all the time, and might benefit from having access to an Endpoint Agent on an ad-hoc basis. This approach enables the following:

1. Incident responders will now be able to offer pre-deployments to their customers at almost zero cost. That is, they can deploy across an organization's entire fleet and lay dormant in [sleeper mode](/v2/docs/sleeper). With agents deployed ahead of an incident, responders can offer competitive SLA's.
2. Product developers can take advantage of usage-based billing to leverage narrow bands of functionality at a low cost. This means getting the functionality they need without building it from the ground up or paying for a full EDR deployment.

## For Lc Adapters Billed on Usage, What Does "Block of Data" Mean & How Will It Impact the Price I Pay?

Some LimaCharlie Adapters are billed based on usage. Updated pricing details can be found on our [pricing page](https://limacharlie.io/pricing).

For example, assume $0.15 per block of data of 1 GB (on the organizational level). This means that 10 adapters with less than 1 GB (total) in the same organization will be $0.15 total for that month.

## How Do I Determine How Much I Need to Pay for an Org If It Was in Usage-Based Billing Mode?

If the organization you are trying to assess has [1-year telemetry retention](https://app.limacharlie.io/add-ons/detail/insight) enabled, you could use the stats API to see the number of events retained:

`https://api.limacharlie.io/v1/usage/OID`
or
`https://api.limacharlie.io/static/swagger/#/Organizations/getOrgUsageStats`

You will want to check the `sensor_events` and `sensor_retained` values.

## How Is the Price of Sensors & Add-Ons Calculated in LimaCharlie?

There are two categories of Sensors: sensors billed on Quota set by the user (vSensor basis) and sensors billed on usage basis.

### vSensors

LimaCharlie has the concept of a vSensor. A vSensor is a virtual sensor used for the purpose of setting up quota and billing of [Endpoint Agents](/v2/docs/endpoint-agent). vSensor pricing matches that listed on our pricing page, and includes a year of full telemetry storage.

Our transparent pricing and quota-based approach allows you to easily mix and match deployments, while staying within a certain price point.

If you set the quota to 100 vSensors, you can have concurrently:

* 50 Windows Sensors + 50 Linux Sensors, OR
* 20 Windows Sensors + 30 Linux Sensors + 50 macOS Sensors, OR
* 100 macOS Sensors
* Or any other combination as long as the total number of sensors does not exceed the quota of 100 vSensors.

### Sensors Over Quota

If the quota is maxed out when a sensor attempts to come online, the sensor will be given a message to go away for a period of time and then they can check again. A `sensor_over_quota` event will be emitted in the deployments stream as well enabling users to set up alerts and be notified about this happening. The amount of time sensors are told to go away for increases if they connect again and the organization is still over quota.

## When Will My Credit Card Be Charged?

Quota-based items are charged a month ahead, while usage items are billed the month following, similar to most cellphone invoices (or hosting).

## How Do I Change My Billing Credit Card?

If you are using a credit card for payment and wish to change your address or card details, navigate to **Billing > Billing & Usage** within the web UI. From there, select **Change Payment Details** to update the appropriate details.

---

# FAQ - Billing

This page contains frequently asked questions about billing within LimaCharlie.

Pricing Details

Please note that our pricing is transparent, and is available via our [Pricing webpage](https://limacharlie.io/pricing).

## How Can I Change My Quota/Upgrade to the Paid Tier?

When you sign up for the LimaCharlie account, you will automatically be on a free tier, allowing you to create two organizations with two sensors each. All add-ons and additional services are free on this tier.

To upgrade to paid tier, simply navigate to the Setup section of the Organization you are looking to upgrade and perform the following actions:

1. Ensure you have a payment method on file by clicking the **Billing & Usage** tab.
2. In the **Billing & Usage** tab, set the quota number you would like and click **Update Quota**. Quota is the number of sensors concurrently online you would like to support.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/billing-1.png)

## What is the Cost of Deploying Payloads via LimaCharlie?

Payload pricing is provided via our [pricing page](https://limacharlie.io/pricing). For example, assume deploying Payloads via LimaCharlie costs $0.19 per 1 GB of data sent. A 1GB payload sent to 10 endpoints will cost $1.9 (10GBs x  $0.19).

This only impacts organizations that leverage Payloads functionality, as well as Atomic Red Team and Dumper services (they are running as Payloads in LC).

To understand the impact on your organization, check the **Metered Usage** section of the **Billing** page. You will notice the new **Payload Data Sent** metric along with the size of payloads deployed and price.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/billing-2.png)

## What is Usage-Based Billing?

Along with our predictable per endpoint pricing model, LimaCharlie offers a pure usage-based billing model for our Endpoint Detection & Response (EDR) capability. Pricing within this model is calculated solely on the time the Sensor is connected, events processed, and events stored. You can find more information about our billing options [here](/v2/docs/billing).

We acknowledge that some might not need the entirety of available components all the time, and might benefit from having access to an Endpoint Agent on an ad-hoc basis. This approach enables the following:

1. Incident responders will now be able to offer pre-deployments to their customers at almost zero cost. That is, they can deploy across an organization's entire fleet and lay dormant in [sleeper mode](/v2/docs/sleeper). With agents deployed ahead of an incident, responders can offer competitive SLA's.
2. Product developers can take advantage of usage-based billing to leverage narrow bands of functionality at a low cost. This means getting the functionality they need without building it from the ground up or paying for a full EDR deployment.

## For Lc Adapters Billed on Usage, What Does "Block of Data" Mean & How Will It Impact the Price I Pay?

Some LimaCharlie Adapters are billed based on usage. Updated pricing details can be found on our [pricing page](https://limacharlie.io/pricing).

For example, assume $0.15 per block of data of 1 GB (on the organizational level). This means that 10 adapters with less than 1 GB (total) in the same organization will be $0.15 total for that month.

## How Do I Determine How Much I Need to Pay for an Org If It Was in Usage-Based Billing Mode?

If the organization you are trying to assess has [1-year telemetry retention](https://app.limacharlie.io/add-ons/detail/insight) enabled, you could use the stats API to see the number of events retained:

`https://api.limacharlie.io/v1/usage/OID`
 or
`https://api.limacharlie.io/static/swagger/#/Organizations/getOrgUsageStats`

You will want to check the `sensor_events` and `sensor_retained` values.

## How Is the Price of Sensors & Add-Ons Calculated in LimaCharlie?

There are two categories of Sensors: sensors billed on Quota set by the user (vSensor basis) and sensors billed on usage basis.

### vSensors

LimaCharlie has the concept of a vSensor. A vSensor is a virtual sensor used for the purpose of setting up quota and billing of [Endpoint Agents](/v2/docs/endpoint-agent). vSensor pricing matches that listed on our pricing page, and includes a year of full telemetry storage.

Our transparent pricing and quota-based approach allows you to easily mix and match deployments, while staying within a certain price point.

If you set the quota to 100 vSensors, you can have concurrently:

* 50 Windows Sensors + 50 Linux Sensors, OR
* 20 Windows Sensors + 30 Linux Sensors + 50 macOS Sensors, OR
* 100 macOS Sensors
* Or any other combination as long as the total number of sensors does not exceed the quota of 100 vSensors.

### Sensors Over Quota

If the quota is maxed out when a sensor attempts to come online, the sensor will be given a message to go away for a period of time and then they can check again. A `sensor_over_quota` event will be emitted in the deployments stream as well enabling users to set up alerts and be notified about this happening. The amount of time sensors are told to go away for increases if they connect again and the organization is still over quota.

## When Will My Credit Card Be Charged?

Quota-based items are charged a month ahead, while usage items are billed the month following, similar to most cellphone invoices (or hosting).

## How Do I Change My Billing Credit Card?

If you are using a credit card for payment and wish to change your address or card details, navigate to **Billing > Billing & Usage** within the web UI. From there, select **Change Payment Details** to update the appropriate details.

---

# FAQ - General

## Is my data secure with LimaCharlie?

LimaCharlie data is secured starting at the endpoint all the way through your infrastructure. The LimaCharlie platform is hosted on the Google Cloud Platform, leveraging multiple capabilities from credentials management to compute isolation in order to limit the attack surface.

Data access is managed through Google Cloud IAM which is used to isolate various components and customer data. Processing is done in Google Kubernetes Engine which provides an additional layer of container isolation.

Each LimaCharlie data center uses independent cryptographic keys at all layers. Key management uses industry best practices such as key encryption at rest.

LimaCharlie is SOC 2 Type 2 and PCI-DSS compliant. Our infrastructure is housed in ISO 27001 compliant data centres.

## Where will my data be processed and stored?

The LimaCharlie global infrastructure is built on the Google Cloud Platform (GCP). Currently, computing resources are available in the USA, Canada, Europe, India, and the United Kingdom. New data centers can be spun up anywhere GCP is available upon request.

When you set up an Organization for the first time, you can select the Data Residency Region of your choice:

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/new org.png)

This provides you with the benefit of being able to select which GCP region you want your data in, and have assurance that it will always be processed in this location and never moved outside. This can be important for data residency requirements as it relates to regulatory compliance. For example, if you want to keep all of your information in the US, you can simply select the US region and know that your data will be both processed and stored there.

> **Note:** Once a region has been selected for an organization, it cannot be changed later.

## Can LimaCharlie staff access my data?

LimaCharlie staff only access your private data when you contact us and give us permission to do so. We will always ask for your permission before we access your private telemetry data.

We consider your sensors and telemetry data to be private and confidential. We understand the tremendous power that is being entrusted to us while we have access to this data. We promise to only access your organization for the exclusive purpose of providing you with the assistance you request from us. We treat your private and confidential information with at least the same due care as we do with our own confidential information, as outlined in our privacy policy.

## Will third parties get access to my data?

The only time we provide your data to a third party is with your explicit consent. (e.g. when you set up an Output in LimaCharlie, you're explicitly telling us to send your data to a 3rd party).

## What control measures do you have in place to ensure that my data won't be accessed without proper authorizations?

We use transparency as a mitigating control against insider threats. In particular, when we access your organization data, an entry is made to the audit log in your organization. You can access the audit log in the web interface and via the API. We also provide the ability for you to send audit log data out of LimaCharlie immediately to a write-only bucket that you control in your own environment.

We use a break-glass system, meaning that LimaCharlie personnel do not have access to customer data by default. This requires an explicit programmatic action (internal to LimaCharlie) that includes its own audit trail that cannot be modified by LimaCharlie staff. This audit trail is regularly reviewed.

LimaCharlie staff access to customer data is restricted to only those who need it to perform their official duties.

LimaCharlie staff must explicitly request permission from the customer before granting access to any data or systems (other than in emergency cases where infrastructure is at risk).

We use role-based access control systems to provide granular control over the type of data access granted.

Access to customer organizations is granted programmatically as to provide a security control.

We require that our staff undergo a background check and take training, including privacy training, prior to being allowed to access customer data.

We are SOC 2 (Type 2) compliant and a copy of our audit report can be provided upon request.

## What is detected by LimaCharlie after it's initially installed?

When the Sensor is installed, LimaCharlie will start recording the telemetry. It will not, however, generate detections or take actions to protect the endpoints automatically. As an infrastructure company, we recognize that each environment is different, and one size fits all approach rarely works well. By default, we take the AWS approach - any new organization starts empty, without any pre-configured settings, add-ons, or rules.

## Can LimaCharlie be deployed on-premises?

LimaCharlie is a cloud-based solution. The LimaCharlie platform is hosted on the Google Cloud Platform (GCP). There are no limits between AWS & GCP but LimaCharlie is not available on premises; if you configure the sensor on the endpoint, it will connect to the cloud.

## Does LimaCharlie detect variants of the latest malware?

When the sensor is installed, LimaCharlie will start recording telemetry. It will not, however, generate detections or take actions to protect the endpoints automatically. As an infrastructure company, we recognize that each environment is different, and one size fits all approach rarely works well. By default, any new organization starts empty, without any pre-configured settings, add-ons, or D&R rules.

LimaCharlie makes it easy to add a detection & response rule as soon as new variants of malware are discovered. This way, you are in a full control of your coverage and there is no need to wait for a vendor to come up with a new detection rule.

## What latency can I expect in LimaCharlie?

LimaCharlie Detection & Response (D&R) engine has very low latency and you can expect that responses are almost instantaneous (e.g. 100ms).

You may notice some latency as it relates to outputs. Some of our outputs are done in batches, such as Amazon S3, SFTP, Google Cloud Storage. You can configure the maximum size and maximum time for these outputs. We also offer live outputs, such as Syslog.

## How can I integrate LimaCharlie with my existing SIEM?

The most common use case we see is sending detections and events data from LimaCharlie into the SIEM.

To do it, you will need to configure outputs. Here are some examples for configuring outputs to go to an email or to Chronicle.

Remember to select the type of data forwarded by this configuration (stream). The available options are as follows:

* **event**: Contains all events coming back from sensors (not cloud detections). It is very verbose.
* **detect**: Contains all detections reported from D&R rules or subscriptions. This is the option you would choose if you want detections to generate emails (you would also need to ensure that D&R rules are configured to generate detections).
* **audit**: Contains auditing events about activity around the management of the platform in the cloud.
* **deployment**: Contains all "deployment" events like sensor enrollment, cloned sensors etc.
* **artifact**: Contains all "artifact" events of files collected through the Artifact Collection mechanism.

While sending detections and events data from LimaCharlie into the SIEM is the most common way we see our users set up the integration between these two systems, you can also bring in the data into LimaCharlie from SIEM or build other custom workflows. Contact our support team if you need help with your use case or if you have further questions.

## What is the retention policy for management/audit logs?

LimaCharlie stores management/audit logs for one year.

We suggest you set up an Output to send logs to an external destination if you are looking to have your logs stored for over one year.

## Does LimaCharlie offer reporting capabilities?

It is very common for users to bring different log, network and endpoint data into the LimaCharlie to leverage our detection and response, advanced correlation and storage. If you wish to leverage data visualization capabilities, we make it easy to send the data you need to Splunk, Tableau or any other solution of your choice via public API.

In LimaCharlie web app, you can track information such as detections and events over time and number of sensors online.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/dashboard.png)

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# FAQ - General

## Is my data secure with LimaCharlie?

LimaCharlie data is secured starting at the endpoint all the way through your infrastructure. The LimaCharlie platform is hosted on the Google Cloud Platform, leveraging multiple capabilities from credentials management to compute isolation in order to limit the attack surface.

Data access is managed through Google Cloud IAM which is used to isolate various components and customer data. Processing is done in Google Kubernetes Engine which provides an additional layer of container isolation.

Each LimaCharlie data center uses independent cryptographic keys at all layers. Key management uses industry best practices such as key encryption at rest.

LimaCharlie is SOC 2 Type 2 and PCI-DSS compliant. Our infrastructure is housed in ISO 27001 compliant data centres.

## Where will my data be processed and stored?

The LimaCharlie global infrastructure is built on the Google Cloud Platform (GCP). Currently, computing resources are available in the USA, Canada, Europe, India, and the United Kingdom. New data centers can be spun up anywhere GCP is available upon request.

When you set up an Organization for the first time, you can select the Data Residency Region of your choice:

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/new org.png)

This provides you with the benefit of being able to select which GCP region you want your data in, and have assurance that it will always be processed in this location and never moved outside. This can be important for data residency requirements as it relates to regulatory compliance. For example, if you want to keep all of your information in the US, you can simply select the US region and know that your data will be both processed and stored there.

> **Note:** Once a region has been selected for an organization, it cannot be changed later.

## Can LimaCharlie staff access my data?

LimaCharlie staff only access your private data when you contact us and give us permission to do so. We will always ask for your permission before we access your private telemetry data.

We consider your sensors and telemetry data to be private and confidential. We understand the tremendous power that is being entrusted to us while we have access to this data. We promise to only access your organization for the exclusive purpose of providing you with the assistance you request from us. We treat your private and confidential information with at least the same due care as we do with our own confidential information, as outlined in our privacy policy.

## Will third parties get access to my data?

The only time we provide your data to a third party is with your explicit consent. (e.g. when you set up an Output in LimaCharlie, you're explicitly telling us to send your data to a 3rd party).

## What control measures do you have in place to ensure that my data won't be accessed without proper authorizations?

We use transparency as a mitigating control against insider threats. In particular, when we access your organization data, an entry is made to the audit log in your organization. You can access the audit log in the web interface and via the API. We also provide the ability for you to send audit log data out of LimaCharlie immediately to a write-only bucket that you control in your own environment.

We use a break-glass system, meaning that LimaCharlie personnel do not have access to customer data by default. This requires an explicit programmatic action (internal to LimaCharlie) that includes its own audit trail that cannot be modified by LimaCharlie staff. This audit trail is regularly reviewed.

LimaCharlie staff access to customer data is restricted to only those who need it to perform their official duties.

LimaCharlie staff must explicitly request permission from the customer before granting access to any data or systems (other than in emergency cases where infrastructure is at risk).

We use role-based access control systems to provide granular control over the type of data access granted.

Access to customer organizations is granted programmatically as to provide a security control.

We require that our staff undergo a background check and take training, including privacy training, prior to being allowed to access customer data.

We are SOC 2 (Type 2) compliant and a copy of our audit report can be provided upon request.

## What is detected by LimaCharlie after it's initially installed?

When the Sensor is installed, LimaCharlie will start recording the telemetry. It will not, however, generate detections or take actions to protect the endpoints automatically. As an infrastructure company, we recognize that each environment is different, and one size fits all approach rarely works well. By default, we take the AWS approach - any new organization starts empty, without any pre-configured settings, add-ons, or rules.

## Can LimaCharlie be deployed on-premises?

LimaCharlie is a cloud-based solution. The LimaCharlie platform is hosted on the Google Cloud Platform (GCP). There are no limits between AWS & GCP but LimaCharlie is not available on premises; if you configure the sensor on the endpoint, it will connect to the cloud.

## Does LimaCharlie detect variants of the latest malware?

When the sensor is installed, LimaCharlie will start recording telemetry. It will not, however, generate detections or take actions to protect the endpoints automatically. As an infrastructure company, we recognize that each environment is different, and one size fits all approach rarely works well. By default, any new organization starts empty, without any pre-configured settings, add-ons, or D&R rules.

LimaCharlie makes it easy to add a detection & response rule as soon as new variants of malware are discovered. This way, you are in a full control of your coverage and there is no need to wait for a vendor to come up with a new detection rule.

## What latency can I expect in LimaCharlie?

LimaCharlie Detection & Response (D&R) engine has very low latency and you can expect that responses are almost instantaneous (e.g. 100ms).

You may notice some latency as it relates to outputs. Some of our outputs are done in batches, such as Amazon S3, SFTP, Google Cloud Storage. You can configure the maximum size and maximum time for these outputs. We also offer live outputs, such as Syslog.

## How can I integrate LimaCharlie with my existing SIEM?

The most common use case we see is sending detections and events data from LimaCharlie into the SIEM.

To do it, you will need to configure outputs. Here are some examples for configuring outputs to go to an email or to Chronicle.

Remember to select the type of data forwarded by this configuration (stream). The available options are as follows:

* **event**: Contains all events coming back from sensors (not cloud detections). It is very verbose.
* **detect**: Contains all detections reported from D&R rules or subscriptions. This is the option you would choose if you want detections to generate emails (you would also need to ensure that D&R rules are configured to generate detections).
* **audit**: Contains auditing events about activity around the management of the platform in the cloud.
* **deployment**: Contains all "deployment" events like sensor enrollment, cloned sensors etc.
* **artifact**: Contains all "artifact" events of files collected through the Artifact Collection mechanism.

While sending detections and events data from LimaCharlie into the SIEM is the most common way we see our users set up the integration between these two systems, you can also bring in the data into LimaCharlie from SIEM or build other custom workflows. Contact our support team if you need help with your use case or if you have further questions.

## What is the retention policy for management/audit logs?

LimaCharlie stores management/audit logs for one year.

We suggest you set up an Output to send logs to an external destination if you are looking to have your logs stored for over one year.

## Does LimaCharlie offer reporting capabilities?

It is very common for users to bring different log, network and endpoint data into the LimaCharlie to leverage our detection and response, advanced correlation and storage. If you wish to leverage data visualization capabilities, we make it easy to send the data you need to Splunk, Tableau or any other solution of your choice via public API.

In LimaCharlie web app, you can track information such as detections and events over time and number of sensors online.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/dashboard.png)

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# FAQ - Invoices

This page contains frequently asked questions about invoices you receive for LimaCharlie service.

## Pricing Details

Please note that our pricing is transparent and is available via our [Pricing webpage](https://limacharlie.io/pricing).

## LimaCharlie Invoices

LimaCharlie offers two types of invoices:

* Individual Organization
* Unified billing

We'll examine each in detail.

### Individual organization invoices

Your invoice will include a detailed breakdown of usage for your LimaCharlie tenant organization. You'll see individual line items for each LimaCharlie product utilized, along with your actual usage for the period. For example:

* Sensors
* Output usage
* Artifact ingestion
* Replay usage

Invoices cover both lines for standard billable items like Sensor quota which are pre-paid for the following month, as well as consumption-based items (e.g. per-gigabyte costs incurred throughout the prior period) which are post-paid after the month has ended.

Your monthly invoices include a detailed breakdown enable you to see the exact periods covered for each product listed.

Because you are able to adjust your organization quota on demand, this will trigger proration of charges. You will see line items on your invoice which indicate "Remaining time on ..." or "Unused time on ..." that are related to the proration, which is done on a per-second basis.

### Unified Billing invoices

Customers who are set up on Unified Billing receive one invoice that contain a roll-up summary of all of their LimaCharlie organizations so that they can pay them all together. The Unified invoice includes one line item per tenant organization. Those tenant organizations include a reference to their sub-invoice number; you can refer to those for detailed line-level information related to each organization.

In addition to the Unified Billing invoice, customers are also provided with a LimaCharlie Global Billing email. This email contains:

1. A table showing all organizations included in the period, along with a link to each individual organization's detailed invoice which shows breakdown of charges. Note that these individual invoices have a zero-dollar balance as the amounts are reflected on the Unified Invoice; this is reflected with a line item called "UNIFIED-BILLING" that shows the invoice total was moved to the unified invoice.
2. A summary report (attachment) in CSV format that contains a list of the organizations included on the global billing invoice. The fields included in the CSV are as follows:
    * A - Org Name
    * B - Org ID
    * C - Payment
    * D - Sub-total
    * E - Total Due
    * F - Total Paid

## Key Terms

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Sensors**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Organization ID (OID)**: In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

---

# FAQ - Sensor Troubleshooting

## Why is there no output in the console?

When running Sensor [console commands](/v2/docs/endpoint-agent-commands), you may encounter a "spinning wheel" or no output back from the Sensor. Oftentimes, this is due to the *response* event not enabled in [Event Collection](/v2/docs/ext-exfil). You will need to configure the response event in order to receive feedback in the console.

For example, the `os_users` Sensor command has two components:

* `OS_USERS_REQ` is the *request* event sent to the Sensor to collect OS user information.
* `OS_USERS_REP` is the *response* event sent back by the Sensor containing the information of interest.

Please ensure that you are collecting the `*_REP` events in order to display output in the console.

## Sensor Not Showing as Online

### Determining Online Status

It is important to note that the online marker in the Web UI does not display real-time information. Instead it refreshes its status between every 30 seconds to every few minutes, depending on the page in question.

This means that an icon showing a sensor as not online may be lagging behind the actual status. If you need to get a positive feedback on whether the sensor is online or not, go to the "Sensors" page which refreshes status more often. Moving to the "Sensors" page also triggers a refresh of the status right away.

### Reasons for Temporary Disconnect

Sensors connect to the cloud via a semi-persistent SSL connection. In general, if a host has connectivity to the internet, the sensor should be online. There are, however, a few situations that result in the sensor temporarily disconnecting from the cloud for a few seconds. This means that if you notice a sensor is offline when you expect it to be online, give it 30 seconds, and in most situations it will come back online within 5 seconds.

## Sensor Not Connecting

Sensors connect to the LimaCharlie.io cloud via an SSL connection on port 443. Make sure your network allows such a connection. It is a very common port typically used for HTTPS so an issue is highly unlikely.

The sensor uses a pinned SSL certificate to talk to the cloud. This means that if you are in a network that enforces SSL inspection (a man-in-the-middle of the SSL connections sometimes used in large corporate environments), this may prevent the sensor from connecting. LimaCharlie uses a pinned certificate to ensure the highest level of security possible, as usage of off-the-shelf certificates can be leveraged by state-sponsored (or advanced) attackers.

If your network uses SSL inspection, we recommend you setup an exception for the LimaCharlie cloud domain relevant to you. Get in touch with us and we can provide you with the necessary information.

Sensors since version 4.21.2 also generate a local log file able to be used to help pinpoint the level at which the connectivity fails. This log file is located:

* Windows: `c:\windows\system32\hcp.log`
* MacOS: `/usr/local/hcp.log`
* Linux: `./hcp.log`

This log provides a simple line for each basic step of connectivity to the cloud. It only logs the first connection attempted to the cloud and rolls over every time the sensor starts. A successful connection should look like:

```
hcp launched
configs applied
conn started
connecting
ssl connected
headers sent
channel up
```

If you are having trouble getting your sensor connected to the cloud, we recommend that you attempt the following on the host:

1. Restart the LimaCharlie service.
2. Check that the service is running.

   * The service process should be called `rphcp`.
3. If the sensor still shows as not online, check the `hcp.log` file mentioned above:

   * Check that the "configs applied" step is reached. If not, it may indicate the Installation Key provided is wrong or has a typo.
   * Check that the proxy is mentioned in the log if you are using a proxy configuration.
   * Check that the "ssl connected" step is reached. If not, this indicates a network configuration issue connecting to the cloud.
   * Check that the "channel up" step is reached. If not, this could indicate one of a few things:

     + Your sensor was deleted (through API or Web interface) from the org. If so, reinstall to get a new identity.
     + Your Organization may be out-of-quota if more sensors than the maximum number you've set in the Billing section are trying to connect at once. Increase your quota and wait a few minutes to fix it.
     + If this is a brand new sensor install, make sure the Installation Key you're using still exists in your Org. Once deleted, an Installation Key cannot be used for NEW sensors, but old sensors that were installed using it will still work fine.

## Sensor Not Responding

Your sensor shows up as "online", but does not respond to interactive tasking.

The most common cause of this problem is a partial uninstall and reinstall of the sensor on the host. The sensor, when installed, creates local files that record the identity the sensor has with the cloud.

When uninstalling, the `-r` mode leaves these identification files behind, so that if you reinstall a new version of the sensor which talks to the same Org in LimaCharlie, the Sensor ID will be the same. On the other hand, the `-c` mode will remove all the identity files as well.

If you uninstall with `-r` and re-enroll the sensor to a different Org, as can often happen during testing, the files on disk that include some cryptographic material will not match with what the cloud expects. This may result in taskings being refused by the sensor.

To make sure this is not what's happening, uninstall the sensor with `-c`. Double-check that the local files `hcp`, `hcp_hbs` and `hcp_conf` are deleted before reinstalling. On Windows these should be in `c:\windows\system32` while on macOS they should be in `/usr/local`.

## Sensor Duplication

Sensor duplication can occur during certain types of installation or deployments, e.g. creation of virtual systems via a "gold image" that has LimaCharlie pre-installed.

However, in niche cases we have seen examples of:

1. LimaCharlie unable to write it's own identity files to disk, causing a constant "new" sensor connection.
2. Third-party security software on the system incorrectly categorizing LimaCharlie as malware, and killing the process before it can start.

One method to troubleshoot and determine root cause is to utilize Sysinternals' [DebugView](https://learn.microsoft.com/en-us/sysinternals/downloads/debugview) to investigate the error caused during Sensor installaton/start-up.

Another quick troubleshooting technique may be to determine whether the Sensor process `rphcp.exe`

## Upgrading Sensors

To ensure the sensor version is up-to-date, open the "Install Sensors" page in the web app (under "Setup") and navigate to the "Upgrading Sensors" section.

Upgrading sensors is done transparently for you once you click the button in the web app interface. You do not need to re download installers (in fact the installer stays the same). The new version should be in effect across the organization within about 20 minutes.

## How can I tell which version of the sensor is running locally?

The LimaCharlie sensor outputs a status file on the endpoint which allows you to see the:

* Sensor ID,
* Organization ID,
* Sensor version, and
* the agent's service uptime.

You can find this log data at the following location, based on your platform:

| Platform | File Path |
| --- | --- |
| Linux | `/opt/limacharlie/hcp_hbs_status.json` |
| macOS | `/Library/Application Support/limacharlie/hcp_hbs_status.json` |
| Windows | `c:\programdata\limacharlie\hcp_hbs_status.json` |

The log data is formatted similarly to the example below:

```
{
      "version": "4.33.0",
      "sid": "be8bc53b-36b2-469d-a914-716d629cb2d8",
      "oid": "d02c08e4-aedc-45eb-88aa-98b09b7d92df",
      "last_update": 1738872790,
      "uptime": 127
}
```

## Sensor Troubleshooting Utility

In some cases we may ask you for sensor health information from an endpoint that is having issues. To get this information, run the LC sensor interactively in the terminal with the -H flag.

On macOS run the command: `sudo /usr/local/bin/rphcp -H`

The diagnostic information will be displayed on screen, and saved to a file. The location of the output file will be shown at the bottom of the message shown on screen (on macOS, typically at `/Library/Application Support/limacharlie/``).

Note that the Sensor Troubleshooting Utility requires sensor [version 4.33.6](https://community.limacharlie.com/t/release-agent-with-sensor-troubleshooting-tool-webapp-4-2-3/276) or newer to be installed on disk on the impacted endpoint.

You can find the output file at the following location, based on your platform:

| Platform | File Path |
| --- | --- |
| Linux | `/opt/limacharlie/sensor_health_YYYY_MM_DD_HH_MM.json` |
| macOS | `/Library/Application Support/limacharlie/sensor_health_YYYY_MM_DD_HH_MM.json` |
| Windows | `c:\programdata\limacharlie\sensor_health_YYYY_MM_DD_HH_MM.json` |

The log data is formatted similarly to the example below:

```
{
  "system": {
    "memory_total": 25769803776,
    "memory_used": 13423722496,
    "name": "Darwin",
    "kernel": "24.4.0",
    "version": "15.4.1",
    "hostname": "Mac",
    "cpu_count": 8,
    "process_list": [

    ]
  },
  "agent": {
    "agent_info": {
      "MacOS": {
        "process": {
          "Ok": {
            "pid": 2024,
            "ppid": 2023,
            "cpu_usage": 0.0,
            "cwd": "/Users/username/Downloads",
            "exe": "/usr/local/bin/rphcp",
            "start_time": 1745890277,
            "run_time": 1,
            "memory": 10125312,
            "virtual_memory": 420875878400,
            "command_line": [
              "/usr/local/bin/rphcp",
              "-H"
            ]
          }
        },
        "agent_service": {
          "Ok": {
            "name": "com.refractionpoint.rphcp",
            "pid": 1521,
            "state": "running",
            "service_type": null,
            "launchd_config": "/Library/LaunchDaemons/com.refractionpoint.rphcp.plist",
            "launchd_type": "LaunchDaemon",
            "program": "/usr/local/bin/rphcp",
            "restart_count": 1,
            "last_signal": null
          }
        },
        "system_extension_process": {
          "Ok": {
            "pid": 1638,
            "ppid": 1,
            "cpu_usage": 0.0,
            "cwd": "/",
            "exe": "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension",
            "start_time": 1745889761,
            "run_time": 517,
            "memory": 15450112,
            "virtual_memory": 423440154624,
            "command_line": [
              "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension"
            ]
          }
        },
        "system_extension": {
          "Ok": {
            "name": "N7N82884NH.com.refractionpoint.rphcp.extension",
            "pid": 1638,
            "state": "running",
            "service_type": null,
            "launchd_config": "(submitted by smd[323])",
            "launchd_type": "Submitted",
            "program": "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension",
            "restart_count": 1,
            "last_signal": null
          }
        },
        "config": {
          "Ok": {
            "launchd_file_hash": {
              "Ok": "01049276aaa1708885f24788230fe9a4c2316e43aadef42354e4061b0aac906c"
            },
            "launchd_file": "ABC+",
            "mdm_silent_file_hash": {
              "Err": "No such file or directory (os error 2)\n"
            },
            "mdm_silent_file": null,
            "system_extensions": {
              "Ok": [
                {
                  "enabled": true,
                  "active": true,
                  "team_id": "N7N82884NH",
                  "bundle_id": "com.refractionpoint.rphcp.extension",
                  "version": "(1.0.250416/1.0.250416)",
                  "name": "RPHCP",
                  "state": "[activated enabled]"
                }
              ]
            },
            "network_extension": {
              "Ok": {
                "name": "com.refractionpoint.rphcp.client",
                "enabled": true
              }
            },
            "profiles": {
              "Ok": [

              ]
            }
          }
        }
      }
    },
    "hbs_status": {
      "Ok": {
        "version": "4.33.6",
        "sid": "da1020f7-c247-4749-b7d7-d05f282e6ca2",
        "oid": "0bb86406-b1f3-4d3b-af5c-118cc5291972",
        "last_update": 1745890057,
        "uptime": 300
      }
    },
    "logs": {
      "Ok": {
        "file": "/usr/local/hcp.log",
        "oid": null,
        "sid": null,
        "data": "MMGgMTq5NTg4OTczNzogaGNwIGxhdW5amGVkClRTIDE3NDU4ODk3Mzc6IGJvb3RzdHJhcCB1c2VkClRTIDE3NDU4ODk3Mzc6IGNvbm4gl3RhcnRlZApUUyAxNzQ1ODg5NzM3OiBjb25uZWN0bW5nClRTIDE3NMU8ODk3Mzg6IHNzbCBjb25uZWN0ZWQKVFMgMTc0UTg4OTczODogaGVhZGVycyBzZW50ClRTIDM3NDU4ODk3Mzg6IGNoYW5uZWwgdXAKVFMgMTc0NTg4OTczODogY29tbXMgd2l0aCBjbG91ZCBkb3duClRTIDE3NDU4ODk3NDM6IGNvbm5lY3RpbmcKVFMgMTc0NTg4OTc0NDogc3NsIGNvbm5lY3RlZApUUyAxNzQ1ODg5NzQ0OiBoZWFkZXJzIHNlbnQKVFMgMTc0NTg4OTc0NDogY2hhbm5lbCB1cApUUyAxNzQ1ODg5NzYyOiBkaXNjb25uZWN0aW5nIGZyb20gYmFkIHNlbmQKVFMgMTc0NTg4OTc2MzogZZJyb3IgcmVjZWl2aW5nIGZyYW1lOgpUUyAxNzQ1ODg5NzYzOiBTU0wgLSBCYWQgaW5wdXQgcGFyYW1ldGVycyB0byBmdW5jdGlvblRTIDE3NDU4ODk3NjM6IApUUyAxNzQ1ODg5NzYzOiBjb21tcyBqaXRoIGNsb3VkIGRvd24KVFMgMTc0NTg4OTc2ODogY29ubmVjdGluZwpUUyAxNzQ1ODg5NzY4OiBzc2wgY29ubmVjdGVkClRTIDE3NDU4ODk3Njg6IGhlYWRlcnMgc2VudApMUyAbNyQ1OEg4NzY58iBjaGGubmVbIHVwUd=="
      }
    }
  },
  "network": {
    "Ok": {
      "endpoint_server": "0651b4f82df0a29c.edr.limacharlie.io",
      "addresses": [
        "34.160.14.29:443"
      ],
      "tcp_connect": true,
      "proxy": {
        "Ok": {
          "proxy_server": null,
          "tcp_connect": false
        }
      },
      "cert_chain": [
        {
          "common_name": "0651b4f82df0a29c.edr.limacharlie.io",
          "issuer": "C = Google Trust Services, O = US, CN = WR3",
          "serial": "00:b3:f6:29:5a:3e:78:03:10:18:38:fd:4c:df:54:c5",
          "not_before": 1742383890,
          "not_after": 1750163165,
          "is_ca": false
        },
        {
          "common_name": "WR3",
          "issuer": "C = Google Trust Services LLC, O = US, CN = GTS Root R1",
          "serial": "7f:f0:05:a9:15:68:d6:3a:bc:22:86:16:84:aa:4b:5a",
          "not_before": 1702458000,
          "not_after": 1866290400,
          "is_ca": true
        },
        {
          "common_name": "GTS Root R1",
          "issuer": "C = GlobalSign nv-sa, O = BE, CN = GlobalSign Root CA",
          "serial": "77:bd:0d:6c:db:36:f9:1a:ea:21:0f:c4:f0:58:d3:0d",
          "not_before": 1592524842,
          "not_after": 1832630442,
          "is_ca": true
        }
      ]
    }
  },
  "verifier": {
    "Ok": {
      "pid": 2024,
      "ppid": 2023,
      "cpu_usage": 0.0,
      "cwd": "/Users/username/Downloads",
      "exe": "/usr/local/bin/rphcp",
      "start_time": 1745890277,
      "run_time": 1,
      "memory": 10125312,
      "virtual_memory": 420875878400,
      "command_line": [
        "/usr/local/bin/rphcp",
        "-H"
      ]
    }
  }
}
```

## Additional Help

If these steps do not help, get in touch with us, and we will help you figure out the issue. The best way of contacting us is via our [Community Site](https://community.limacharlie.com/), followed by `support@limacharlie.io`.

---

# FAQ - Sensor Troubleshooting

## Why is there no output in the console?

When running Sensor [console commands](/v2/docs/endpoint-agent-commands), you may encounter a "spinning wheel" or no output back from the Sensor. Oftentimes, this is due to the *response* event not enabled in [Event Collection](/v2/docs/ext-exfil). You will need to configure the response event in order to receive feedback in the console.

For example, the `os_users` Sensor command has two components:

* `OS_USERS_REQ` is the *request* event sent to the Sensor to collect OS user information.
* `OS_USERS_REP` is the *response* event sent back by the Sensor containing the information of interest.

Please ensure that you are collecting the `*_REP` events in order to display output in the console.

## Sensor Not Showing as Online

### Determining Online Status

It is important to note that the online marker in the Web UI does not display real-time information. Instead it refreshes its status between every 30 seconds to every few minutes, depending on the page in question.

This means that an icon showing a sensor as not online may be lagging behind the actual status. If you need to get a positive feedback on whether the sensor is online or not, go to the "Sensors" page which refreshes status more often. Moving to the "Sensors" page also triggers a refresh of the status right away.

### Reasons for Temporary Disconnect

Sensors connect to the cloud via a semi-persistent SSL connection. In general, if a host has connectivity to the internet, the sensor should be online. There are, however, a few situations that result in the sensor temporarily disconnecting from the cloud for a few seconds. This means that if you notice a sensor is offline when you expect it to be online, give it 30 seconds, and in most situations it will come back online within 5 seconds.

## Sensor Not Connecting

Sensors connect to the LimaCharlie.io cloud via an SSL connection on port 443. Make sure your network allows such a connection. It is a very common port typically used for HTTPS so an issue is highly unlikely.

The sensor uses a pinned SSL certificate to talk to the cloud. This means that if you are in a network that enforces SSL inspection (a man-in-the-middle of the SSL connections sometimes used in large corporate environments), this may prevent the sensor from connecting. LimaCharlie uses a pinned certificate to ensure the highest level of security possible, as usage of off-the-shelf certificates can be leveraged by state-sponsored (or advanced) attackers.

If your network uses SSL inspection, we recommend you setup an exception for the LimaCharlie cloud domain relevant to you. Get in touch with us and we can provide you with the necessary information.

Sensors since version 4.21.2 also generate a local log file able to be used to help pinpoint the level at which the connectivity fails. This log file is located:

* Windows: `c:\windows\system32\hcp.log`
* MacOS: `/usr/local/hcp.log`
* Linux: `./hcp.log`

This log provides a simple line for each basic step of connectivity to the cloud. It only logs the first connection attempted to the cloud and rolls over every time the sensor starts. A successful connection should look like:

```
hcp launched
configs applied
conn started
connecting
ssl connected
headers sent
channel up
```

If you are having trouble getting your sensor connected to the cloud, we recommend that you attempt the following on the host:

1. Restart the LimaCharlie service.
2. Check that the service is running.

   * The service process should be called `rphcp`.
3. If the sensor still shows as not online, check the `hcp.log` file mentioned above:

   * Check that the "configs applied" step is reached. If not, it may indicate the Installation Key provided is wrong or has a typo.
   * Check that the proxy is mentioned in the log if you are using a proxy configuration.
   * Check that the "ssl connected" step is reached. If not, this indicates a network configuration issue connecting to the cloud.
   * Check that the "channel up" step is reached. If not, this could indicate one of a few things:

     + Your sensor was deleted (through API or Web interface) from the org. If so, reinstall to get a new identity.
     + Your Organization may be out-of-quota if more sensors than the maximum number you've set in the Billing section are trying to connect at once. Increase your quota and wait a few minutes to fix it.
     + If this is a brand new sensor install, make sure the Installation Key you're using still exists in your Org. Once deleted, an Installation Key cannot be used for NEW sensors, but old sensors that were installed using it will still work fine.

## Sensor Not Responding

Your sensor shows up as "online", but does not respond to interactive tasking.

The most common cause of this problem is a partial uninstall and reinstall of the sensor on the host. The sensor, when installed, creates local files that record the identity the sensor has with the cloud.

When uninstalling, the `-r` mode leaves these identification files behind, so that if you reinstall a new version of the sensor which talks to the same Org in LimaCharlie, the Sensor ID will be the same. On the other hand, the `-c` mode will remove all the identity files as well.

If you uninstall with `-r` and re-enroll the sensor to a different Org, as can often happen during testing, the files on disk that include some cryptographic material will not match with what the cloud expects. This may result in taskings being refused by the sensor.

To make sure this is not what's happening, uninstall the sensor with `-c`. Double-check that the local files `hcp`, `hcp_hbs` and `hcp_conf` are deleted before reinstalling. On Windows these should be in `c:\windows\system32` while on macOS they should be in `/usr/local`.

## Sensor Duplication

Sensor duplication can occur during certain types of installation or deployments, e.g. creation of virtual systems via a "gold image" that has LimaCharlie pre-installed.

However, in niche cases we have seen examples of:

1. LimaCharlie unable to write it's own identity files to disk, causing a constant "new" sensor connection.
2. Third-party security software on the system incorrectly categorizing LimaCharlie as malware, and killing the process before it can start.

One method to troubleshoot and determine root cause is to utilize Sysinternals' [DebugView](https://learn.microsoft.com/en-us/sysinternals/downloads/debugview) to investigate the error caused during Sensor installaton/start-up.

Another quick troubleshooting technique may be to determine whether the Sensor process `rphcp.exe`

## Upgrading Sensors

To ensure the sensor version is up-to-date, open the "Install Sensors" page in the web app (under "Setup") and navigate to the "Upgrading Sensors" section.

Upgrading sensors is done transparently for you once you click the button in the web app interface. You do not need to re download installers (in fact the installer stays the same). The new version should be in effect across the organization within about 20 minutes.

## How can I tell which version of the sensor is running locally?

The LimaCharlie sensor outputs a status file on the endpoint which allows you to see the:

* Sensor ID,
* Organization ID,
* Sensor version, and
* the agent's service uptime.

You can find this log data at the following location, based on your platform:

| Platform | File Path |
| --- | --- |
| Linux | `/opt/limacharlie/hcp_hbs_status.json` |
| macOS | `/Library/Application Support/limacharlie/hcp_hbs_status.json` |
| Windows | `c:\programdata\limacharlie\hcp_hbs_status.json` |

The log data is formatted similarly to the example below:

```
{
      "version": "4.33.0",
      "sid": "be8bc53b-36b2-469d-a914-716d629cb2d8",
      "oid": "d02c08e4-aedc-45eb-88aa-98b09b7d92df",
      "last_update": 1738872790,
      "uptime": 127
}
```

## Sensor Troubleshooting Utility

In some cases we may ask you for sensor health information from an endpoint that is having issues. To get this information, run the LC sensor interactively in the terminal with the -H flag.

On macOS run the command: `sudo /usr/local/bin/rphcp -H`

The diagnostic information will be displayed on screen, and saved to a file. The location of the output file will be shown at the bottom of the message shown on screen (on macOS, typically at `/Library/Application Support/limacharlie/`).

Note that the Sensor Troubleshooting Utility requires sensor [version 4.33.6](https://community.limacharlie.com/t/release-agent-with-sensor-troubleshooting-tool-webapp-4-2-3/276) or newer to be installed on disk on the impacted endpoint.

You can find the output file at the following location, based on your platform:

| Platform | File Path |
| --- | --- |
| Linux | `/opt/limacharlie/sensor_health_YYYY_MM_DD_HH_MM.json` |
| macOS | `/Library/Application Support/limacharlie/sensor_health_YYYY_MM_DD_HH_MM.json` |
| Windows | `c:\programdata\limacharlie\sensor_health_YYYY_MM_DD_HH_MM.json` |

The log data is formatted similarly to the example below:

```
{
  "system": {
    "memory_total": 25769803776,
    "memory_used": 13423722496,
    "name": "Darwin",
    "kernel": "24.4.0",
    "version": "15.4.1",
    "hostname": "Mac",
    "cpu_count": 8,
    "process_list": [

    ]
  },
  "agent": {
    "agent_info": {
      "MacOS": {
        "process": {
          "Ok": {
            "pid": 2024,
            "ppid": 2023,
            "cpu_usage": 0.0,
            "cwd": "/Users/username/Downloads",
            "exe": "/usr/local/bin/rphcp",
            "start_time": 1745890277,
            "run_time": 1,
            "memory": 10125312,
            "virtual_memory": 420875878400,
            "command_line": [
              "/usr/local/bin/rphcp",
              "-H"
            ]
          }
        },
        "agent_service": {
          "Ok": {
            "name": "com.refractionpoint.rphcp",
            "pid": 1521,
            "state": "running",
            "service_type": null,
            "launchd_config": "/Library/LaunchDaemons/com.refractionpoint.rphcp.plist",
            "launchd_type": "LaunchDaemon",
            "program": "/usr/local/bin/rphcp",
            "restart_count": 1,
            "last_signal": null
          }
        },
        "system_extension_process": {
          "Ok": {
            "pid": 1638,
            "ppid": 1,
            "cpu_usage": 0.0,
            "cwd": "/",
            "exe": "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension",
            "start_time": 1745889761,
            "run_time": 517,
            "memory": 15450112,
            "virtual_memory": 423440154624,
            "command_line": [
              "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension"
            ]
          }
        },
        "system_extension": {
          "Ok": {
            "name": "N7N82884NH.com.refractionpoint.rphcp.extension",
            "pid": 1638,
            "state": "running",
            "service_type": null,
            "launchd_config": "(submitted by smd[323])",
            "launchd_type": "Submitted",
            "program": "/Library/SystemExtensions/3C420533-7D6B-409C-A2B4-BB9D526AB7E2/com.refractionpoint.rphcp.extension.systemextension/Contents/MacOS/com.refractionpoint.rphcp.extension",
            "restart_count": 1,
            "last_signal": null
          }
        },
        "config": {
          "Ok": {
            "launchd_file_hash": {
              "Ok": "01049276aaa1708885f24788230fe9a4c2316e43aadef42354e4061b0aac906c"
            },
            "launchd_file": "ABC+",
            "mdm_silent_file_hash": {
              "Err": "No such file or directory (os error 2)\n"
            },
            "mdm_silent_file": null,
            "system_extensions": {
              "Ok": [
                {
                  "enabled": true,
                  "active": true,
                  "team_id": "N7N82884NH",
                  "bundle_id": "com.refractionpoint.rphcp.extension",
                  "version": "(1.0.250416/1.0.250416)",
                  "name": "RPHCP",
                  "state": "[activated enabled]"
                }
              ]
            },
            "network_extension": {
              "Ok": {
                "name": "com.refractionpoint.rphcp.client",
                "enabled": true
              }
            },
            "profiles": {
              "Ok": [

              ]
            }
          }
        }
      }
    },
    "hbs_status": {
      "Ok": {
        "version": "4.33.6",
        "sid": "da1020f7-c247-4749-b7d7-d05f282e6ca2",
        "oid": "0bb86406-b1f3-4d3b-af5c-118cc5291972",
        "last_update": 1745890057,
        "uptime": 300
      }
    },
    "logs": {
      "Ok": {
        "file": "/usr/local/hcp.log",
        "oid": null,
        "sid": null,
        "data": "MMGgMTq5NTg4OTczNzogaGNwIGxhdW5amGVkClRTIDE3NDU4ODk3Mzc6IGJvb3RzdHJhcCB1c2VkClRTIDE3NDU4ODk3Mzc6IGNvbm4gl3RhcnRlZApUUyAxNzQ1ODg5NzM3OiBjb25uZWN0bW5nClRTIDE3NMU8ODk3Mzg6IHNzbCBjb25uZWN0ZWQKVFMgMTc0UTg4OTczODogaGVhZGVycyBzZW50ClRTIDM3NDU4ODk3Mzg6IGNoYW5uZWwgdXAKVFMgMTc0NTg4OTczODogY29tbXMgd2l0aCBjbG91ZCBkb3duClRTIDE3NDU4ODk3NDM6IGNvbm5lY3RpbmcKVFMgMTc0NTg4OTc0NDogc3NsIGNvbm5lY3RlZApUUyAxNzQ1ODg5NzQ0OiBoZWFkZXJzIHNlbnQKVFMgMTc0NTg4OTc0NDogY2hhbm5lbCB1cApUUyAxNzQ1ODg5NzYyOiBkaXNjb25uZWN0aW5nIGZyb20gYmFkIHNlbmQKVFMgMTc0NTg4OTc2MzogZZJyb3IgcmVjZWl2aW5nIGZyYW1lOgpUUyAxNzQ1ODg5NzYzOiBTU0wgLSBCYWQgaW5wdXQgcGFyYW1ldGVycyB0byBmdW5jdGlvblRTIDE3NDU4ODk3NjM6IApUUyAxNzQ1ODg5NzYzOiBjb21tcyBqaXRoIGNsb3VkIGRvd24KVFMgMTc0NTg4OTc2ODogY29ubmVjdGluZwpUUyAxNzQ1ODg5NzY4OiBzc2wgY29ubmVjdGVkClRTIDE3NDU4ODk3Njg6IGhlYWRlcnMgc2VudApMUyAbNyQ1OEg4NzY58iBjaGGubmVbIHVwUd=="
      }
    }
  },
  "network": {
    "Ok": {
      "endpoint_server": "0651b4f82df0a29c.edr.limacharlie.io",
      "addresses": [
        "34.160.14.29:443"
      ],
      "tcp_connect": true,
      "proxy": {
        "Ok": {
          "proxy_server": null,
          "tcp_connect": false
        }
      },
      "cert_chain": [
        {
          "common_name": "0651b4f82df0a29c.edr.limacharlie.io",
          "issuer": "C = Google Trust Services, O = US, CN = WR3",
          "serial": "00:b3:f6:29:5a:3e:78:03:10:18:38:fd:4c:df:54:c5",
          "not_before": 1742383890,
          "not_after": 1750163165,
          "is_ca": false
        },
        {
          "common_name": "WR3",
          "issuer": "C = Google Trust Services LLC, O = US, CN = GTS Root R1",
          "serial": "7f:f0:05:a9:15:68:d6:3a:bc:22:86:16:84:aa:4b:5a",
          "not_before": 1702458000,
          "not_after": 1866290400,
          "is_ca": true
        },
        {
          "common_name": "GTS Root R1",
          "issuer": "C = GlobalSign nv-sa, O = BE, CN = GlobalSign Root CA",
          "serial": "77:bd:0d:6c:db:36:f9:1a:ea:21:0f:c4:f0:58:d3:0d",
          "not_before": 1592524842,
          "not_after": 1832630442,
          "is_ca": true
        }
      ]
    }
  },
  "verifier": {
    "Ok": {
      "pid": 2024,
      "ppid": 2023,
      "cpu_usage": 0.0,
      "cwd": "/Users/username/Downloads",
      "exe": "/usr/local/bin/rphcp",
      "start_time": 1745890277,
      "run_time": 1,
      "memory": 10125312,
      "virtual_memory": 420875878400,
      "command_line": [
        "/usr/local/bin/rphcp",
        "-H"
      ]
    }
  }
}
```

## Additional Help

If these steps do not help, get in touch with us, and we will help you figure out the issue. The best way of contacting us is via our [Community Site](https://community.limacharlie.com/), followed by `support@limacharlie.io`.

---

# FAQ

## Account Management

[View FAQ - Account Management](/docs/en/faq-account-management)

## Billing

[View FAQ - Billing](/docs/en/faq-billing)

## General

[View FAQ - General](/docs/en/faq-general)

## Sensor Installation

[View FAQ - Sensor Installation](/docs/en/faq-sensor-installation)

## Privacy

[View FAQ - Privacy](/docs/en/faq-privacy)

## Sensor Troubleshooting

[View FAQ - Sensor Troubleshooting](/docs/en/faq-troubleshooting)

## Invoices

[View FAQ - Invoices](/docs/en/faq-invoices)

---

# FAQ

*This documentation section appears to be empty or in transition between versions. The FAQ tag currently has no associated articles.*

**Note**: This appears to be a placeholder or navigation page rather than actual documentation content. If you're looking for FAQ content, you may want to check:
- The main documentation index
- Specific product/feature sections
- The v2 documentation area (as v1 is deprecated)

---

# Getting Started

# Quickstart

LimaCharlie is infrastructure to connect sources of security data, automate activity based on what's being observed, and forward data to where you need it. There's no *correct* way to use it - every environment is different.

That said, the majority of LimaCharlie users require basic endpoint detection and response (EDR) capabilities. This guide will cover:

1. Creating a new **Organization**
2. Deploying a **Sensor** to the Organization
3. Adding **Sigma rules** to detect suspicious activity
4. Forwarding detections to an external destination as an **Output**

All of this can be done within our free tier, which offers full platform functionality for up to two (2) sensors. If you haven't already signed up for a free account, please do so at [app.limacharlie.io](https://app.limacharlie.io).

Let's get started!

## Creating an Organization

LimaCharlie organizations are isolated tenants in the cloud, conceptually equivalent to "projects". They can be configured to suit the needs of each deployment.

After accepting the initial Terms of Service, you'll be offered a prompt to create an organization in a selected `Region` with a globally unique `Name`.

> **Region Selection**: The region that you select for an organization is permanent. Please also consider regulatory requirements for you and/or your customers' data.

Once the organization is created, you'll be forwarded to our initial dashboard and Sensor list, which will be empty and ready for the next step.

## Deploying a Sensor

From the Sensors page in your new organization, click `Add Sensor` to open the setup flow for new sensors. Generally speaking, Sensors are executables that install on hosts and connect them to the LimaCharlie cloud to send telemetry, receive commands, and other capabilities.

> **Sensors Overview**: For a full overview of types of sensors and their capabilities, check out Sensors.

The setup flow should make this process straightforward. For example's sake, let's say we're installing a sensor on a Windows 10 (64 bit) machine we have in front of us.

* Choose the Windows sensor type
* Create an Installation Key - this registers the executable to communicate securely with your organization
* Choose the `64 bit (.exe)` installer
* Follow the on-screen instructions to execute the installer properly
* See immediate feedback when the sensor registers successfully with the cloud

> **Potential Issues**: Since sensors are executables that talk to the cloud, antivirus software and networking layers may interfere with installation. If you run into an issue, take a look at troubleshooting.

With a Windows sensor connected to the cloud, you should gain a lot of visibility into the endpoint. If we view the new sensor inside the web application, we'll have access to views such as:

* `Timeline`: the viewer for telemetry events being collected from the endpoint
* `Processes`: the list of processes running on the endpoint, their level of network activity, and commands to manipulate processes (i.e. kill / pause / resume process, or view modules)
* `File System`: an explorer for the endpoint's file system, right in the browser
* `Console`: a safe shell-like environment for issuing commands
* `Live Feed`: a running view of the live output of all the sensor's events

With telemetry coming in from the cloud, let's add rules to detect potentially malicious activity.

## Adding Sigma Rules

Writing security rules and automations from scratch is a huge effort. To set an open, baseline standard of coverage, LimaCharlie maintains a `sigma` add-on which can be enabled for free, and is kept up to date with the [openly maintained threat signatures](https://github.com/SigmaHQ/sigma).

Enabling the Sigma add-on will automatically apply rules to your organization to match these threat signatures so we can begin to see Detections on incoming endpoint telemetry.

> **Writing Detection and Response rules**: Writing your own rules is outside the scope of this guide, but we do encourage checking out Detection & Response when you're finished.

## Output

Security data generated from sensors is yours to do with as you wish. For example's sake, let's say we want to forward detections to an [Amazon S3 bucket](https://aws.amazon.com/s3/) for longer-lived storage of detections.

From the Outputs page in your organization, click `Add Output` to open the setup flow for new outputs. Again, the setup flow should make this process straightforward.

* Choose the Detections stream
* Choose the Amazon S3 destination
* Configure the Output and ensure it connects securely to the correct bucket:
  + Output Name
  + Bucket Name
  + Key ID
  + Secret Key
  + Region
* Optionally, you can view samples of the detection stream's data (assuming recent detections have occurred)

With this output in place you can extend the life of your detections beyond the 1 year LimaCharlie retains them, and stage them for any tool that can pull from S3.

## Endpoint Detection & Response

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Use Cases

LimaCharlie's vast array of capabilities can be applied to countless scenarios.

The following table contains some common use cases driving the adoption of the SecOps Cloud Platform among our customer base.

## Common LimaCharlie Use Cases

| Common LimaCharlie Use Cases | | |
| --- | --- | --- |
| [Security Service Providers (MSSP, MSP, MDR)](/v2/docs/service-providers-mssp-msp-mdr) | [Enterprise SOC](/v2/docs/enerprises) | [Building Products](/v2/docs/building-products) |
| [Threat Hunting](/v2/docs/threat-hunting) | [Incident Response](/v2/docs/incident-response) | [SOAR / Automation](/v2/docs/soar-automation) |
| [Cost Effective SIEM Alternative](/v2/docs/cost-effective-siem) | [Network Monitoring](/v2/docs/network-monitoring) | [Cloud Security](/v2/docs/cloud-security) |
| [Observability Pipeline](/v2/docs/observability-pipeline) | [Endpoint Detection and Response (EDR)](/v2/docs/endpoint-detection-and-response-edr) | [Build CTI Capabilties](/v2/docs/build-cti-capabilities) |
| [File and Registry Integrity Monitoring (FIM)](/v2/docs/file-and-registry-integrity-monitoring-fim-deployments) | [Uncovering Adversary Techniques](/v2/docs/uncovering-adversary-techniques) | [WEL Monitoring](/v2/docs/wel-monitoring) |
| [SecOps Development](/v2/docs/secops-development) | [Purple Teaming](/v2/docs/purple-teaming) | [Security Monitoring for DevOps](/v2/docs/security-monitoring-for-devops) |
| [M&A Cyber Due Dilligence](/v2/docs/ma-cyber-due-diligence) | [Table Top Exercises](/v2/docs/table-top-exercises) | [ChromeOS Support](/v2/docs/chromeos-support) |
|  | [Sleeper Mode](/v2/docs/sleeper-mode) |  |

The SCP is under continuous development which includes the regular addition of expanded features and capabilities. Newer features such as our Endpoint protection ([EPP](/v2/docs/ext-epp)) and our [MCP server](/v2/docs/mcp-server) are not yet included in the use case table, but make strong cases for adopting LimaCharlie in their own right.

---

# Use Cases

LimaCharlie's vast array of capabilities can be applied to countless scenarios.

The following table contains some common use cases driving the adoption of the SecOps Cloud Platform among our customer base.

## Common LimaCharlie Use Cases

| Common LimaCharlie Use Cases | | |
| --- | --- | --- |
| [Security Service Providers (MSSP, MSP, MDR)](/v2/docs/service-providers-mssp-msp-mdr) | [Enterprise SOC](/v2/docs/enerprises) | [Building Products](/v2/docs/building-products) |
| [Threat Hunting](/v2/docs/threat-hunting) | [Incident Response](/v2/docs/incident-response) | [SOAR / Automation](/v2/docs/soar-automation) |
| [Cost Effective SIEM Alternative](/v2/docs/cost-effective-siem) | [Network Monitoring](/v2/docs/network-monitoring) | [Cloud Security](/v2/docs/cloud-security) |
| [Observability Pipeline](/v2/docs/observability-pipeline) | [Endpoint Detection and Response (EDR)](/v2/docs/endpoint-detection-and-response-edr) | [Build CTI Capabilties](/v2/docs/build-cti-capabilities) |
| [File and Registry Integrity Monitoring (FIM)](/v2/docs/file-and-registry-integrity-monitoring-fim-deployments) | [Uncovering Adversary Techniques](/v2/docs/uncovering-adversary-techniques) | [WEL Monitoring](/v2/docs/wel-monitoring) |
| [SecOps Development](/v2/docs/secops-development) | [Purple Teaming](/v2/docs/purple-teaming) | [Security Monitoring for DevOps](/v2/docs/security-monitoring-for-devops) |
| [M&A Cyber Due Dilligence](/v2/docs/ma-cyber-due-diligence) | [Table Top Exercises](/v2/docs/table-top-exercises) | [ChromeOS Support](/v2/docs/chromeos-support) |
|  | [Sleeper Mode](/v2/docs/sleeper-mode) |  |

The SCP is under continuous development which includes the regular addition of expanded features and capabilities. Newer features such as our Endpoint protection ([EPP](/v2/docs/ext-epp)) and our [MCP server](/v2/docs/mcp-server) are not yet included in the use case table, but make strong cases for adopting LimaCharlie in their own right.

---

# What is LimaCharlie?

LimaCharlie is the **SecOps Cloud Platform** - delivering security operations for the modern era.

LimaCharlie's SecOps Cloud Platform provides you with comprehensive enterprise protection that brings together critical cybersecurity capabilities and eliminates integration challenges and security gaps for more effective protection against today's threats.

The SecOps Cloud Platform offers a unified platform where you can build customized solutions effortlessly. With open APIs, centralized telemetry, and automated detection and response mechanisms, it's time cybersecurity moves into the modern era.

Simplifying procurement, deployment and integration of best-of-breed cybersecurity solutions, the SecOps Cloud Platform delivers complete protection tailored to each organization's specific needs, much in the same way IT Clouds have supported enterprises for years.

Our documentation can walk you through setting up your own Organization, deploying Sensors, writing detection and response rules, or outputting your data to any destination of your choosing. To dive in immediately, see our [Quickstart](/v2/docs/quickstart) guide.

Dig in, and build the security program you need and have always wanted.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Other

# 1Password

[1Password](https://1password.com/) provides an events API to fetch audit logs. Events can be ingested directly via a cloud-to-cloud or CLI Adapter.

See 1Password's official API documentation [here](https://developer.1password.com/docs/events-api/reference/).

1Password telemetry can be addressed via the `1password` platform.

## Adapter Deployment

1Password events can be collected directly from the 1Password API, via a cloud-to-cloud Adapter, or via the CLI Adapter. 1Password adapters require the following options:

* `token`: the API token provisioned through 1password.
* `endpoint`: the API endpoint to use, depending on your 1password plan, see their documentation below.

You can generate an access token from 1Password at [this link](https://support.1password.com/events-reporting/).

## Cloud-to-Cloud Adapter

LimaCharlie offers a 1Password guided configuration in the web UI. From your 1Password instance, you will need:

* 1Password API Access Token
* Endpoint; one of the following:

  + 1Password.com (Business)
  + 1Password.com (Enterprise)
  + 1Password.ca
  + 1Password.eu

After providing an [Installation Key](/v2/docs/installation-keys), provide the required values and LimaCharlie will establish a Cloud Adapter for 1Password events

### Infrastructure as Code Deployment

LimaCharlie IaC Adapter can also be used to ingest Slack events.

```
# 1Password Specific Docs: https://docs.limacharlie.io/docs/adapter-types-1password

sensor_type: "1password"
  1password:
    token: "hive://secret/your-1password-api-token-secret"
    endpoint: "business"  # or "enterprise", "ca", "eu"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_1PASSWORD"
      hostname: "1password-audit-adapter"
      platform: "json"
      sensor_seed_key: "1password-sensor-unique-name"
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# 1Password

[1Password](https://1password.com/) provides an events API to fetch audit logs. Events can be ingested directly via a cloud-to-cloud or CLI Adapter.

See 1Password's official API documentation [here](https://developer.1password.com/docs/events-api/reference/).

1Password telemetry can be addressed via the `1password` platform.

## Adapter Deployment

1Password events can be collected directly from the 1Password API, via a cloud-to-cloud Adapter, or via the CLI Adapter. 1Password adapters require the following options:

* `token`: the API token provisioned through 1password.
* `endpoint`: the API endpoint to use, depending on your 1password plan, see their documentation below.

You can generate an access token from 1Password at [this link](https://support.1password.com/events-reporting/).

## Cloud-to-Cloud Adapter

LimaCharlie offers a 1Password guided configuration in the web UI. From your 1Password instance, you will need:

* 1Password API Access Token
* Endpoint; one of the following:
  + 1Password.com (Business)
  + 1Password.com (Enterprise)
  + 1Password.ca
  + 1Password.eu

After providing an [Installation Key](/v2/docs/installation-keys), provide the required values and LimaCharlie will establish a Cloud Adapter for 1Password events

### Infrastructure as Code Deployment

LimaCharlie IaC Adapter can also be used to ingest Slack events.

```
# 1Password Specific Docs: https://docs.limacharlie.io/docs/adapter-types-1password

sensor_type: "1password"
  1password:
    token: "hive://secret/your-1password-api-token-secret"
    endpoint: "business"  # or "enterprise", "ca", "eu"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_1PASSWORD"
      hostname: "1password-audit-adapter"
      platform: "json"
      sensor_seed_key: "1password-sensor-unique-name"
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# 1Password

The 1Password CLI brings 1Password to the terminal, allowing you to interact with a 1Password instance from LimaCharlie.

This Extension makes use of 1Password's native CLI, which can be found [here](https://developer.1password.com/docs/cli).

> **1Password Account Types**
>
> Please note that some 1Password functionality is limited to 1Password Business. Please validate you have the correct type of account(s) to ensure that commands run.

## Example

Returns a list of all items the account has read access to by default.

```
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    cloud: '{{ "op" }}'
    command_line: '{{ "item list" }}'
    credentials: '{{ "hive://secret/secret-name" }}'
```

## Credentials

To utilize 1Password's automated CLI capabilities, you will need to create and utilize a Service Account. More information can be found [here](https://developer.1password.com/docs/service-accounts/get-started/).

* Create a secret in the secrets manager in the following format:

```
serviceAccountToken
```

---

# AWS

AWS CLI is a unified tool that provides a consistent interface for interacting with AWS from the command line. With this component of the Cloud CLI Extension, you can interact with AWS directly from LimaCharlie.

This extension makes use of AWS's native CLI tool, which can be found [here](https://awscli.amazonaws.com/v2/documentation/api/latest/index.html).

## Example

The following example would execute in response to AWS telemetry that 1) met certain criteria and 2) had an `instance_id` for an EC2 instance(s). The following response action would utilize the `.event.instance_id` to stop the corresponding EC2 instances.

```
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    cloud: '{{ "aws" }}'
    command_tokens:
      - ec2
      - stop-instances
      - '--instance-ids'
      - '{{ .event.instance_id  }}'
      - '--region'
      - us-east-1
    credentials: '{{ "hive://secret/secret-name" }}'
```

## Credentials

To utilize AWS CLI capabilities, you will need:

* You will need an AWS access key ID and AWS secret access key
* Create a secret in the secrets manager in the following format:

  ```
  accessKeyID/secretAccessKey
  ```

Documentation on creating and managing AWS access keys and other IAM components can be found [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

---

# Access and Permissions

LimaCharlie is [multi-tenant](https://en.wikipedia.org/wiki/Multitenancy); tenants are called Organizations and both data and billing are tied to the Organization.

Users, API Keys and Groups exist as ways of managing access and permissions to Organizations.

## Users

Users are operators or administrators. Permissions are applied directly to the User account and allow for fine-grained access control.

One user can be a member of multiple organizations.

## API Keys

An API Key represents a set of permissions and are used to interact with LimaCharlie.

Full documentation on API Keys can be [here](/v2/docs/api-keys).

## Groups

Groups provides a way for managing permissions for multiple Users across multiple Organizations.

Groups each have a set of permissions associated with them that are applied (additively) to all Users in the group, for all Organizations in the group. Groups drastically reduce the admin overhead in managing fine-grained access control.

More information [here](/v2/docs/user-access).

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Artifact

The Artifact Extension provides low-level collection capabilities which can be configured to run automatically via Detection & Response rules, Sensor collections, or pushed via REST API. When enabled, an Artifact Collection menu will be available within the LimaCharlie web UI.

> **Billing for Artifacts**
>
> Note that while the Artifact extension is free to enable, ingested artifacts do incur a charge. Please refer to pricing details to confirm Artifact ingestion and retention costs.

## Enabling the Artifact Extension

To enable the Artifact extension, navigate to the [Artifact extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-artifact) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe.**

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/artifact-1.png)

After clicking **Subscribe**, the Artifact extension should be available almost immediately.

> Note that the Artifact extension first requires enabling of the Reliable Tasking extension. You can find more on that extension [here](/v2/docs/ext-reliable-tasking).

## Using the Artifact Extension

When enabled, you will see an **Artifact Collection** option under **Sensors** menu for the respective organization.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/artifact-2.png)

Within the Artifact Collection page, you can configure:

* Artifact collection rules for files.
* Artifact collection rules to stream Windows Event Log (WEL) events.
* Artifact collection rules to stream Mac Unified Log (MUL) events.
* PCAP capture rules to capture network traffic (Only available on Linux)

The following screenshot provides examples of capturing Windows Security and Sysmon Windows Event Logs via Artifact Collection. Rather than using an Adapter, capturing WEL events via the `wel://` pattern adds the corresponding events to the sensor telemetry, creating a real-time stream of Windows Event Log data. However, you can also specify the pattern to collect the specific `.evtx` files.

More information on Artifact collections can be found [here](/v2/docs/artifacts).

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/artifact-3.png)

---

# Artifacts

## Overview

The Artifact Collection system allows you to ingest artifact types like:

* Plain text logs (syslog for example)
* Windows Event Logs
* PCAPs
* Windows Prefetch files
* Windows PE (executables) files
* [Zeek](https://zeek.org) (previously Bro)
* Full memory dumps
* Generic JSON
* OLE (MS Word, Excel etc)
* Windows MFT CSV Listing
* Apple Binary/XML plists

Those artifacts can be ingested from hosts running a LimaCharlie Sensor, or they can be pushed to the LimaCharlie platform via a REST interface.

Once ingested, the artifacts are retained and made available to you with a custom retention period. Ingested artifacts are also indexed similarly to LimaCharlie events. This means that you can search all of your artifact for the last year for Indicators like IP Addresses, Domain Names, User Names, Hashes etc.

This in turn makes it possible for you to be looking at sensor data, identify an IP of interest, and launch a quick search to see if this IP has been observed in any artifacts over the past year. If it has, with one click you can visualize the relevant artifact entries to assist you in your investigation.

We call this "artifact operationalization". It is not meant to be a general viewing and querying tool like Splunk, but as a tactical tool providing you with critical answers as you need them during security operations.

Note that Artifact Collection configurations are synchronized with sensors every few minutes.

## Ingestion

There are multiple ways to ingest artifacts within LimaCharlie, depending on the need of the user.

**Please note, if you are looking for real-time streaming of a file (such as a system log), we'd recommend using the** `file`[**adapter**](/v2/docs/adapters) **instead of Artifact Collection.**

### Using LC Sensors

The LimaCharlie sensor can be used to retrieve artifact files directly from hosts.

#### Manually

To instruct the ingestion of an artifact file located on a host where LC is installed, simply issue the `artifact_get` command. You should receive two events in response to this command: a general receipt indicating the sensor received the command, and a response with a status code indicating whether the ingestion was successful (an error code of `200` (as in HTTP) indicates success).

```
artifact_get --type pcap --file /path/to/file.pcap --days-retention $days
```

#### Using the Extension

**File Collection - Artifact Collection Rules**

With the [Artifact Collection Extension](https://app.limacharlie.io/add-ons/extension-detail/ext-artifact) enabled, a new section should be open in the web interface. It will allow you to manage the automatic collection of files from your fleet without manual input or configuration.

The extension manages this through the use of Rules that specify a set of Platforms (like Windows), Tags (sensor tags), a retention time and file patterns.

Rules define which file path patterns should be monitored for changes and ingested for specific sets of hosts.

Filter tags are tags that must ALL be present on a sensor for it to match (ANDed), while the platform of the sensor much match one of the platforms in the filter (ORed).

Patterns are file path where the file expression at the end of the path can contain patterns line (`*`, `?`, `+`).

These wildcards are NOT supported in the path portion of the pattern. Windows directory separators (backslash, `\`) must be escaped like `\\`.

* Good example: `/var/log/*.1`
* Bad example: `/var/*/syslog`

Note that matching files are watched for changes. When a change is detected, the entire file is ingested. *This means you usually want to target logs that get rolled over after a certain time.*

For example syslog is rolled from `syslog` to `syslog.1` after a day, you want to target `syslog.1` to avoid duplicating records from a file being appended to.

Rules may also specify special accesses to log. For example, specifying a rule with a file path of `wel://Security:*` will begin collection of the Windows Event Logs (`wel`) in real-time directly from the sensor. See the Windows Event Logs section below.

**Network Capture - PCAP Capture Rules**

The extension also offers a rule system to do network capture from the host. **This feature is currently only available on Linux.**

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(311).png)

To see the network interfaces available for capture, issue the `pcap_ifaces` command to the sensor.

Each capture rule filters a set of sensors per Tag. The second part of the rule is the list of patterns to capture from. Each pattern defines a network interface to use and a [tcpdump-like](https://www.tcpdump.org/manpages/pcap-filter.7.html) filter expression to select traffic from that interface.

The filter part of the capture pattern will automatically receive an additional "filter out" expression that removes traffic related to LimaCharlie itself (to avoid a feedback loop of traffic).

For example, you could specify the filter:

```
tcp port 80
```

which would automatically be expanded for you as

```
tcp port 80 and not lc.aaa.limacharlie.io and not ...
```

These rules get synced with agents every 10 minutes. Once a capture on the agent reaches a certain threshold (about 30MB), the capture will get automatically sent to the LimaCharlie cloud as an artifact with the retention specified in the rule. From there you can specify rules to process further the pcap data automatically, like using the [Zeek extension](/v2/docs/ext-zeek).

### Using the CLI

To simplify the task on ingesting via the REST API, you can use the LC CLI tool (`pip install limacharlie`). Using this tool, you can use the `limacharlie artifacts --help` command it installs. This is the recommended way of ingesting logs from external systems where LC is not installed.

### Using the REST API

When the sensor is tasked to ingest an artifacts file, it itself uses the REST API.

The REST API uses Ingestion Keys, which can be managed through the REST API section of the LC web interface. Access to manage these Ingestion Keys requires the `ingestkey.ctrl` permission.

The REST endpoint is located at a per-datacenter URL. You can query the relevant URL for your Organization using [this REST call](https://api.limacharlie.io/static/swagger/#/orgs/get_orgs__oid__url).

The endpoint is authenticated using Basic Authentication with the user name being the Organization ID (OID) and the password the Ingestion Key, via a POST.

The body of the POST contains the artifact file to ingest. Additional metadata is provided using the following Header fields:

* `lc-source` is a free form string used as an identifier of the origin of the artifact. When an artifact is ingested from a LC sensor, this value is the Sensor ID (SID) of the sensor.
* `lc-hint` if present, this indicates to the backend how the file should be interpreted. It default to `auto` which results in the backend auto-detecting the formal. Currently supported hints include `wel` (Windows Events Log), `prefetch` (Windows prefetch file), `pcap`, `txt`, `pe`, `zeek`, `json`.
* `lc-payload-id` if present, this is a globally unique identifier for the artifact file. It can be used to ingest artifacts in an idempotent way, meaning a second file ingested with this same value will be ignored.
* `lc-path` if present, should be a base-64 encoded string representing the original file path of the artifact on the source system.
* `lc-part` if present, is used to track multi-part artifact uploads. If set, it should be an integer starting at `0` and incrementing for every part with the last part being set to `done`. The `lc-payload-id` MUST be set and constant across all parts.

## Accessing Artifacts

The left navigation contains a link to "Artifacts" which displays all artifacts ingested across all sensors in the organization. From there you can select a specific artifact and view it, or choose to download the original and/or parsed version.

You can also see the artifacts collected for a particular sensor by going to the "Sensors" section, clicking into a sensor, and then clicking on "Artifacts".

## Windows Event Logs

### From Real-Time Events

*Only supported on Windows 2008 and up*

It is possible to subscribe to receive Windows Event Logs in real-time from the sensor. By doing this, the targeted Windows Events will be sent to
the cloud as normal LimaCharlie telemetry events encapsulated in an event type of `WEL`. The Windows Events in those cases will be structured as JSON similarly
to other LimaCharlie telemetry. This means you can create [detection and response rules](/v2/docs/detection-and-response) that operate directly on Windows Events, or even correlate between Windows Events and native LimaCharlie telemetry events.

To configure this collection, you need to specify a special kind of log path as a collection pattern. The format is as follows:

```
wel://EventSource:FilterExpression
```

The `wel://` prefix tells LimaCharlie this is not a file at rest, but a live API request from the sensor. The `EventSource` part of the expression refers
to the `ChannelPath` described in the Windows documentation here: https://docs.microsoft.com/en-us/windows/win32/api/winevt/nf-winevt-evtsubscribe.
The `FilterExpression` component refers to the `Query` parameter described in the same documentation. Additional documentation on the filter format can also be found here: https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events.

Examples of supported patterns:

* `wel://Security:*`
* `wel://Application:*`
* `wel://System:*`
* `wel://Microsoft-Windows-Windows Defender/Operational:*`
* `wel://Microsoft-Windows-PowerShell/Operational:*`
* `wel://Microsoft-Windows-Sysmon/Operational:*`
* `wel://System:Event[System[EventID=4624]]`
* `wel://System:*[System[(EventID='7040')]]`

These WEL events will come in to the sensor's Timeline as `WEL` events like:

```
{
  "EVENT": {
    "EventData": {
      "AuthenticationPackageName": "NTLM",
      "FailureReason": "%%2313",
      "IpAddress": "185.198.69.35",
      "IpPort": "0",
      "KeyLength": "0",
      "LmPackageName": "-",
      "LogonProcessName": "NtLmSsp",
      "LogonType": "3",
      "ProcessId": "0x0",
      "ProcessName": "-",
      "Status": "0xc000006d",
      "SubStatus": "0xc0000064",
      "SubjectDomainName": "-",
      "SubjectLogonId": "0x0",
      "SubjectUserName": "-",
      "SubjectUserSid": "S-1-0-0",
      "TargetDomainName": "",
      "TargetUserName": "USER",
      "TargetUserSid": "S-1-0-0",
      "TransmittedServices": "-",
      "WorkstationName": "-"
    },
    "System": {
      "Channel": "Security",
      "Computer": "demo-win-2016",
      "Correlation": {
        "ActivityID": "{F207C050-075F-0001-952F-331047A7DA01}"
      },
      "EventID": "4625",
      "EventRecordID": "29089367",
      "Execution": {
        "ProcessID": "568",
        "ThreadID": "3456"
      },
      "Keywords": "0x8010000000000000",
      "Level": "0",
      "Opcode": "0",
      "Provider": {
        "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
        "Name": "Microsoft-Windows-Security-Auditing"
      },
      "Security": "",
      "Task": "12544",
      "TimeCreated": {
        "SystemTime": "2024-05-30T22:17:45.516965200Z"
      },
      "Version": "0",
      "_event_id": "4625"
    }
  }
}
```

### From Files at Rest

When running D&R rules against Windows Event Logs (`target: artifact` and `artifact type: wel`) that were acquired from files at rest, although the Artifact Collection Service may ingest
the same Windows Event Log file that contains some records that have already been processed by the rules, the LimaCharlie platform will keep track of the processed `EventRecordID` and therefore will NOT run the same D&R rule over the same record multiple times.

This means you can safely set the Artifact Collection Service to collect various Windows Event Logs from your hosts and run D&R rules over them without risking producing the same alert multiple times.

For most Windows Event Logs available, see `c:\windows\system32\winevt\logs\`.

## Mac Unified Logs

Like with Windows Event Logs (WEL), it is possible to collect Mac Unified Logs (MUL) in real-time from the endpoint.

The mechanism used is very similar to WEL: specify a collection rule with a log path like:

```
mul://<Predicate>
```

where `<Predicate>` is a [predicate filter](https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/Predicates/Articles/pSyntax.html) supplied to the API on the endpoint, selecting a subset of the local Mac Unified Logs to be streamed to the cloud.

Example: `mul://process == "Safari"`
*Note that predicates are case sensitive.*

These selected MUL events will come in to the sensor's Timeline as an event of type `MUL` like:

```
{
  "activityIdentifier": 0,
  "category": "entry",
  "date": 1717107501.628475,
  "level": 2,
  "nessage": "CopyData('DefaultAsciiKeyboardLayoutPasteboard' (<CFUUID 0x600001d47ac0> 425712C6-DAB1-497D-A573-168ECB75AF4A) gen: -1 item: 1264739405 flavor: 'DefaultAsciiKeyboardLayoutFlavor')",
  "process": "Safari",
  "processIdentifier": 77998,
  "sender": "CoreFoundation",
  "storeCategory": 0,
  "subsystem": "com.apple.CFPasteboard",
  "threadIdentifier": 20382671,
  "type": "log"
}
```

---

# Atomic Red Team

**Atomic Red Team** is a library of tests mapped to the MITRE ATT&CK framework, provided by Red Canary. With this Extension, LimaCharlie users can use Atomic Red Team to quickly, portably, and reproducibly test their environments.

Find more information about it [here](https://atomicredteam.io/).

> **Note:** The Atomic Red Team **Extension** has replaced the Atomic Red Team **Service**. Ensure that your Organization disabled/removes the Service and subscribes to the Extension. This documentation applies to the Atomic Red Team extension.

## Enabling the Atomic Red Team Extension

Enabling Atomic Red Team can be done within the LimaCharlie **Marketplace**, or at [this link](https://beta.app.limacharlie.io/add-ons/extension-detail/ext-atomic-red-team).

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-1.png)

Under the Organization dropdown, select a tenant (organization) you want to subscribe to Atomic Red Team and click subscribe.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-2.png)

Please note that Extensions are applied on the per-tenant basis. If you have multiple organizations you want to subscribe to Atomic Red Team, you will need to subscribe each organization to the extension separately.

You can also manage add-ons from the **Subscriptions** menu under **Billing**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-3.png)

Tenants that have been subscribed to the extension, will be marked with a green check mark in the **Organization** dropdown.

## Running Atomic Red Team test(s)

After Atomic Red Team has been enabled for your organization, the **Atomic Red Team** option will be available under the **Extensions** menu in the web UI. Selecting this Extension will render the Atomic Red Team test selection menu.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-4.png)

> **Note:** Currently, LimaCharlie supports Atomic Red Team tests on Sensors installed on Windows operating systems. Furthermore, sensors must be online in order for tests to run.

Within the Atomic Red Team menu, you can select a **Sensor** to run test(s) against. Furthermore, you can also pre-select a set of tests from the full Atomic Red Team suite.

### System Changes

Running Atomic Red Team tests will likely modify some system configurations. LimaCharlie attempts to revert any configuration changes performed, but the core logic is handled by the Atomic Red Team project. The following actions may occur:

* Modify PowerShell scripting permissions
* Modify PowerShell script execution policies
* Check/Modify Microsoft Defender status
* Install dependencies like Nuget
* Install Atomic Red Team technique-specific dependencies
* Technique-specific configuration changes

The list of available tests is updated every time the window is open, ensuring that you are getting all available options from the Atomic Red Team repository.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-5.png)

Select your test(s) of choice, and click 'Run Tests'. You will receive a dialog box with a job id that is associated with this particular run of test(s).

## Checking Atomic Red Team Results

When the Atomic Red Team extension is enabled, you will see an Adapter named `ext-atomic-red-team`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-6.png)

This Adapter corresponds to all Atomic Red Team activity, including jobs run and results returned. As a separate adapter, this also means that Atomic Red Team tests are actionable events. For example, you could construct a rule based on Atomic Red Team test results or feedback from system telemetry.

Viewing the **Timeline** within the `ext-atomic-red-team` adapter will display the test(s) run and associated results, if available.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-7.png)

Note that results are easily distinguished via a `result <MITRE ATT&CK ID>` event name, allowing for easy filtering and analysis.

Within the **Timeline** of the system on which you ran a test, you will also find `RECEIPT` event(s) that contain more details about executed tests. For example, the following output shows data related to a test for ATT&CK ID T1053.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/atomic-8.png)

Between `RECEIPT` events and output in the `ext-atomic-red-team` adapter, you can correlate and identify successful and failed Atomic Red Team tests.

---

# Azure Monitor

Azure Monitor Logs are a feature of Azure Monitor that collect and organize log and performance data from monitored resources. More information on Azure Monitor Logs can be found [here](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-platform-logs).

LimaCharlie can ingest and natively parse Azure Monitor Logs.

## Log Ingestion

Azure Monitor logs can be ingested via:

* [Azure Event Hub](/v2/docs/adapter-types-azure-event-hub)
* LimaCharlie [Webhooks](/v2/docs/tutorial-creating-a-webhook-adapter)

Upon ingestion, the log `category` field is used to define the Event Type.

---

# Azure SQL Audit Logs

Microsoft Azure SQL is a scalable, cloud-hosted database that integrates with the Azure ecosystem. More information on Azure SQL can be found [here](https://azure.microsoft.com/en-us/products/azure-sql/database).

LimaCharlie can ingest and natively parse Azure SQL Server audit logs.

## Log Ingestion

Azure SQL Server audit logs can be ingested via:

* [Azure Event Hub](/v2/docs/adapter-types-azure-event-hub)
* LimaCharlie [Webhooks](/v2/docs/tutorial-creating-a-webhook-adapter)

Upon ingestion, the log `category` field is used to define the Event Type.

---

# Billing

Billing in LimaCharlie is done on monthly cycles and per-Organization (multi-tenant). Sensors can be billed on a quota, set within the organization, or billed on a usage basis.

> **Note**: Some features, such as centralized billing are available to larger LC users like MSSPs. For more details contact us at sales@limacharlie.io.

Exact pricing is available on the [LimaCharlie website](https://limacharlie.io) or [Web App](https://app.limacharlie.io).

## Services

### Sensors

There are two categories of sensors: sensors billed on Quota set by the user (vSensor basis), and sensors billed on usage basis.

| Sensor Type | Billed on | Cost |
| --- | --- | --- |
| Windows | Quota | $3.00/sensor/month |
| Linux | Quota | $3.00/sensor/month |
| macOS | Quota | $3.00/sensor/month |
| Docker | Quota | $3.00/sensor/month |
| VMWare Carbon Black EDR | Quota | $0.6/sensor/month |
| Chrome OS | Quota | $0.30/sensor/month |
| Syslog | Usage basis | $0.20/GB |
| Amazon AWS CloudTrail Logs | Usage basis | $0.20/GB |
| Google Cloud Platform (GCP) Logs | Usage basis | $0.20/GB |
| 1Password | Usage basis | $0.20/GB |
| Microsoft/Office 365 | Usage basis | $0.20/GB |
| Windows Event Logs | Usage basis | $0.20/GB |
| Microsoft Defender | Usage basis | $0.20/GB |
| Duo | Usage basis | $0.20/GB |
| GitHub | Usage basis | $0.20/GB |
| Slack | Usage basis | $0.20/GB |
| CrowdStrike | Usage basis | $0.20/GB |
| IT Glue | Usage basis | $0.20/GB |
| Other external sources | Usage basis | $0.20/GB |

For more information about vSensors and the examples, visit our [help center page](https://help.limacharlie.io/en/articles/5931547-how-is-the-cost-of-sensors-add-ons-calculated-in-limacharlie).

The Quota is the number of sensors (agents) concurrently online that should be supported by the given Organization. The Quota applies to concurrently online sensors, meaning that you may have more sensors registered than your quota.

If sensors attempt to connect to the cloud while the Quota is full, they will simply be turned away for a short period of time. In that case, a special `sensor_over_quota` will also be emitted which you can use in [D&R rules](/v2/docs/detection-and-response) for automation.

To avoid frequent churn, Quota modifications are limited by:

* Up to one quota decrease per day.
* Any number of quota increases per day.

The endpoint service includes [Outputs](/v2/docs/outputs) as well as [D&R rules](/v2/docs/detection-and-response) processed in real-time.

### Insight (Retention)

Insight is also a foundational service of LimaCharlie. It provides a flat 1 year of full retention (full telemetry) for a single price in order to make billing more predictable.

### Usage Based Billing

See [Sleeper Deployment](/docs/sleeper) for more details.

| Connected Time | Events Processed | Events Retained |
| --- | --- | --- |
| $0.10 per 30 days | $0.67 per 100,000 events | $0.17 per 100,000 events |

### Replay (Retroactive Scanning)

[Replay](/v2/docs/replay) allows you to run [D&R rules](/v2/docs/detection-and-response) or [False Positives](/v2/docs/false-positive-rules) against external or historical telemetry. Not to be confused with Searching for specific IoCs which is a free feature of Insight.

Its pricing is based on the number of events (telemetry) scanned.

Pricing is $0.01 per block of 200,000 events. So a query scanning 1,000,000 events will cost $0.05.

The "dry run" mechanism of Replay can also provide you a high watermark of the cost of a query without actually running it.

Replay jobs can also be launched with a maximum number of operation evaluations to consume during the life-cycle of the job. This limit is approximate due to the de-centralized nature of Replay jobs and may vary a bit.

### Artifact Collection

[Artifacts](/v2/docs/artifacts) allows you to ingest artifacts like Syslog, Windows Events Logs as well as more complex file formats like Packet Captures (PCAP), Windows Prefetch files, Portable Executable (PE) etc.

Ingested files can then be downloaded as originals or viewed in parsed formats right from your browser. You can also run [D&R rules](/v2/docs/detection-and-response) against them.

Unlike Insight, the retention period is variable based on a number of days (up to 365) as specified by the user at ingestion time.

All billing for it is done at ingestion time based on the number of days and the size of the file. The billing metric is therefore "byte-days".

For example, a file that is 100 MB and is ingested with a retention period of 10 days would be one-time billed for `100 X 10 MB-days`.

## Add-Ons

LimaCharlie Add-Ons are billed on the vSensor basis. When an add-on is used with a sensor billed on usage (eg., 1Password), the Add-On is free. For more information and the examples, visit our [billing FAQ](/v2/docs/faq-billing).

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Billing Options

LimaCharlie users have multiple billing options available to them, depending on their unique needs. We'll walk through these two options, *Default Billing* and *Unified Billing*, below.

## Default Billing

By default, every Organization is billed using a credit card set at the individual organization level. The billing cycle for each organization starts at the time the organization goes from the free tier into a paid tier. The invoices go to the email address of the user who initially created the organization.

## Unified Billing

For customers that require flexibility managing multiple organizations, LimaCharlie offers unified billing - the ability to customize billing to satisfy their needs.

All the options described below apply based on the "billing domain", which is the domain name of the email address of a user. For example, the users `ceo@mycorp.com` and `sales@mycorp.com` both belong to the `mycorp.com` billing domain.

All organizations under the same billing domain will have their billing cycles on the same day, regardless of the creation time or the time the organization first exits free tier. Instead of receiving one invoice per organization, all invoices for a billing domain will be aggregated together under a single invoice sent manually monthly.

The following are the options you can customize as a part of the Unified Billing:

* Override the email where each individual organization's invoice goes to. Instead of the email of the creator, a central email address (like billing@mycorp.com) is used. Billing domains with unified billing enabled will receive a monthly report summarizing all organizations under the domain and their respective billing.
* Choose to be invoiced manually. Organizations in a billing domain can have their invoices sent manually by email without the use of a credit card. This will then allow the recipient to pay invoices using ACH or credit card, but this will have to be done manually each month.

## Default Billing Setup vs Unified Billing

|  | Default Billing Setup | Unified Billing |
| --- | --- | --- |
| **Can be used by** | Anyone | Customers that have their users share a custom domain name of the email address (for example, the users `ceo@mycorp.com` and `sales@mycorp.com` both belong to the `mycorp.com` domain). |
| **Best suited for** | \* Customers that have one or a few (1-3) tenants to manage \* Enterprise clients that want to manage billing at the department level (billed to different cards) | \* Service providers (MSP, MSSP, DFIR) who manage multiple tenants \* Enterprise clients that want to manage billing at the company level (billed to one card) |
| **Payment method used** | Billed using a credit card set at the individual organization level. | One payment method will be used for all organizations under the same billing domain. |
| **Manual invoicing** | Not available | Available  Organizations in a billing domain can have their invoices sent manually by email without the use of a credit card. This will then allow the recipient to pay invoices using ACH or credit card, but this will have to be done manually each month. |
| **Billing cycle** | Starts at the time the organization goes from the free tier into a paying tier (different billing cycle for each tenant). | All organizations under the same billing domain will have their billing cycles on the same day. |
| **Invoicing** | Users will receive one invoice per organization. | All invoices for a billing domain will be aggregated together under a single invoice sent manually monthly. |
| **Email invoices go to** | Email address of the user who initially created the organization. | Instead of the email of the creator, a central email address (like `billing@mycorp.com`) is used. Billing domains with unified billing enabled will receive a monthly report summarizing all organizations under the domain and their respective billing. |

To learn more or to get setup with Unified Billing, [contact us](https://limacharlie.io/contact).

> **Note:** In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Abbreviations:**
- MSP: Managed Security Services Provider
- MSSP: Managed Security Services Provider
- DFIR: Digital Forensics & Incident Response

---

# BinLib

Binary Library, or "BinLib", is a collection of executable binaries (such as EXE or ELF files) that have been observed within your environment. If enabled, this Extension helps you build your own private collection of observed executables for subsequent analysis and searching.

When LimaCharlie observes a binary and path for the first time a `CODE_IDENTITY` event is generated. The metadata from this event is stored within `binlib`, and is available for searching, tagging, and downloading. Additionally, you can run YARA scans against observed binaries.

## Enabling BinLib

BinLib requires subscribing to the `ext-reliable-tasking` Extension in order to function properly. This can be enabled in the Add-ons marketplace.

BinLib can be a powerful additional to your detection and response capabilities. Analysts can:

* Look for historical evidence of malicious binaries
* Tag previously-observed files for data enrichment (i.e. MITRE ATT&CK Techniques)
* Compare observed hashes to known good or known bad lists
* YARA scan and auto-tag for integration in detection & response rules

## Usage

First, subscribe your tenant to the BinLib extension.

To perform one of the following operations against your own library, choose the command and select **Run Request.**

The BinLib page in the web app offers an easy way to get started with some of the core requests exposed by the extension: Check Hash, Search, and Yara Scan.

### check_hash

*Accepted Values: MD5, SHA1, SHA256*

The `check_hash` operation lets you search to see if a particular hash has been observed in your Organization. Output includes a boolean if the hash was found and three hash values, if available.

Sample Output:

```
{
  "data": {
    "found": true,
    "md5": "e977bded5d4198d4895ac75150271158",
    "sha1": "9e2b05f142c35448c9bc48c40a732d632485c719",
    "sha256": "2f5d0c6159b194d6f0f2eae0b7734708368a23aebf9af4db9293865b57ffcaeb"
  }
}
```

### get_hash_data

*Accepted Values: MD5, SHA1, SHA256*

**Careful Downloading Binaries**

LimaCharlie does not filter the binaries observed by your organization. You must exercise caution if downloading a malicious file. We recommend downloading potential malicious binaries to an isolated analysis system.

The `get_hash_data` operation provides a link to the raw data for the hash of interest, allowing you to download the resulting binary file (if previously observed within your environment).

Sample Output:

```
{
  "data": {
    "download_url": "https://storage.googleapis.com/lc-library-bin/b_2f5d0c...",
    "found": true,
    "md5": "e977bded5d4198d4895ac75150271158",
    "sha1": "9e2b05f142c35448c9bc48c40a732d632485c719",
    "sha256": "2f5d0c6159b194d6f0f2eae0b7734708368a23aebf9af4db9293865b57ffcaeb"
  }
}
```

### get_hash_metadata

*Accepted Values: MD5, SHA1, SHA256*

The `get_hash_metadata` operation obtains the metadata for a hash of interest, including signing details, file type, and additional hashes.

```
{
  "data": {
    "found": true,
    "md5": "e977bded5d4198d4895ac75150271158",
    "metadata": {
      "imp_hash": "c105252faa9163fd63fb81bb334c61bf",
      "res_company_name": "Google LLC",
      "res_file_description": "Google Chrome Installer",
      "res_product_name": "Google Chrome Installer",
      "res_product_version": "113.0.5672.127",
      "sha256": "2f5d0c6159b194d6f0f2eae0b7734708368a23aebf9af4db9293865b57ffcaeb",
      "sig_authentihash": "028f24e2c1fd42a3edaf0dcf8a59afe39201fa7d3bb5804dca8559fde41b3f34",
      "sig_issuer": "US, DigiCert Trusted G4 Code Signing RSA4096 SHA384 2021 CA1",
      "sig_serial": "0e4418e2dede36dd2974c3443afb5ce5",
      "sig_subject": "US, California, Mountain View, Google LLC, Google LLC",
      "size": 5155608,
      "type": "pe"
    },
    "sha1": "9e2b05f142c35448c9bc48c40a732d632485c719",
    "sha256": "2f5d0c6159b194d6f0f2eae0b7734708368a23aebf9af4db9293865b57ffcaeb"
  }
}
```

### search

The `search` operation searches the library for binary data points, including or *other than* a known hash.

Searchable fields include:

* imp_hash
* res_company_name
* res_file_description
* res_product_name
* sha256
* sig_authentihash
* sig_hash
* sig_issuer
* sig_subject
* size
* type

Note that search criteria are ANDed. Binaries must meet ALL criteria to be returned.

Search results can be downloaded as a CSV.

### tag

The `tag` operation allows you to add tag(s) to a hash, allowing for additional classification within binlib.

The below example Tags the Google Installer with the `google` tag.

Successful tagging yields an `updated` event:

```
{
  "data": {
    "found": true,
    "md5": "e977bded5d4198d4895ac75150271158",
    "sha1": "9e2b05f142c35448c9bc48c40a732d632485c719",
    "sha256": "2f5d0c6159b194d6f0f2eae0b7734708368a23aebf9af4db9293865b57ffcaeb",
    "updated": true
  }
}
```

### untag

The `untag` operation removes a tag from a binary.

### YARA scan

The `yara_scan` operation lets you run YARA scans across observed files. Scans require:

* Criteria or hash to filter files to be scanned
* Rule name(s) or rule(s)

You also have the option to tag hits on match.

Note that search criteria are ANDed. Binaries must meet ALL criteria to be returned.

## Automating

Here are some examples of useful rules that could be used to automate interactions with Binlib.

### Scan all acquired files with Yara

This rule will automatically scan all acquired files in binlib with a Yara rule:

```
detect:

event: acquired
op: is tagged
tag: ext:binlib

respond:

- action: report
  name: binlib-test
- action: extension request
  extension action: yara_scan
  extension name: binlib
  extension request:
    hash: '{{ .event.sha256 }}'
    rule_names:
      - yara_rule_name_here
```

and this rule will alert on matches:

```
detect:

event: yara_scan
op: exists
path: event/matches/hash

respond:

- action: report
  name: YARA Match via Binlib
```

---

# Build CTI Capabilities

Ready to ditch the data silos and blind spots? LimaCharlie empowers you with a centralized intelligence hub, seamless integrations, BinLib, your own private VirusTotal-like solution, and the unparalleled precision of YARA scanning. Gain a comprehensive understanding of the threat landscape, proactively hunt for hidden attackers, and build a resilient security posture that leaves no stone unturned in the fight against malware.

## Cyber threat intelligence (CTI) gathering problems

* **Fragmented and siloed data:** Security teams often struggle to gain a holistic understanding of threats due to siloed data from disparate security tools and sensors. This fragmented intelligence hinders effective threat detection, investigation, and response.
* **Manual correlation:** Manually correlating data points from diverse sources is time-consuming and error-prone, making it difficult to identify emerging threats and uncover hidden connections.
* **Visibility gaps:** Unknown malware and suspicious binaries often fly under the radar of traditional antivirus solutions, leaving organizations vulnerable to zero-day attacks and advanced threats.

## LimaCharlie's solution

* **Data consolidation:** Aggregate telemetry from all your security tools, endpoints, and network sources into a single platform. LimaCharlie's comprehensive data ingestion capabilities break down data silos and unify your threat intelligence landscape.
* **Seamless integrations:** Leverage LimaCharlie's robust API integrations to seamlessly connect with external threat feeds, threat intelligence platforms, and security tools. Enrich your internal data with external insights for a broader view of the threat landscape.
* **Private binary library:** Analyze unknown binaries and suspicious files with LimaCharlie's built-in Binary Library. This private VirusTotal-like environment leverages community and internal threat intelligence to rapidly identify malware, even zero-day variants, and assess associated risks.
* **Utilize YARA rules:** Conduct enterprise-wide malware scanning with LimaCharlie's integrated YARA engine. Utilize your own or community-developed YARA rules to detect specific malware families, variants, and even customized threats tailored to your environment, leaving no malicious code undetected.

---

# Build CTI Capabilities

Ready to ditch the data silos and blind spots? LimaCharlie empowers you with a centralized intelligence hub, seamless integrations, BinLib, your own private VirusTotal-like solution, and the unparalleled precision of YARA scanning. Gain a comprehensive understanding of the threat landscape, proactively hunt for hidden attackers, and build a resilient security posture that leaves no stone unturned in the fight against malware.

## Cyber threat intelligence (CTI) gathering problems

* **Fragmented and siloed data:** Security teams often struggle to gain a holistic understanding of threats due to siloed data from disparate security tools and sensors. This fragmented intelligence hinders effective threat detection, investigation, and response.
* **Manual correlation:** Manually correlating data points from diverse sources is time-consuming and error-prone, making it difficult to identify emerging threats and uncover hidden connections.
* **Visibility gaps:** Unknown malware and suspicious binaries often fly under the radar of traditional antivirus solutions, leaving organizations vulnerable to zero-day attacks and advanced threats.

## LimaCharlie's solution

* **Data consolidation:** Aggregate telemetry from all your security tools, endpoints, and network sources into a single platform. LimaCharlie's comprehensive data ingestion capabilities break down data silos and unify your threat intelligence landscape.
* **Seamless integrations:** Leverage LimaCharlie's robust API integrations to seamlessly connect with external threat feeds, threat intelligence platforms, and security tools. Enrich your internal data with external insights for a broader view of the threat landscape.
* **Private binary library:** Analyze unknown binaries and suspicious files with LimaCharlie's built-in Binary Library. This private VirusTotal-like environment leverages community and internal threat intelligence to rapidly identify malware, even zero-day variants, and assess associated risks.
* **Utilize YARA rules:** Conduct enterprise-wide malware scanning with LimaCharlie's integrated YARA engine. Utilize your own or community-developed YARA rules to detect specific malware families, variants, and even customized threats tailored to your environment, leaving no malicious code undetected.

---

# Building Products

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For cybersecurity startups and builders, the SecOps Cloud Platform offers a robust foundation to create valuable products and services. The SCP helps innovators get to market faster, build genuinely independent businesses, increase their probability of success, and scale successful offerings with ease.

## 3 ways to go to market more effectively

The SecOps Cloud Platform provides the tools and infrastructure needed to secure any given organizationand is designed to be flexible and highly customizable. Because of this, the SCP enables many different types of solutions. Individual builders' use cases can vary significantly. Nevertheless, all startups and product developers using the platform will benefit from the following three recommendations:

* Focus on Your Core Value
* Reduce Up-front Costs
* Build to Scale

## Focus on Your Core Value

The SecOps Cloud Platform delivers foundational, well-understood security technologies as capabilities: as open, cloud-native primitives instead of black-box tools. Here's how builders can use this fact to create better products and service offerings:

**Clarify your differentiators.** In a crowded marketplace where buyers are already wary of tool sprawl, it's difficult to stand outand challenging to convince buyers to take on another vendor. To succeed, startups must demonstrate clear value and differentiate themselves. Determine what sets you apart, and where you can deliver the greatest value to customers. This is where your internal engineering resources should be focused.

**Offload infrastructure work.** The SecOps Cloud Platform offers the kinds of mature cybersecurity capabilities that teams used to have to develop themselves or purchase as part of a product. This includes things like: Deploying endpoint capabilities via a multiplatform agent, alerting and correlating logs from any source, automating real-time analysis and response regardless of the environment, routing telemetry data to any destination, performing historical threat hunts, isolating endpoints from a network remotely, and many more.

In short, cybersecurity builders no longer need to "reinvent the wheel" in order to get to market. Here again, the clear analogy is to the IT public cloud. Most software developers today wouldn't invest in physical servers or develop complex, in-house solutions to handle application deployment and scaling. They would simply leverage cloud-based services like AWS Lambda or Azure Functions and run their applications without ever worrying about the underlying infrastructure.

Similarly, by using the infrastructure capabilities of the SCP, cybersecurity builders can spend their time and resources on their core value propositionthereby reducing maintenance and integration challenges, eliminating external dependencies, and avoiding the risk that comes from building on someone else's product.

**Work with SCP engineers to develop custom integrations.** The SecOps Cloud Platform is a vendor-neutral provider of tooling and infrastructure for the cybersecurity industry. It is not a potential competitor.

As you develop on the SCP, reach out to LimaCharlie engineers for support in creating customized integrations, advice on best practices for a configurations or deployments, or feature requests that you'd like to see in the development roadmap. The SCP's public cloud business model means that the platform succeeds when its users succeed, so someone will always be on hand to help.

By building on a public cloud for cybersecurity, startups can focus on what they do best without having to develop and maintain DIY solutionsand without putting their business in the hands of a traditional vendor.

## Reduce up-front costs

The SecOps Cloud Platform has a transparent, pay-per-use pricing model and delivers all capabilities on demand. In addition, the platform offers a number of valuable free resources. This helps builders to cut costs and reduce initial investment in several ways:

**Conduct research and develop a prototype for free.** The SCP gives all users access to a fully featured free tier that includes two sensors. There is thus zero up-front cost to begin researching the platform, testing your idea, or even developing a prototype. Start by seeing if the SCP is the right choice for your project. Then, save money on early-stage development work once you begin.

**Build without lock-in.** The SCP's pricing model means you only pay for what you need, for as long as you use it. You don't have to deal with mandatory minimums, long-term contracts, complex licensing, or termination fees. This enables you to create on the platform secure in the knowledge that you are not committed to a given level of spending before your growth justifies itand that you aren't locked into your infrastructure provider.

**Use available SCP resources to save money.** Building on the SCP also offers several direct and indirect ways to lower costs during development.

The SecOps Cloud Platform is designed to be as user-friendly and easy to master as possible. In addition, the SCP is supported by extensive documentation, an active community forum of users, and a learning library full of tutorials and walkthroughs. This means developers will spend less time learning a new technologyand more time building.

In addition, you can make use of more direct forms of assistance. Users can reach out to SCP engineers at any time for help. Qualified builders can also apply for a $1000 platform credit through the platform's Cybersecurity Infrastructure Grant Program. Leverage these resources to reduce your development costs and ensure that your engineers are spending their time on tasks that add the most value.

**Meet compliance needs with free storage.** All telemetry data brought into the SCP is stored for the cost of ingestion for one full year. If your project has data retention or compliance needs, leverage the SCP's default storage capability to help keep your data storage costs down.

**Take advantage of discounted pricing.** If you've decided to build with the SCP for the foreseeable futureor if your product or service has started to see significant uptakeuse discounted pricing options to save money as you grow. The SecOps Cloud Platform provides volume-based discounts to help you improve your savings as usage increases, as well as annual or multi-year discounts for those ready to commit to longer-term platform usage.

The SCP gives cybersecurity builders many of the competitive advantages the IT public cloud offers to startups in other verticalsand a high degree of direct assistance and support as well.

## Build to scale

The SecOps Cloud Platform enables scalable cybersecurity operations. Here's how developers can benefit from building on such a platform:

**Future-proof your infrastructure.** Cybersecurity startups often turn to open-source or custom-built infrastructure to save money and stay independent. But while this approach may work early on, its limitations become apparent over time. It's possible to build performant and successful cybersecurity projects on open-source or DIY technologies.

However, many businesses that take this route experience difficulties when they grow. The complexity, integration challenges, and troubleshooting work that are manageable with a small user base can quickly become untenable at scale. Before basing a part of your project on a custom or open-source solution, consider the challenges you will encounter later on if you are successful. You may be better served by using the SCP for that aspect of your offering.

**Build on a scalable platform.** The SecOps Cloud Platform is designed to help organizations scale their security operations. Basic assumptions of the platform include things like multitenancy, flexibility, open APIs, and rich automation capabilities. Builders should plan to scale from the outsetleveraging the SCP's engineering-centric approach to support future growth by developing architectures, integrations, and workflows that will enable scaling without limits.

Any successful cybersecurity business will encounter challenges as it attempts to increase its customer support or its development work. However, building on an engineering-centric platform enables startups to plan for the future from day oneand makes growth easier and more trouble-free.

**Scale with your revenue.** A major problem for early-stage cybersecurity startups is that they must spend money on fixed infrastructure costs without having enough users for that to be profitable. If funding runs out before a productmarket fit is found, the business fails.

The SecOps Cloud Platform offers an alternative route. Leverage the SCP's pay-per-use pricing to scale your infrastructure spending with your revenue. Even if you start off with a small customer base, you won't be losing money on infrastructure costs. Conserve your resources and allocate your spending to development, marketing, and sales efforts instead.

The SCP offers builders a firm foundation for success. It provides a platform that is built to scaleand its pay-as-you-go pricing helps startups extend their runway and grow gradually and safely.

---

# Building Products

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For cybersecurity startups and builders, the SecOps Cloud Platform offers a robust foundation to create valuable products and services. The SCP helps innovators get to market faster, build genuinely independent businesses, increase their probability of success, and scale successful offerings with ease.

## 3 ways to go to market more effectively

The SecOps Cloud Platform provides the tools and infrastructure needed to secure any given organizationand is designed to be flexible and highly customizable. Because of this, the SCP enables many different types of solutions. Individual builders' use cases can vary significantly. Nevertheless, all startups and product developers using the platform will benefit from the following three recommendations:

* Focus on Your Core Value
* Reduce Up-front Costs
* Build to Scale

## Focus on Your Core Value

The SecOps Cloud Platform delivers foundational, well-understood security technologies as capabilities: as open, cloud-native primitives instead of black-box tools. Here's how builders can use this fact to create better products and service offerings:

**Clarify your differentiators.** In a crowded marketplace where buyers are already wary of tool sprawl, it's difficult to stand outand challenging to convince buyers to take on another vendor. To succeed, startups must demonstrate clear value and differentiate themselves. Determine what sets you apart, and where you can deliver the greatest value to customers. This is where your internal engineering resources should be focused.

**Offload infrastructure work.** The SecOps Cloud Platform offers the kinds of mature cybersecurity capabilities that teams used to have to develop themselves or purchase as part of a product. This includes things like: Deploying endpoint capabilities via a multiplatform agent, alerting and correlating logs from any source, automating real-time analysis and response regardless of the environment, routing telemetry data to any destination, performing historical threat hunts, isolating endpoints from a network remotely, and many more.

In short, cybersecurity builders no longer need to "reinvent the wheel" in order to get to market. Here again, the clear analogy is to the IT public cloud. Most software developers today wouldn't invest in physical servers or develop complex, in-house solutions to handle application deployment and scaling. They would simply leverage cloud-based services like AWS Lambda or Azure Functions and run their applications without ever worrying about the underlying infrastructure.

Similarly, by using the infrastructure capabilities of the SCP, cybersecurity builders can spend their time and resources on their core value propositionthereby reducing maintenance and integration challenges, eliminating external dependencies, and avoiding the risk that comes from building on someone else's product.

**Work with SCP engineers to develop custom integrations.** The SecOps Cloud Platform is a vendor-neutral provider of tooling and infrastructure for the cybersecurity industry. It is not a potential competitor.

As you develop on the SCP, reach out to LimaCharlie engineers for support in creating customized integrations, advice on best practices for a configurations or deployments, or feature requests that you'd like to see in the development roadmap. The SCP's public cloud business model means that the platform succeeds when its users succeed, so someone will always be on hand to help.

By building on a public cloud for cybersecurity, startups can focus on what they do best without having to develop and maintain DIY solutionsand without putting their business in the hands of a traditional vendor.

## Reduce up-front costs

The SecOps Cloud Platform has a transparent, pay-per-use pricing model and delivers all capabilities on demand. In addition, the platform offers a number of valuable free resources. This helps builders to cut costs and reduce initial investment in several ways:

**Conduct research and develop a prototype for free.** The SCP gives all users access to a fully featured free tier that includes two sensors. There is thus zero up-front cost to begin researching the platform, testing your idea, or even developing a prototype. Start by seeing if the SCP is the right choice for your project. Then, save money on early-stage development work once you begin.

**Build without lock-in.** The SCP's pricing model means you only pay for what you need, for as long as you use it. You don't have to deal with mandatory minimums, long-term contracts, complex licensing, or termination fees. This enables you to create on the platform secure in the knowledge that you are not committed to a given level of spending before your growth justifies itand that you aren't locked into your infrastructure provider.

**Use available SCP resources to save money.** Building on the SCP also offers several direct and indirect ways to lower costs during development.

The SecOps Cloud Platform is designed to be as user-friendly and easy to master as possible. In addition, the SCP is supported by extensive documentation, an active community forum of users, and a learning library full of tutorials and walkthroughs. This means developers will spend less time learning a new technologyand more time building.

In addition, you can make use of more direct forms of assistance. Users can reach out to SCP engineers at any time for help. Qualified builders can also apply for a $1000 platform credit through the platform's Cybersecurity Infrastructure Grant Program. Leverage these resources to reduce your development costs and ensure that your engineers are spending their time on tasks that add the most value.

**Meet compliance needs with free storage.** All telemetry data brought into the SCP is stored for the cost of ingestion for one full year. If your project has data retention or compliance needs, leverage the SCP's default storage capability to help keep your data storage costs down.

**Take advantage of discounted pricing.** If you've decided to build with the SCP for the foreseeable futureor if your product or service has started to see significant uptakeuse discounted pricing options to save money as you grow. The SecOps Cloud Platform provides volume-based discounts to help you improve your savings as usage increases, as well as annual or multi-year discounts for those ready to commit to longer-term platform usage.

The SCP gives cybersecurity builders many of the competitive advantages the IT public cloud offers to startups in other verticalsand a high degree of direct assistance and support as well.

## Build to scale

The SecOps Cloud Platform enables scalable cybersecurity operations. Here's how developers can benefit from building on such a platform:

**Future-proof your infrastructure.** Cybersecurity startups often turn to open-source or custom-built infrastructure to save money and stay independent. But while this approach may work early on, its limitations become apparent over time. It's possible to build performant and successful cybersecurity projects on open-source or DIY technologies.

However, many businesses that take this route experience difficulties when they grow. The complexity, integration challenges, and troubleshooting work that are manageable with a small user base can quickly become untenable at scale. Before basing a part of your project on a custom or open-source solution, consider the challenges you will encounter later on if you are successful. You may be better served by using the SCP for that aspect of your offering.

**Build on a scalable platform.** The SecOps Cloud Platform is designed to help organizations scale their security operations. Basic assumptions of the platform include things like multitenancy, flexibility, open APIs, and rich automation capabilities. Builders should plan to scale from the outsetleveraging the SCP's engineering-centric approach to support future growth by developing architectures, integrations, and workflows that will enable scaling without limits.

Any successful cybersecurity business will encounter challenges as it attempts to increase its customer support or its development work. However, building on an engineering-centric platform enables startups to plan for the future from day oneand makes growth easier and more trouble-free.

**Scale with your revenue.** A major problem for early-stage cybersecurity startups is that they must spend money on fixed infrastructure costs without having enough users for that to be profitable. If funding runs out before a productmarket fit is found, the business fails.

The SecOps Cloud Platform offers an alternative route. Leverage the SCP's pay-per-use pricing to scale your infrastructure spending with your revenue. Even if you start off with a small customer base, you won't be losing money on infrastructure costs. Conserve your resources and allocate your spending to development, marketing, and sales efforts instead.

The SCP offers builders a firm foundation for success. It provides a platform that is built to scaleand its pay-as-you-go pricing helps startups extend their runway and grow gradually and safely.

---

# Building the User Interface

## Auto Generated UI

The Extensions UI uses the information provided in the schema to auto-determine its UI elements, and for most simple extensions, the UI will be able to auto conform based on the bare minimum schema definition alone. However, further customization may be made in the schema for more complex or specific use cases by adjusting the layout, or adjusting the details for a specific field.

### Deconstructing the Page

Generally the top of the extension page will show the extension label and its short description. If it exists, it will also show a button for quick access to this extension's "associated sensor".

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-1.png)

In the top right, any actions (as defined in your request schema) will be displayed as a dropdown and button.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-2.png)

Note that there are small changes to this structure depending on the layout selected, however all variations should be intuitive as they do not deviate much from this general page structure. Beyond this, main content of the page is determined by the layout.

### Picking Your Layout Type

* `auto` (default layout, it will pick one of the below)
* `config` (use this if you have a configuration)
* `editor` (very specific use-case for editing large code blocks like yaml)
* `action` (use this to prioritize certain actions in the UI)
* `description`
* `key` (just a variation of description)

For the action, and editor layouts, make sure you define one (or more) default actions as well. The editor UI for the action layout will show all the actions in-page, as opposed to a button on the top right. When set to the editor layout, the UI will automatically run the default action and display the results and a supported action.

### Form Data Types

Every field has the following optional details to further adjust the UI.

* **label**: Add a label if you want a more 'human-legible' label on this field
* **placeholder**: Placeholder text on the input can serve as an example for the user
* **description**: A description for this field can be added that will be available as a tooltip on the UI next to the field label
* **display_index**: The display index starts at 1 (not 0) and guides the GUI on the order to show the fields. A display index of 1, will display before a display index of 2.
* **default_value**: A default value for the field, will auto-populate the field with this value

Some other configurations that conditionally apply to specific data_types:

* **filter**: Available on select primitive data_types.
* **enum_values**: Details on the available enums, to support the enum data type.
* **complex_enum_values**: Details to support the complex enum data type. Supports reference links, and categories.
* **object**: An object that contains nested key-value pairs for more fields, and serves to detail the nested fields.

For the complete list of all data types, please see the [page on data types](/v2/docs/schema-data-types).

## Nuanced Usage

If your extension requires it, there are more opportunities to adjust the UI in order to better guide or facilitate a user on using your extension.

### Multiple Layouts as Tabs

In the schema, it is possible to define several views to utilize a combination of layout types. This may be useful in order to guide a user on how you want them to use your extension.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-3.png)

### Setting Supported Actions

Functionality for this field is set to be expanded in the future

Please feel free to reach out to us on our community slack if you'd like to stay up to date on

Supported actions are tied to a request's (also called "actions") response. It allows the response data to be modified and passed along to a follow-up action. This may be useful when operating a dry run, or triggering a workflow.

---

# Building the User Interface

## Auto Generated UI

The Extensions UI uses the information provided in the schema to auto-determine it's UI elements, and for most simple extensions, the UI will be able to auto conform based on the bare minimum schema definition alone. However, further customization may be made in the schema for more complex or specific use cases by adjusting the layout, or adjusting the details for a specific field.

### Deconstructing the Page

Generally the top of the extension page will show the extension label and it's short description. If it exists, it will also show a button for quick access to this extension's "associated sensor".

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-1.png)

In the top right, any actions (as defined in your request schema) will be displayed as a dropdown and button.
![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-2.png)

Note that there are small changes to this structure depending on the layout selected, however all variations should be intuitive as they do not deviate much from this general page structure.
Beyond this, main content of the page is determined by the layout.

### Picking Your Layout Type

* `auto` (default layout, it will pick one of the below)
* `config` (use this if you have a configuration)
* `editor` (very specific use-case for editing large code blocks like yaml)
* `action` (use this to prioritize certain actions in the UI)
* `description`
* `key` (just a variation of description)

For the action, and editor layouts, make sure you define one (or more) default actions as well. The editor UI for the action layout will show all the actions in-page, as opposed to a button on the top right. When set to the editor layout, the UI will automatically run the default action and display the results and a supported action.

### Form Data Types

Every field has the following optional details to further adjust the UI.

* **label**: Add a label if you want a more 'human-legible' label on this field
* **placeholder**: Placeholder text on the input can serve as an example for the user
* **description**: A description for this field can be added that will be available as a tooltip on the UI next to the field label
* **display\_index**: The display index starts at 1 (not 0) and guides the GUI on the order to show the fields. A display index of 1, will display before a display index of 2.
* **default\_value**: A default value for the field, will auto-populate the field with this value

Some other configurations that conditionally apply to specific data\_types:

* **filter**: Available on select primitive data\_types.
* **enum\_values**: Details on the available enums, to support the enum data type.
* **complex\_enum\_values**: Details to support the complex enum data type. Supports reference links, and categories.
* **object**: An object that contains nested key-value pairs for more fields, and serves to detail the nested fields.

For the complete list of all data types, please see the [page on data types](/v2/docs/schema-data-types).

## Nuanced Usage

If your extension requires it, there are more opportunities to adjust the UI in order to better guide or facilitate a user on using your extension.

### Multiple Layouts as Tabs

In the schema, it is possible to define several views to utilize a combination of layout types. This may be useful in order to guide a user on how you want them to use your extension.
![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-3.png)

### Setting Supported Actions

Functionality for this field is set to be expanded in the future

Please feel free to reach out to us on our community slack if you'd like to stay up to date on

Supported actions are tied to a request's (also called "actions") response. It allows the response data to be modified and passed along to a follow-up action. This may be useful when operating a dry run, or triggering a workflow.

---

# ChromeOS Support

Gain visibility in your Chrome fleet and enhance ChromeOS security to unify endpoint protection across your organization with LimaCharlie's comprehensive, centralized, and adaptable platform.

## ChromeOS support problems

* **Growing in popularity:** Chrome books are the fastest growing segment of personal computers and are being adopted by organizations and schools en masse. Despite this, there is very little security coverage (other than that which is conferred by the platform) and without LimaCharlie, visibility requires an SSL proxy.
* **Visibility gap:** Managing and monitoring ChromeOS devices within traditional security infrastructures is challenging due to compatibility limitations and lack of centralized visibility, especially in enterprise environments.
* **Chromebook threats rising:** ChromeOS vulnerabilities and zero-day exploits are victim to targeted attacks, demanding a robust and adaptable security solution.

## LimaCharlie's solution

* **Comprehensive endpoint protection:** LimaCharlie's Chrome Sensor delivers endpoint detection and response (EDR) capabilities directly on Chromebooks and for ChromeOS, providing deep visibility into network activity, installed packages, and downloaded files.
* **Centralized management and insights:** Integrate ChromeOS devices seamlessly into your existing security ecosystem, gain centralized telemetry and threat detection across all endpoints, and simplify management through policies and rules.
* **Advanced detection and threat hunting:** Utilize LimaCharlie's advanced analytics and threat intelligence to proactively identify suspicious activities, hunt for hidden threats, and respond effectively to evolving cyber threats.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# ChromeOS with Google Chrome Enterprise

You can mass deploy the LimaCharlie Sensor for ChromeOS with Google Workspace and [Google Chrome Enterprise](https://chromeenterprise.google/).

## Configuration

1. Log into Google Workspace Admin and go to [Devices -> Chrome -> Apps & extensions -> Users & Browsers](https://admin.google.com/ac/chrome/apps/user).
2. In the **Users & browsers** tab click the "+" button in the bottom right, then choose the option to "Add from Chrome Web Store".
3. Search for the [LimaCharlie Sensor](https://chrome.google.com/webstore/detail/limacharlie-sensor/ljdgkaegafdgakkjekimaehhneieecki) extension and click Select.
4. Click on the LimaCharlie Sensor app to show the installation policy.
5. Set the "Installation Policy" to "Force install".
6. Set the "Policy for extensions" value as follows:

```
{
    "installation_key": {
        "Value": "\"KEY\""
    }
}
```

IMPORTANT: Replace the text "KEY" with the actual value of your Installation Key, in particular the **Chrome Key** which you can obtain from within the LimaCharlie web app.

*Example*
![App_Management_-_Admin_Console.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/App_Management_-_Admin_Console.png)

## Verifying Configuration

ChromeOS endpoints should now start appearing within the related LimaCharlie Organization's sensor list.

You can verify that the configuration was completed successfully by verifying on an individual endpoint.

1. Confirm that the LimaCharlie Sensor extension appears in the list of extensions.
2. Verify that the installation key got applied on the endpoint by going to:  `chrome://policy` and look for the LimaCharlie Sensor. There you should see the Policy name set to `installation_key` and the Policy Value set with your installation key. The Source should list "Cloud".

![endpoint.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/endpoint.png)

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Cloud CLI

LimaCharlie's Cloud CLI Extension (`ext-cloud-cli`) allows you to trigger actions against CLI or API endpoints for third-party products. This extension facilitates bi-directional communication between LimaCharlie and nearly any telemetry source. Actions can be triggered from the Cloud CLI UI or automated via rules.

For a list of platforms supported by this extension, see the nested items on the left-side navigation.

## Usage

The Cloud CLI extension is enabled via the Add-Ons Marketplace. When enabled, the Cloud CLI extension provides the following UI, available via the Extensions menu in LimaCharlie.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/ext-cloud-cli.png)

From this UI, you can build and execute commands against the CLI or API endpoints of the chosen product.

Cloud CLI commands can also be executed via D&R rules and the use of the `extension request` action.

**Example 1:** Stop EC2 instances based on an `instance_id` parameter found in AWS telemetry.

```
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    tool: '{{ "aws" }}'
    command_tokens:
      - ec2
      - stop-instances
      - '--instance-ids'
      - '{{ .event.instance_id  }}'
      - '--region'
      - us-east-1
    credentials: '{{ "hive://secret/secret-name" }}'
```

**Example 2:** Enumerate a list of VMs from an Azure tenant.

```
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    tool: '{{ "az" }}'
    command_line: '{{ "vm list" }}'
    credentials: '{{ "hive://secret/secret-name" }}'
```

### Credentials

You must set up credentials in the respective third-party tools or platforms prior to utilizing this extension. Once procured, credentials can be stored in the Secrets config hive or provided ad-hoc to the extension in the UI. We recommend storing credentials in the Secrets config hive if you plan to make repetitive calls with this extension.

Where available, details for procuring third-party credentials are provided in their respective sub-pages.

---

# Cloud Security

The SecOps Cloud Platform simplifies the difficult task of securing cloud resources and managing complex cloud-based or hybrid infrastructure. LimaCharlie brings better visibility, robust interoperability, and better options for storing data to cloud and hybrid environments.

## Cloud security problems

Organizations are increasingly embracing the benefits of cloud services and solutions. As your business infrastructure integrates with cloud resources securing your environment becomes more complex, leading to:

* **Visibility challenges:** Cloud environments suffer from a lack of visibility that makes it difficult to secure their complex infrastructure.
* **Data volume challenges:** Cloud environments can generate a high volume of logs forcing security teams to make trade-offs when it comes to data ingest cost and visibility.
* **Multi-cloud challenges:** Many organizations use multiple cloud platforms causing visibility challenges across platforms.

## LimaCharlie's solution

LimaCharlie simplifies management of cloud and multi-cloud environments by unifying them on a single platform. In addition to making integrations easier and bringing scalability to security operations, the SecOps Cloud Platform offers:

* **Centralized monitoring:** The LimaCharlie SecOps Cloud Platform offers a unified view across cloud environments that gives users granular visibility into their operation and current state.
* **Highly efficient storage:** LimaCharlie's proprietary high performance storage reduces the complexity and cost associated with long term data retention. All data ingested and alerts generated include a full year of searchable data retention to meet regulatory compliance and security requirements.
* **Platform-independence:** LimaCharlie is platform-independent and can ingest log data from any cloud platform or SaaS application with log retention often cheaper than the cloud vendors themselves.

---

# Cloud Security

The SecOps Cloud Platform simplifies the difficult task of securing cloud resources and managing complex cloud-based or hybrid infrastructure. LimaCharlie brings better visibility, robust interoperability, and better options for storing data to cloud and hybrid environments.

## Cloud security problems

Organizations are increasingly embracing the benefits of cloud services and solutions. As your business infrastructure integrates with cloud resources securing your environment becomes more complex, leading to:

* **Visibility challenges:** Cloud environments suffer from a lack of visibility that makes it difficult to secure their complex infrastructure.
* **Data volume challenges:** Cloud environments can generate a high volume of logs forcing security teams to make trade-offs when it comes to data ingest cost and visibility.
* **Multi-cloud challenges:** Many organizations use multiple cloud platforms causing visibility challenges across platforms.

## LimaCharlie's solution

LimaCharlie simplifies management of cloud and multi-cloud environments by unifying them on a single platform. In addition to making integrations easier and bringing scalability to security operations, the SecOps Cloud Platform offers:

* **Centralized monitoring:** The LimaCharlie SecOps Cloud Platform offers a unified view across cloud environments that gives users granular visibility into their operation and current state.
* **Highly efficient storage:** LimaCharlie's proprietary high performance storage reduces the complexity and cost associated with long term data retention. All data ingested and alerts generated include a full year of searchable data retention to meet regulatory compliance and security requirements.
* **Platform-independence:** LimaCharlie is platform-independent and can ingest log data from any cloud platform or SaaS application with log retention often cheaper than the cloud vendors themselves.

---

# Config Hive

The Config Hive, or "Hive", is the main configuration system for LimaCharlie. It is a generic set of APIs using by LimaCharlie to maintain the configuration of various systems within the platform. We make the Config Hive accessible to help you configure the wide range of systems, features, and extensions within LimaCharlie in a cohesive way. Each feature or system configuration lives in its own "hive".

Components managed through Hive:

* Cloud Sensors (`cloud_sensor`)
* Detection & Response Rules (`dr-general`, `dr-managed` and `dr-service`)
* False Positive Rules (`fp`)
* Lookups (`lookup`)
* Secrets (`secret`)

The Hive contains configuration records organized in a simple hierarchy: `/hive/{hive_name}/{oid}/{record_name}`. Let's examine each part of this record:

* The `hive_name` represents the type of records it contains. For example, the `cloud_sensor` hive name will contain all records relating to cloud sensors (cloud hosted LimaCharlie Adapters).
* The `oid` is a "partition" for the records, in this case an Organization ID.
* The `record_name` is the unique name for the record.

Setting and updating records in Hive will automatically orchestrate the necessary changes in the relevant service. For example, updating a `cloud_sensor` record will automatically reapply the new configurations to the cloud hosted LimaCharlie Adapter. Deleting the same record will stop the Adapter.

The record data itself will be dependant on the hive name, but it will always be a JSON dictionary.

## Exploring

The best way to explore configurations in LimaCharlie and hive is through the LimaCharlie CLI (`pip install limacharlie`).

The CLI offers a simple interface to list, get and modify records in a single unified way regardless of the type of configuration.

The core command line commands are:

* `limacharlie hive list --help`
* `limacharlie hive get --help`
* `limacharlie hive set --help`
* `limacharlie hive update --help`
* `limacharlie hive remove --help`

For example, if you want to explore the rules ("general" namespace) stored in LimaCharlie, you could issue:

```
limacharlie hive list dr-general
```

## Record Structure

Records contain 3 components:

* The record data itself (referenced to as `data`), who's format is dependant on the hive where it lives.
* User Metadata (referenced to as `usr_mtd`). As outlined below, this is metadata that can modified directly by you and can be exposed to users using specific permissions without giving access to the full record data.
* System Metadata (referenced to as `sys_mtd`). This is metadata that is generated and maintained by the Hive system.

### User Metadata

The user metadata format is the following:

```
{
    "expiry": 123,          // a milisecond epoch time when the record will automatically expire and be deleted.
    "tags": ["abc", "def"], // a list of tags on this record.
    "enabled": true         // a boolean indicating whether the record is in an "enabled" state or not.
}
```

### System Metadata

The system metadata format is the following:

```
{
    "etag": "abc",        // a unique tag representing the current state of data of the record. Can be used for optimistic transactions: https://en.wikipedia.org/wiki/HTTP_ETag
    "last_author": "abc", // the identity of the last entity having modified the record.
    "last_mod": 123,      // a milisecond epoch of the last time the record was modified.
    "created_by": "abc",  // the identity of the entity that originally created the record.
    "created_at": 123,    // a milisecond epoch of the time the record was originally created.
    "guid": "abc",        // a globally unique identifier of the record (not its data).
    "last_error": "abc",  // the contents of the last error related to the record.
    "last_error_ts": 123  // the milisecond epoch of the last time an error occured relating to the record.
}
```

## Accessing

### REST

The config hive can be accessed through the LimaCharlie REST API (https://api.limacharlie.io/static/swagger/).

### Python CLI

Install the Python LimaCharlie CLI using `pip install limacharlie`.

Possible operations: `limacharlie hive --help`

Repository: https://github.com/refractionPOINT/python-limacharlie/

## Conditional Update

One of the advantages of the Hive system is the ability to perform conditional updates (where you prevent two entities from updating and overwriting each other's changes).

You may perform conditional record updates using the `etag` parameter. When set during an update, the hive system will verify that the record it is about to update currently has the etag provided. If the etags do not match, the update is not performed. This allows you to:

1. Get a Record X
2. Update some values of X locally
3. Set the updated Record X, including the etag received during the Get

This enables you to detect when "update collision" occur. An example implementation can be found in the `update` function of the Python SDK [here](https://github.com/refractionPOINT/python-limacharlie/blob/016abfe041877132e4c6dd948f1532b173ca7883/limacharlie/Hive.py#L121).

### Infrastructure as Code

The Hive system also simplifies how you can store and apply your configurations through infrastructure as code.

All hive related configurations are found under the key `hives`, followed by the hive name. For example:

```
hives:
  dr-general:
    Microsoft Defender MALWAREPROTECTION_RTP_DISABLED:
      data:
        detect:
          event: WEL
          op: and
          rules:
            - op: is
              path: event/EVENT/System/Channel
              value: Microsoft-Windows-Windows Defender/Operational
            - op: is
              path: event/EVENT/System/EventID
              value: "5001"
        respond:
          - action: report
            name: Microsoft-defender-MALWAREPROTECTION_RTP_DISABLED
      usr_mtd:
        enabled: true
        expiry: 0
        tags:
          - defender
```

The above example refers to the `dr-general` hive (general namespace for D&R rules), to the record named `Microsoft Defender MALWAREPROTECTION_RTP_DISABLED` who's `data` contains the actual content of the D&R rule, and this record is enabled, does not expire. The record is tagged with `defender`.

---

# Config Hive: Lookups

## Format

Lookups are dictionaries/maps/key-value-pairs where the key is a string. The lookup can then be queried by various parts of LimaCharlie (like rules). The value component of a lookup must be a dictionary and represents metadata associated with the given key, which will be returned to the rule using the lookup.

Lookup data can be ingested by specifying one of the following root keys indicating the format of the lookup data:

* `lookup_data`: represented direct as parsed JSON.
* `newline_content`: a string where each key is separated by a newline, LimaCharlie will assume the metadata is empty.
* `yaml_content`: a string in YAML format that contains a dictionary with the string keys and dictionary metadata like the `lookup_data`.

## Permissions

* `lookup.get`
* `lookup.set`
* `lookup.del`
* `lookup.get.mtd`
* `lookup.set.mtd`

## Usage

### Infrastructure as Code

```
hives:
    lookup:                             # Example lookup in the lookup hive
        example-lookup:
            data:
                lookup_data:
                    8.8.8.8: {}
                    8.8.4.4: {}
                    1.1.1.1: {}
                optimized_lookup_data:
                    _LC_INDICATORS: null
                    _LC_METADATA: null
            usr_mtd:
                enabled: true
                expiry: 0
                tags:
                    - example-lookup
                comment: ""
    extension_config:                   # Example lookup manager extension config
        ext-lookup-manager:
            data:
                lookup_manager_rules:
                    - arl: ""
                      format: json
                      name: tor
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/tor-ips.json]'
                      tags:
                        - tor
                    - arl: ""
                      format: json
                      name: talos
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/talos-ip-blacklist.json]'
                      tags:
                        - talos
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: ""
```

### Manually in the GUI

Lookups can be added in the web interface by navigating to Automation --> Lookups. Name your lookup, choose the format, and copy paste the contents of your lookup in the `JSON data` field.

LimaCharlie also provides several publicly available lookups for use in your Organization. More information and the contents of these can be found on [GitHub](https://github.com/refractionpoint/lc-public-lookups). The contents of these lookups can be used here as well.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/lookups.png)

### Automatically via the Lookup Manager

If your lookups change frequently and you wish to keep them up to date, LimaCharlie offers the lookup manager extension as a mechanism to automatically update your lookups every 24 hours. Documentation on the lookup manager can be found [here](/v2/docs/ext-lookup-manager).

## Example Lookup

```
{
  "lookup_data": {
    "c:\\windows\\system32\\ping.exe": {
      "mtd1": "known_bin",
      "mtd2": 4
    },
    "c:\\windows\\system32\\sysmon.exe": {
      "mtd1": "good_val",
      "mtd2": 10
    }
  }
}
```

or

```
{
  "newline_content": "lvalue1\nlvalue2\nlvalue3"
}
```

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Config Hive: Secrets

With its multitude of data ingestion and output options, LimaCharlie users can end up with a myriad of credentials and secret keys to faciliate unique data operations. However, not all users should be privy to these secret keys. Within the Config Hive, the `secrets` hive component allows you to decouple secrets from their usage or configuration across LimaCharlie. Furthermore, you can also grant permissions to users that allows them to see the configuration of an output, but not have access to the associated credentials.

The most common usage is for storing secret keys used by various [Adapters](/v2/docs/adapters) or [Outputs](/v2/docs/outputs). By referencing `secrets` within the Config Hive, we can configure these services without needing to reveal secret keys to all users.

Watch the video below to learn more about hive secrets, or continue reading below.

## Format

A secret record in `hive` has a very basic format:

```
{
    "secret": "data"
}
```

The `data` portion of the records in this hive must have a single key called `secret` who's value will be used by various LimaCharlie components.

## Permissions

The `secret` hive requires the following permissions for the various operations:

* `secret.get`
* `secret.set`
* `secret.del`
* `secret.get.mtd`
* `secret.set.mtd`

## Secret Management

Over time, and with enough integrations, you may need to create and/or update secrets on demand. We provide quick options for both via either the LimaCharlie CLI or web app.

### Creating Secrets

With the appropriate permissions, users can create secrets in the following ways:

1. Using the LimaCharlie CLI, secrets can be created using the `limacharlie hive set secret` command (example below).
2. Via the web app, under **Organization Settings** > **Secrets Manager**.

### Updating Secrets

Once they are set, secrets can be updated via the following methods:

1. Using the LimaCharlie CLI, secrets can be updated using the `limacharlie hive update secret` command.
2. Via the web app, **Organization Settings** > **Secrets Manager**. Select the secret you wish to update, and update in the dialog box. Click **Save Secret** to save changes in the platform.

## Usage

Using a secret in combination with an output has very few steps:

1. Create a secret in the `secret` hive
2. Create an Output and use the format `hive://secret/my-secret-name` as the value for a credentials field.

## Example

Let's create a simple secret using the LimaCharlie CLI in a terminal. First, create a small file with the secret record in it:

```
$ echo "my-secret-value" > my-secret
```

Next, set this secret in Hive via the LimaCharlie CLI:

```
$ limacharlie hive set secret --key my-secret --data my-secret --data-key secret
```

You should get a confirmation that the secret was created, including metadata of the secret and associated OID:

```
{
    "guid": "3a7a2865-a439-4d1a-8f50-b9a6d833075c",
    "hive": {
        "name": "secret",
        "partition": "8cbe27f4-aaaa-bbbb-cccc-138cd51389cd"
        },
    "name": "my-secret"
}
```

Next, create an output in the web app, using the value `hive://secret/my-secret` as the Secret Key value.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/secret.png)

And that's it! The output should start as expected, however when viewing the output's configuration, the secret should refer to the `hive` ARN, rather than the actual credentials.

---

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

---

# Config Hive: Yara

## Format

A yara record in `hive` has a very basic format:

```
{
    "rule": "data"
}
```

The `data` portion of the records in this hive must have a single key called `rule` who's value will be the yara rule content used by various LimaCharlie components.

A single rule record can contain a series of actual Yara rule, like this: https://github.com/Yara-Rules/rules/blob/master/malware/APT_APT1.yar

## Permissions

The `yara` hive requires the following permissions for the various operations:

* `yara.get`
* `yara.set`
* `yara.del`
* `yara.get.mtd`
* `yara.set.mtd`

## Usage

Yara rules can be create in the `yara` Hive. Those rules will then be available, either through the `ext-yara` Extension, or directly using the `yara_scan` command directly using the reference `hive://yara/your-rule-name`.

## Example

Let's create a new Yara rule using the LimaCharlie CLI in a terminal.

Assuming you have a Yara rule in the `rule.yara` file.

Load the rule in the LimaCharlie Hive via the CLI:

```
$ limacharlie hive set yara --key my-rule --data rule.yara --data-key rule
```

You should get a confirmation that the rule was created, including metadata of the rule associated OID:

```
{
  "guid": "d88826b7-d583-4bcc-b7d3-4f450a12e1be",
  "hive": {
    "name": "yara",
    "partition": "8cbe27f4-aaaa-bbbb-cccc-138cd51389cd"
  },
  "name": "my-rule"
}
```

Next, assuming you want to issue a scan command directly to a Sensor (via the Console or a rule):

```
yara_scan hive://yara/my-rule
```

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Config Hive: Yara

## Format

A yara record in `hive` has a very basic format:

```
{
    "rule": "data"
}
```

The `data` portion of the records in this hive must have a single key called `rule` who's value will be the yara rule content used by various LimaCharlie components.

A single rule record can contain a series of actual Yara rule, like this: https://github.com/Yara-Rules/rules/blob/master/malware/APT\_APT1.yar

## Permissions

The `yara` hive requires the following permissions for the various operations:

* `yara.get`
* `yara.set`
* `yara.del`
* `yara.get.mtd`
* `yara.set.mtd`

## Usage

Yara rules can be create in the `yara` Hive. Those rules will then be available, either through the `ext-yara` Extension, or directly using the `yara_scan` command directly using the reference `hive://yara/your-rule-name`.

## Example

Let's create a new Yara rule using the LimaCharlie CLI in a terminal.

Assuming you have a Yara rule in the `rule.yara` file.

Load the rule in the LimaCharlie Hive via the CLI:

```
$ limacharlie hive set yara --key my-rule --data rule.yara --data-key rule
```

You should get a confirmation that the rule was created, including metadata of the rule associated OID:

```
{
  "guid": "d88826b7-d583-4bcc-b7d3-4f450a12e1be",
  "hive": {
    "name": "yara",
    "partition": "8cbe27f4-aaaa-bbbb-cccc-138cd51389cd"
  },
  "name": "my-rule"
}
```

Next, assuming you want to issue a scan command directly to a Sensor (via the Console or a rule):

```
yara_scan hive://yara/my-rule
```

> **Note:** In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

> **Note:** Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Container Clusters

You can also run LimaCharlie at the host level in a container cluster system like Kubernetes in order to monitor all running containers on the host with a single Sensor. In fact, this is the preferred method as it reduces the overhead of running LC within every single container.

This is accomplished by a combination of a few techniques:

1. A privileged container running LC.
2. LC runs with `HOST_FS` environment variable pointing to the host's root filesystem mounted within the container.
3. LC runs with the `NET_NS` environment variable pointing to the host's directory listing network namespaces.
4. Running the container with the required flags to make sure it can have proper access.

The first step is straight forward. For example, set the environment like `ENV HOST_FS=/rootfs` and `ENV NET_NS=/netns` as part of your `Dockerfile`. This will let the LC sensor know where it can expect host-level information.

The second step is to run the container like: `docker run --privileged --net=host -v /:/rootfs:ro --env HOST_FS=/rootfs -v /var/run/docker/netns:/netns:ro --env NET_NS=/netns --env LC_INSTALLATION_KEY=your_key your-lc-container-name`.

Remember to pick the appropriate LC sensor architecture installer for the *container* that will be running LC (not the host). So if your privileged container runs Alpine Linux, use the `alpine64` version of LC.

A public version of the container described below is available from dockerhub as: `refractionpoint/limacharlie_sensor:latest`.

## Sample Configurations

This is a sample `Dockerfile` you may use to run LC within a privileged container as described above:

```
# Requires an LC_INSTALLATION_KEY environment variable
# specifying the installation key value.
# Requires a HOST_FS environment variable that specifies where
# the host's root filesystem is mounted within the container
# like "/rootfs".
# Requires a NET_NS environment variable that specific where
# the host's namespaces directory is mounted within the container
# like "/netns".
# Example:
# export ENV HOST_FS=/rootfs
# docker run --privileged --net=host -v /:/rootfs:ro -v /var/run/docker/netns:/netns:ro --env HOST_FS=/rootfs --env NET_NS=/netns --env LC_INSTALLATION_KEY=your_key refractionpoint/limacharlie_sensor

FROM alpine

RUN mkdir lc
WORKDIR /lc

RUN wget https://downloads.limacharlie.io/sensor/linux/alpine64 -O lc_sensor
RUN chmod 500 ./lc_sensor

CMD ./lc_sensor -d -
```

And this is a sample Kubernetes `deployment` on a cluster supporting eBPF (kernel > 5.7):

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: lc-sensor
  namespace: lc-monitoring
  labels:
    app: lc-monitoring
spec:
  minReadySeconds: 30
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      app: lc-monitoring
  template:
    metadata:
      namespace: lc-monitoring
      labels:
        app: lc-monitoring
    spec:
      hostNetwork: true
      hostPID: true
      containers:
        - name: lc-sensor
          image: refractionpoint/limacharlie_sensor:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
            capabilities:
              add: ['CAP_SYS_ADMIN']
          resources:
            requests:
              memory: 128M
              cpu: 0.01
            limits:
              memory: 256M
              cpu: 0.9
          volumeMounts:
            - mountPath: /rootfs
              name: all-host
            - mountPath: /netns
              name: all-host-ns
            - mountPath: /sys/kernel/debug
              name: all-host-krnl
            - mountPath: /sys/kernel/btf
              name: btf
            - mountPath: /lib/modules
              name: libmodules
          env:
            - name: HOST_FS
              value: /rootfs
            - name: NET_NS
              value: /netns
            - name: LC_INSTALLATION_KEY
              value: <<<< YOUR INSTALLATION KEY GOES HERE >>>>
      volumes:
        - name: all-host
          hostPath:
            path: /
        - name: all-host-ns
          hostPath:
            path: /var/run/docker/netns
        - name: all-host-krnl
          hostPath:
            path: /sys/kernel/debug
        - name: btf
          hostPath:
            path: /sys/kernel/btf
        - name: libmodules
          hostPath:
            path: /lib/modules
```

For a cluster not supporting eBPF (kernel < 5.7):

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: lc-sensor
  namespace: lc-monitoring
  labels:
    app: lc-monitoring
spec:
  minReadySeconds: 30
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      app: lc-monitoring
  template:
    metadata:
      namespace: lc-monitoring
      labels:
        app: lc-monitoring
    spec:
      containers:
        - name: lc-sensor
          image: refractionpoint/limacharlie_sensor:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            allowPrivilegeEscalation: true
            privileged: true
          resources:
            requests:
              memory: 128M
              cpu: 0.01
            limits:
              memory: 256M
              cpu: 0.9
          volumeMounts:
            - mountPath: /rootfs
              name: all-host-fs
            - mountPath: /netns
              name: all-host-ns
          env:
            - name: HOST_FS
              value: /rootfs
            - name: NET_NS
              value: /netns
            - name: LC_INSTALLATION_KEY
              value: <<<< YOUR INSTALLATION KEY GOES HERE >>>>
      volumes:
        - name: all-host-fs
          hostPath:
            path: /
        - name: all-host-ns
          hostPath:
            path: /var/run/docker/netns
      hostNetwork: true
```

## SELinux

On some hardened versions of Linux, certain file paths are prevented from loading `.so` (Shared Object) files. LimaCharlie requires a location where it can write `.so` files and load them. To enable this on hardened versions of Linux, you can specify a `LC_MOD_LOAD_LOC` environment variable containing a path to a valid directory for loading, like `/lc` for example. This environment variable needs to be set for the sensor executable (`rphcp`) at runtime.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Developer Grant Program

The Developer Grant Program is designed to help fuel the growth of LimaCharlie add-ons and other projects that utilize the LimaCharlie platform. To help developers with their projects, we offer a $1,000 credit that can be applied towards using LimaCharlie to develop any kind of project you want.

If you are looking to commercialize an idea we can help you get it into our marketplace and if there is traction there, we can further support you in growing.

Interested parties can apply for the grant program [here](https://limacharlie.io/grant-program).

---

# Developer Grant Program

The Developer Grant Program is designed to help fuel the growth of LimaCharlie add-ons and other projects that utilize the LimaCharlie platform. To help developers with their projects, we offer a $1,000 credit that can be applied towards using LimaCharlie to develop any kind of project you want.

If you are looking to commercialize an idea we can help you get it into our marketplace and if there is traction there, we can further support you in growing.

Interested parties can apply for the grant program [here](https://limacharlie.io/grant-program).

---

# Dumper

The Dumper Extension provides the ability to do dumping of several forensic artifacts on Windows hosts. It supports a single action, which is to dump.

It supports multiple targets -- `memory` to dump the memory of the host, and `mft` to dump the MFT of the file system to CSV. The extension then automates the ingestion of the resulting dump (and dump metadata) to LimaCharlie's [Artifact Ingestion system](/v2/docs/artifacts) where it can be downloaded or analyzed, and where you can create rules to automate detections of characteristics of those dumps.

## Usage

When enabled, dumper will be added to the Extensions view inside your Organization. It will accept the following parameters:

* `sid` - a Sensor ID for the host to perform the memory dump
* `target` - memory or mft
* `retention` - the number of days the memory dump should be retained for (default is 30)
* `ignore_cert` - ignore cert errors for payload and collection purposes (default `false`)

Upon submission of a request, the extension will perform a full memory dump of a host and upload the resulting dumps to LimaCharlie's artifact ingestion system and delete the local dumps afterwards.

Dumper requests can also be made via D&R rules. Here is is example of a D&R rule action that makes a request to Dumper:

```
- action: extension request
  extension name: ext-dumper
  extension action: request_dump
  extension request:
    target: memory
    sid: <<routing.sid>>
    retention: 30 #default 30
    ignore_cert: true # default false
```

**Notes:**

The dumper extension does not currently validate that the host has enough available disc space for the memory dump. Although the dumper extension is free, the resulting memory dumps uploaded to LimaCharlie are subject to external logs pricing. This add-on relies on other paid resources (payloads) billed based on usage.

---

# Endpoint Protection

## Overview

The Endpoint Protection (EPP) management in LimaCharlie enables users to view the status of existing EPP solutions (including Windows Free Defender), manage parameters of the deployment and unify alerting from the deployment at scale. This makes it perfect for teams wanting a unified view of the EPP solution, or service providers looking to offer Managed EPP to their customers at scale.

The only requirement is for the LimaCharlie agent to be deployed and the EPP Extension enabled (free).

Once deployed, EPP can be used natively along with the rest of LimaCharlie's automation and routing capabilities.

## How it Works

LimaCharlie Endpoint Protection integrates with third-party EDR solutions to provide a better view of security operations and extend agent's capabilities. Currently this extension applies to:

* Microsoft Windows Defender

The LimaCharlie agent communicates with Windows Defender to determine its status, transfer events, and trigger remediation commands. LimaCharlie Endpoint Protection codifies the best practices of collecting events and alerting on detections. When enabled, this extension creates a starter set of rules. In addition to alerting, these rules can be customized to better align with the operational complexity of user's environments. The LC Endpoint Protection extension provides a reliable and cost efficient way of securing endpoints at scale.

The Endpoint Protection add-on requires agent version `4.33.5` or higher.

## Enabling and configuring Endpoint Protection

To enable Endpoint Protection, first ensure LimaCharlie Endpoint Agent version is 4.33.5 and above, [update](/v2/docs/endpoint-agent-versioning-and-upgrades) if necessary.

Navigate to the [Endpoint Protection extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-epp) in the Add-Ons marketplace. Choose the target Organization and select `Subscribe`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(335).png)

Once subscribed, you can see the Endpoint Protection in the list of Extensions.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(332).png)

The Endpoint Protection extension does two things once both sync settings are enabled:

1. Creates an artifact collection rule named `defender-log-streaming`
   * This rule adds a WEL pattern that collects MS Defender logs, `wel://Microsoft-Windows-Windows Defender/Operational:*` so that LimaCharlie receives the events the Defender produces.
   
   > Note
   >
   > If you already have Defender logs coming in via the Artifact extension, you can uncheck the `Sync Extension Config` box to avoid duplicating entries.

2. Creates D&R rules
   * Generates several D&R rules that alert on various detections and actions taken by Defender.![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(291).png)

3. To apply the Artifact extension configuration and D&R rules, click `Apply Configuration`.

When the SYNC toggles are on, the collection rules and D&R rules are continuously synchronized with LimaCharlie library of best practices.

Once the extension is enabled, it also extends the Web UI with Endpoint Protection functionality, as described below.

## Using the Endpoint Protection extension

Endpoint Protection capabilities are used in three ways.

### Verify Protection

Select a Windows Sensor in the organization. In the Sensor Overview, there is a new section, "Endpoint Protection" that shows the current protection status. Verify that Defender is listed as active on the sensor.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(333).png)

### Perform Scan

Select a Windows Sensor.

Click on File System. Select the folder, and click on the scan icon `Scan with EPP`

### Endpoint Protection Commands

Select a Windows Sensor. Open the Sensor Console As you type "epp" you'll see the available commands. Try `epp_status` - it will return the status.

> Events required in Exfil config
>
> The EPP solution relies on some new events. They are now defaults, and the extension adds them to existing orgs. In rare case you may need to add them manually to Sensor / Event Collection / Event Collection or your Infra As Code. Here is the list:
>
> ```
> EPP_STATUS_REP,EPP_LIST_EXCLUSIONS_REP,EPP_ADD_EXCLUSION_REP,EPP_REM_EXCLUSION_REP,
> EPP_LIST_QUARANTINE_REP,EPP_SCAN_REP
> ```

For reference, this is a list of Endpoint Protection commands:

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(323).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(325).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(327).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(329).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(328).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(330).png)

---

# Enterprise SOC

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For enterprises and other large organizations, the SecOps Cloud Platform is a powerful way to take control of security posture and scale operations. The SCP can help teams gain visibility into their environments, eliminate coverage gaps, solve integration challenges, reduce spending on high-cost tools, free themselves from vendor lock-in, and build custom security solutions to meet their organization's unique needs.

## 3 implementation plans for immediate value

The SecOps Cloud Platform is a comprehensive platform for cybersecurity operationsbut it doesn't have to be implemented all at once. The SCP's public cloud-like delivery model eliminates adoption hurdles for enterprises. Easily scaled and API-first, the SCP enables teams to integrate the platform into their security operations gradually, leveraging its capabilities progressively as they go. Here are three recommended first steps to help enterprises realize value from the SCP quickly.

### 1. Centralize telemetry data to improve visibility and streamline operations

The SecOps Cloud Platform allows enterprises to bring all of their telemetry data into one placeimproving visibility, eliminating coverage gaps, and enabling streamlined SecOps workflows. Here is a general outline of what that looks like:

**Bring your telemetry data into the SCP.** The SecOps Cloud Platform allows enterprise teams to ingest data from any source. The platform's endpoint detection and response (EDR)-type sensors can be deployed directly on Windows, Mac, and Linux endpoints with full feature parity across these OSes. These sensors allow security teams to capture system events and other telemetry data in real timeor import event data from third-party EDR tools such as VMWare Carbon Black, CrowdStrike, SentinelOne, or Microsoft Defender. There are also browser-based EDR sensors to support Chrome and Edge deployments.

Log-type data can also be brought into the SCP using a system of adapters or via webhook. Supported log data sources include O365, 1Password, AWS CloudTrail, Google Cloud Platform (GCP), Slack Audit logs, and many more. For a comprehensive list, refer to the SCP documentation.

**Visualize and manage your telemetry data under a single plane.** Telemetry data brought into the SCP is normalized to a common JSON format and explorable through a single interface. The immediate advantage for security teams is improved visibilityand an end to coverage gaps that can jeopardize organizational security and compliance. In addition, the ability to manipulate data through a single UI helps teams eliminate integration challenges caused by other solutions and streamline their internal workflows.

**Go beyond observability.** The SecOps Cloud Platform's data-routing capabilities mean that it can be used as a simple observability point solution if you choose. But the SCP is capable of far more than this. All telemetry data brought into the platform can be run through an advanced detection and response engine, and wire-speed response actions can be taken on endpoints via the multiplatform SCP agent. From day one, security teams using the SCP for centralization and observability can also apply their own custom detection and response (D&R) logic to all telemetry data brought into the platform, leverage curated rulesets like Sigma, Soteria, or SOC Prime rules for the same purpose, or run historical threat hunts against data stored in the SCP.

The SecOps Cloud Platform helps enterprises improve visibility, eliminate coverage gaps, solve integration challenges, and make their workflows more efficientand this is just the first step in what teams can achieve with the platform.

### 2. Reduce spending on SIEMs and other high-cost solutions

Because the SCP lets security teams bring in data from any source and export it to any destination, the platform can also be used as a pass-through to observe, transform, enrich, and anonymize data in-flight and route it to different destinations in a fine-grained way. This strategy can significantly reduce the costs of security information and event management (SIEM) tools and other expensive third-party solutions.

**Identify inefficiencies in your current data flow.** Many organizations simply send 100% of their telemetry data to their SIEM. They only use a fraction of that data, but they pay for all of it. Conduct a thorough review of how you are currently routing your telemetry data. Determine what data truly needs to be sent to your highest-cost toolsand what can be retained in lower-cost storage.

**Use the SCP's output controls to optimize your data routing.** Your options here are highly flexible and customizable:

Telemetry data can be sent to Splunk, Humio, Elastic, Amazon S3 buckets, Azure Event Hubs, Google Cloud Storage, and many other destinations.

Data can also be streamed to your destination(s) of choice with different degrees of granularity. On the more verbose end of the spectrum, it is possible to send all data events from a sensor to a given destination. But you can also create a tailored stream that sends only specific events to your output destination.

Enterprise teams can thus route their data for optimal cost savings. For example, a team might send only high-priority detections and failed 1Password login attempts to Splunk, a secondary tranche of log data and events to an Amazon S3 bucket, and retain everything else in low-cost cold storage.

**Use free storage and transparent pricing for compliance and additional savings.** The SCP offers one year of free storage of all telemetry data for the cost of ingestion. Pricing is transparent and easy to calculate, making it simple to determine the most cost-effective data flow and storage sites for your telemetry. All telemetry data is retained for one year by default in a fully searchable and explorable format, so you don't have to worry about losing data that you may need later on. Because the total cost of storage in the SCP cloud is often far more affordable than traditional data lakes, many organizations will be able to use the platform's built-in storage to address compliance requirements and reduce costs.

The SCP's data routing capabilities put enterprise teams in full control of their telemetry data, allowing them to cut spending on high-cost solutions while ensuring access to critical data in order to meet compliance and operational needs.

### 3. Simplify tooling and control your infrastructure

The SecOps Cloud Platform delivers the core components required to secure and monitor any organization. Over time, enterprises can leverage the SCP's numerous capabilities to develop a custom security infrastructure that they control completely. And while that is clearly a long-term project, enterprises that adopt the SCP can begin using the platform to simplify their stack right away:

**Replace one-off solutions.** The increasing specialization of cybersecurity products means most enterprise teams rely on a patchwork of solutionsand are sometimes forced to buy a tool to satisfy one, extremely narrow use case. Teams should begin by identifying their one-off tools and vendors and determining how they can be replaced with an SCP solution. The SecOps Cloud Platform offers a rich ecosystem of 100+ cybersecurity capabilities and integrations and a marketplace of add-ons to extend the platform. In many cases, teams will find that it is possible to replace single-use vendors with an SCP solution that offers equal or better performance, reducing tool sprawl and improving security operations at the same time.

**Upgrade existing tools or features.** The fragmentation of the current cybersecurity vendor space means that many enterprise teams end up using tools that excel in one arena but fall short in others. Instead of simply accepting the unsatisfactory parts of their stack, teams can use the SCP to augment or replace underperforming tools and features with best-in-breed alternatives.

**Begin your transition to infrastructure independence.** After teams shed one-off and redundant tools, they should begin to think strategically about how to leverage the SCP to free themselves from vendor lock-in once and for all. Look for vendor contracts due to expire or products nearing end-of-life and work with LimaCharlie engineers to develop, validate, and deploy a custom replacement ahead of time.

In the near term, the SecOps Cloud Platform lets enterprises simplify their deployments significantly. In the long term, it allows organizations to take full control of their tooling, infrastructure, and security posture.

---

# Enterprise SOC

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For enterprises and other large organizations, the SecOps Cloud Platform is a powerful way to take control of security posture and scale operations. The SCP can help teams gain visibility into their environments, eliminate coverage gaps, solve integration challenges, reduce spending on high-cost tools, free themselves from vendor lock-in, and build custom security solutions to meet their organization's unique needs.

## 3 implementation plans for immediate value

The SecOps Cloud Platform is a comprehensive platform for cybersecurity operationsbut it doesn't have to be implemented all at once. The SCP's public cloud-like delivery model eliminates adoption hurdles for enterprises. Easily scaled and API-first, the SCP enables teams to integrate the platform into their security operations gradually, leveraging its capabilities progressively as they go. Here are three recommended first steps to help enterprises realize value from the SCP quickly.

### 1. Centralize telemetry data to improve visibility and streamline operations

The SecOps Cloud Platform allows enterprises to bring all of their telemetry data into one placeimproving visibility, eliminating coverage gaps, and enabling streamlined SecOps workflows. Here is a general outline of what that looks like:

**Bring your telemetry data into the SCP.** The SecOps Cloud Platform allows enterprise teams to ingest data from any source. The platform's endpoint detection and response (EDR)-type sensors can be deployed directly on Windows, Mac, and Linux endpoints with full feature parity across these OSes. These sensors allow security teams to capture system events and other telemetry data in real timeor import event data from third-party EDR tools such as VMWare Carbon Black, CrowdStrike, SentinelOne, or Microsoft Defender. There are also browser-based EDR sensors to support Chrome and Edge deployments.

Log-type data can also be brought into the SCP using a system of adapters or via webhook. Supported log data sources include O365, 1Password, AWS CloudTrail, Google Cloud Platform (GCP), Slack Audit logs, and many more. For a comprehensive list, refer to the SCP documentation.

**Visualize and manage your telemetry data under a single plane.** Telemetry data brought into the SCP is normalized to a common JSON format and explorable through a single interface. The immediate advantage for security teams is improved visibilityand an end to coverage gaps that can jeopardize organizational security and compliance. In addition, the ability to manipulate data through a single UI helps teams eliminate integration challenges caused by other solutions and streamline their internal workflows.

**Go beyond observability.** The SecOps Cloud Platform's data-routing capabilities mean that it can be used as a simple observability point solution if you choose. But the SCP is capable of far more than this. All telemetry data brought into the platform can be run through an advanced detection and response engine, and wire-speed response actions can be taken on endpoints via the multiplatform SCP agent. From day one, security teams using the SCP for centralization and observability can also apply their own custom detection and response (D&R) logic to all telemetry data brought into the platform, leverage curated rulesets like Sigma, Soteria, or SOC Prime rules for the same purpose, or run historical threat hunts against data stored in the SCP.

The SecOps Cloud Platform helps enterprises improve visibility, eliminate coverage gaps, solve integration challenges, and make their workflows more efficientand this is just the first step in what teams can achieve with the platform.

### 2. Reduce spending on SIEMs and other high-cost solutions

Because the SCP lets security teams bring in data from any source and export it to any destination, the platform can also be used as a pass-through to observe, transform, enrich, and anonymize data in-flight and route it to different destinations in a fine-grained way. This strategy can significantly reduce the costs of security information and event management (SIEM) tools and other expensive third-party solutions.

**Identify inefficiencies in your current data flow.** Many organizations simply send 100% of their telemetry data to their SIEM. They only use a fraction of that data, but they pay for all of it. Conduct a thorough review of how you are currently routing your telemetry data. Determine what data truly needs to be sent to your highest-cost toolsand what can be retained in lower-cost storage.

**Use the SCP's output controls to optimize your data routing.** Your options here are highly flexible and customizable:

Telemetry data can be sent to Splunk, Humio, Elastic, Amazon S3 buckets, Azure Event Hubs, Google Cloud Storage, and many other destinations.

Data can also be streamed to your destination(s) of choice with different degrees of granularity. On the more verbose end of the spectrum, it is possible to send all data events from a sensor to a given destination. But you can also create a tailored stream that sends only specific events to your output destination.

Enterprise teams can thus route their data for optimal cost savings. For example, a team might send only high-priority detections and failed 1Password login attempts to Splunk, a secondary tranche of log data and events to an Amazon S3 bucket, and retain everything else in low-cost cold storage.

**Use free storage and transparent pricing for compliance and additional savings.** The SCP offers one year of free storage of all telemetry data for the cost of ingestion. Pricing is transparent and easy to calculate, making it simple to determine the most cost-effective data flow and storage sites for your telemetry. All telemetry data is retained for one year by default in a fully searchable and explorable format, so you don't have to worry about losing data that you may need later on. Because the total cost of storage in the SCP cloud is often far more affordable than traditional data lakes, many organizations will be able to use the platform's built-in storage to address compliance requirements and reduce costs.

The SCP's data routing capabilities put enterprise teams in full control of their telemetry data, allowing them to cut spending on high-cost solutions while ensuring access to critical data in order to meet compliance and operational needs.

### 3. Simplify tooling and control your infrastructure

The SecOps Cloud Platform delivers the core components required to secure and monitor any organization. Over time, enterprises can leverage the SCP's numerous capabilities to develop a custom security infrastructure that they control completely. And while that is clearly a long-term project, enterprises that adopt the SCP can begin using the platform to simplify their stack right away:

**Replace one-off solutions.** The increasing specialization of cybersecurity products means most enterprise teams rely on a patchwork of solutionsand are sometimes forced to buy a tool to satisfy one, extremely narrow use case. Teams should begin by identifying their one-off tools and vendors and determining how they can be replaced with an SCP solution. The SecOps Cloud Platform offers a rich ecosystem of 100+ cybersecurity capabilities and integrations and a marketplace of add-ons to extend the platform. In many cases, teams will find that it is possible to replace single-use vendors with an SCP solution that offers equal or better performance, reducing tool sprawl and improving security operations at the same time.

**Upgrade existing tools or features.** The fragmentation of the current cybersecurity vendor space means that many enterprise teams end up using tools that excel in one arena but fall short in others. Instead of simply accepting the unsatisfactory parts of their stack, teams can use the SCP to augment or replace underperforming tools and features with best-in-breed alternatives.

**Begin your transition to infrastructure independence.** After teams shed one-off and redundant tools, they should begin to think strategically about how to leverage the SCP to free themselves from vendor lock-in once and for all. Look for vendor contracts due to expire or products nearing end-of-life and work with LimaCharlie engineers to develop, validate, and deploy a custom replacement ahead of time.

In the near term, the SecOps Cloud Platform lets enterprises simplify their deployments significantly. In the long term, it allows organizations to take full control of their tooling, infrastructure, and security posture.

---

# Exfil (Event Collection)

The Exfil Extension helps manage which real-time [events](/v2/docs/reference-edr-events) get sent from EDR sensors to LimaCharlie. By default, LimaCharlie Sensors send events to the cloud based on a standard profile. This extension exposes those profiles for customization. The Exfil extension allows you to customize Event Collection from LimaCharlie Sensors, as well as mitigate sensors with high I/O or large [detection and response](/v2/docs/detection-and-response) rulesets.

> Event Collection Rule Synchronization
>
> Please note that Exfil (or Event Collection) rule configurations are synchronized with sensors every few minutes.

## Enabling the Exfil Extension

To enable the Exfil extension, navigate to the [Exfil extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-exfil) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/exfil-1.png "image(231).png")

After clicking subscribe, the Exfil extension should be available almost immediately.

## Using the Exfil Extension

Once the extension is enabled, you will see an **Event Collection** option under **Sensors** in the LimaCharlie web UI.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/exfil-2.png "image(227).png")

There are three rule options within the Exfil extension:

* **Event Collection Rules** manage events sent by the Sensor to the LimaCharlie cloud.
* **Performance Rules** are useful for high I/O servers, but may impact event accuracy. This feature is available only on Windows Sensors.
* **Watch Rules** allow for conditional operators for an event, allowing you to specify a list of sensors to help manage high-volume events. Conditional operators for Watch Rule events include:

  + The **event** itself, such as `MODULE_LOAD`.
  + The **path** within the event component to be evaluated, such as `FILE_PATH`.
  + The **operator** to evaluate or compare that should be done between the path and the value.
  + The **value** to be used in comparison with the operator.

A sample **Watch Rule** might be

```
Event: MODULE_LOAD
Path: FILE_PATH
Operator: ends with
Value: wininet.dll
```

The above rule would configures the sensor(s) to send *only* `MODULE_LOAD` events where the `FILE_PATH` ends with the value `wininet.dll`.

> Performance Rules
>
> Performance rules, applied via tag to a set of Sensors, are useful for high I/O systems. These rules can be set via the web application or REST API.

### Throughput Limits

Enabling *every* event for Exfil can produce an exceedingly large amount of traffic. Our first recommendation would be to optimize events required for detection & response rules, in order to ensure that all rules are active. We'd also recommend prioritizing events that contribute to outputs, such as forwarded `DNS_REQUESTS`.

LimaCharlie attempts to process all events in real-time. However, if events fall behind, they are enqueued to a certain limit. If that limit is reached (e.g. in the case of a long, sustained burst or enabling *all* events at the same time), the queue may eventually get dropped. In that event, an error is emitted to the platform logs.

Seeing event collection errors is a sign you may need to do one of the following:

1. Reduce the population of events collected.
2. Reduce the number of rules you run or rule complexity.
3. Adopt a selective subset of events by utilizing Watch Rules that only bring back events with specific values.
4. Enable the IR mode (below).

#### Afterburner

Before a backlogged queue is dropped, LimaCharlie attempts to increase performance by entering a special mode we call "afterburner." This mode tries to address one of the common scenarios that can lead to a large influx of data: spammy processes starting over and over. This happens in situations such as the building of software, in which executables like `devenv.exe` or `git` can be called hundreds of times per second. The afterburner mode attempts to (1) de-duplicate those processes and (2) assess only each one through the D&R rules and Outputs.

#### IR Mode

The afterburner mode does not address all possible causes or situations. To help with this, LimaCharlie offers "IR mode." This mode is enabled by tagging a LimaCharlie sensor with the tag `ir`. The goal of "IR mode" is to provide a solution for users who want to record a very large number of events, but do not need to run D&R rules over all of them. When enabled, "IR mode" will not de-duplicate events. Furthermore, D&R rules will *only* be run against the follow event types:

1. `CODE_IDENTITY`
2. `DNS_REQUEST`
3. `NETWORK_CONNECTIONS`
4. `NEW_PROCESS`

IR mode is designed to give a balance between recording all events, while maintaining basic D&R rule capabilities.

## Actions via REST API

The following REST API actions can be sent to interact with the Exfil extension:

**List Rules**

```
{
  "action": "list_rules"
}
```

### Event Collection Rules

**Add Event Collection Rule**

```
{
  "action": "add_event_rule",
  "name": "windows-vip",
  "events": [
    "NEW_TCP4_CONNECTION",
    "NEW_TCP6_CONNECTION"
  ],
  "tags": [
    "vip"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Event Collection Rule**

```
{
  "action": "remove_event_rule",
  "name": "windows-vip"
}
```

### Watch Rules

**Add Watch Rule**

```
{
  "action": "add_watch",
  "name": "wininet-loading",
  "event": "MODULE_LOAD",
  "operator": "ends with",
  "value": "wininet.dll",
  "path": [
    "FILE_PATH"
  ],
  "tags": [
    "server"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Watch Rule**

```
{
  "action": "remove_watch",
  "name": "wininet-loading"
}
```

### Performance Rules

**Add Performance Rule**

```
{
  "action": "add_perf_rule",
  "name": "sql-servers",
  "tags": [
    "sql"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Performance Rule**

```
{
  "action": "remove_perf_rule",
  "name": "sql-servers"
}
```

---

# Exfil (Event Collection)

The Exfil Extension helps manage which real-time [events](/v2/docs/reference-edr-events) get sent from EDR sensors to LimaCharlie. By default, LimaCharlie Sensors send events to the cloud based on a standard profile. This extension exposes those profiles for customization. The Exfil extension allows you to customize Event Collection from LimaCharlie Sensors, as well as mitigate sensors with high I/O or large [detection and response](/v2/docs/detection-and-response) rulesets.

> Event Collection Rule Synchronization
>
> Please note that Exfil (or Event Collection) rule configurations are synchronized with sensors every few minutes.

## Enabling the Exfil Extension

To enable the Exfil extension, navigate to the [Exfil extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-exfil) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/exfil-1.png "image(231).png")

After clicking subscribe, the Exfil extension should be available almost immediately.

## Using the Exfil Extension

Once the extension is enabled, you will see an **Event Collection** option under **Sensors** in the LimaCharlie web UI.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/exfil-2.png "image(227).png")

There are three rule options within the Exfil extension:

* **Event Collection Rules** manage events sent by the Sensor to the LimaCharlie cloud.
* **Performance Rules** are useful for high I/O servers, but may impact event accuracy. This feature is available only on Windows Sensors.
* **Watch Rules** allow for conditional operators for an event, allowing you to specify a list of sensors to help manage high-volume events. Conditional operators for Watch Rule events include:

  + The **event** itself, such as `MODULE_LOAD`.
  + The **path** within the event component to be evaluated, such as `FILE_PATH`.
  + The **operator** to evaluate or compare that should be done between the path and the value.
  + The **value** to be used in comparison with the operator.

A sample **Watch Rule** might be

```
Event: MODULE_LOAD
Path: FILE_PATH
Operator: ends with
Value: wininet.dll
```

The above rule would configures the sensor(s) to send *only* `MODULE_LOAD` events where the `FILE_PATH` ends with the value `wininet.dll`.

> Performance Rules
>
> Performance rules, applied via tag to a set of Sensors, are useful for high I/O systems. These rules can be set via the web application or REST API.

### Throughput Limits

Enabling *every* event for Exfil can produce an exceedingly large amount of traffic. Our first recommendation would be to optimize events required for detection & response rules, in order to ensure that all rules are active. We'd also recommend prioritizing events that contribute to outputs, such as forwarded `DNS_REQUESTS`.

LimaCharlie attempts to process all events in real-time. However, if events fall behind, they are enqueued to a certain limit. If that limit is reached (e.g. in the case of a long, sustained burst or enabling *all* events at the same time), the queue may eventually get dropped. In that event, an error is emitted to the platform logs.

Seeing event collection errors is a sign you may need to do one of the following:

1. Reduce the population of events collected.
2. Reduce the number of  rules you run or rule complexity.
3. Adopt a selective subset of events by utilizing Watch Rules that only bring back events with specific values.
4. Enable the IR mode (below).

#### Afterburner

Before a backlogged queue is dropped, LimaCharlie attempts to increase performance by entering a special mode we call "afterburner." This mode tries to address one of the common scenarios that can lead to a large influx of data: spammy processes starting over and over. This happens in situations such as the building of software, in which executables like `devenv.exe` or `git` can be called hundreds of times per second. The afterburner mode attempts to (1) de-duplicate those processes and (2) assess only each one through the D&R rules and Outputs.

#### IR Mode

The afterburner mode does not address all possible causes or situations. To help with this, LimaCharlie offers "IR mode." This mode is enabled by tagging a LimaCharlie sensor with the tag `ir`. The goal of "IR mode" is to provide a solution for users who want to record a very large number of events, but do not need to run D&R rules over all of them. When enabled, "IR mode" will not de-duplicate events. Furthermore, D&R rules will *only* be run against the follow event types:

1. `CODE_IDENTITY`
2. `DNS_REQUEST`
3. `NETWORK_CONNECTIONS`
4. `NEW_PROCESS`

IR mode is designed to give a balance between recording all events, while maintaining basic D&R rule capabilities.

## Actions via REST API

The following REST API actions can be sent to interact with the Exfil extension:

**List Rules**

```
{
  "action": "list_rules"
}
```

### Event Collection Rules

**Add Event Collection Rule**

```
{
  "action": "add_event_rule",
  "name": "windows-vip",
  "events": [
    "NEW_TCP4_CONNECTION",
    "NEW_TCP6_CONNECTION"
  ],
  "tags": [
    "vip"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Event Collection Rule**

```
{
  "action": "remove_event_rule",
  "name": "windows-vip"
}
```

### Watch Rules

**Add Watch Rule**

```
{
  "action": "add_watch",
  "name": "wininet-loading",
  "event": "MODULE_LOAD",
  "operator": "ends with",
  "value": "wininet.dll",
  "path": [
    "FILE_PATH"
  ],
  "tags": [
    "server"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Watch Rule**

```
{
  "action": "remove_watch",
  "name": "wininet-loading"
}
```

### Performance Rules

**Add Performance Rule**

```
{
  "action": "add_perf_rule",
  "name": "sql-servers",
  "tags": [
    "sql"
  ],
  "platforms": [
    "windows"
  ]
}
```

**Remove Performance Rule**

```
{
  "action": "remove_perf_rule",
  "name": "sql-servers"
}
```

---

# Git Sync

The Git Sync Extension is a tool that automates the management of Infrastructure-as-Code (IaC) configurations. It simplifies the process of deploying and managing infrastructure by synchronizing changes between a Git repository and target organizations.

**Key features:**

* **Centralized Configuration:** Stores all IaC configurations in a single Git repository.
* **Recurring Apply:** Can automatically sync IaC changes between Git and LC organizations at regular intervals.
* **Recurring Export:** Can automatically export IaC from LC organizations to GitHub at regular intervals.
* **Export Request:** Allows you to export the configuration of an Organization into the Git repository.
* **Automated Deployment:** Helps automate the deployment process, reducing manual effort.
* **MSSP-Friendly:** Designed to accommodate multiple organizations within a single repository, allowing for global configurations to be shared between orgs.
* **Flexible Configuration:** Allows for customization and additional configuration directories.
* **Transparent Operations:** Tracks operations through an extension Sensor.

By using `ext-git-sync`, you can streamline your IaC workflows, improve consistency, and reduce the risk of errors.

## Use Cases

### Sync FROM Git

If you have a properly structured git repository containing org configurations, the extension can sync the running org configurations with the contents of the configs in git.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/d2 (1).png "pull_config(1).png")

### Export TO Git

Assuming you have an empty git repository, you can configure the extension to export the current org configuration to the repository. It will be placed in an `exports` subdirectory.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/d2 (2).png "push_config(1).png")

## Git Repo Structure

For applying org configs from a git repository, the repo must adhere to the following structure. The root of the repository must contain an `orgs` directory with `[org-id]` child directories, each containing an `index.yaml` .

```
.
 orgs [required]
     a326700d-3cd7-49d1-ad08-20b396d8549d [required]
         index.yaml [required]
```

The `index.yaml` determines which other files in the repo are included in the configuration for this org.

For instance, assume all of the configurations for this org were unique to this org and could be nested inside of the org's directory.

```
.
 orgs
     a326700d-3cd7-49d1-ad08-20b396d8549d
         extensions.yaml
         hives
            cloud_sensor.yaml
            dr-general.yaml
            dr-managed.yaml
            dr-service.yaml
            extension_config.yaml
            fp.yaml
            lookup.yaml
            query.yaml
            secret.yaml
            yara.yaml
         index.yaml
         installation_keys.yaml
         org_values.yaml
         outputs.yaml
         resources.yaml
```

Notice that all configurations for this org are contained within the org's own directory. In this case, the `index.yaml` would simply contain references to the relative path of this org's configuration files. See below for an example of the contents of `index.yaml` for this use case.

```
version: 3
include:
    - extensions.yaml
    - hives/fp.yaml
    - outputs.yaml
    - resources.yaml
    - hives/query.yaml
    - hives/yara.yaml
    - hives/dr-managed.yaml
    - hives/lookup.yaml
    - hives/dr-service.yaml
    - org_values.yaml
    - installation_keys.yaml
    - hives/secret.yaml
    - hives/cloud_sensor.yaml
    - hives/dr-general.yaml
    - hives/extension_config.yaml
```

### Sharing configurations across multiple orgs

Now, assume you have a global rule set you want to apply across many orgs. You could structure the repo similar to the example below.

```
.
 hives
    dr-general.yaml
    yara.yaml
 orgs
     7e41e07b-c44c-43a3-b78d-41f34204789d
        index.yaml
     a326700d-3cd7-49d1-ad08-20b396d8549d
        index.yaml
     cb639126-e0bc-4563-a577-2e559c0610b2
         index.yaml
```

The corresponding `index.yaml` at each org level would look similar to the following

```
version: 3
include:
    - ../../hives/yara.yaml
    - ../../hives/dr-general.yaml
```

### Exporting configurations

Configuration exports will be placed in a separate `exports` subdirectory to avoid overwriting configurations that are pushed across multiple organizations.

```
.
 exports
     orgs
         a326700d-3cd7-49d1-ad08-20b396d8549d
             extensions.yaml
             hives
                cloud_sensor.yaml
                dr-general.yaml
                dr-managed.yaml
                dr-service.yaml
                extension_config.yaml
                fp.yaml
                lookup.yaml
                query.yaml
                secret.yaml
                yara.yaml
             index.yaml
             installation_keys.yaml
             org_values.yaml
             outputs.yaml
             resources.yaml
```

## Setting up Git Sync with Github

This guide walks you through the process of configuring Git synchronization between GitHub and LimaCharlie, allowing for automated deployment and version control of your security configurations.

### Step 0: Making a Git Sync specific SSH Key

* First create the directory

`mkdir -p ~/.ssh/gitsync`

* Set appropriate permissions for the directory

`chmod 700 ~/.ssh/gitsync`

* Now generate the SSH key

`ssh-keygen -t ed25519 -C "limacharlie-gitsync" -f ~/.ssh/gitsync/id_ed25519`

### Step 1: Generate GitHub Deploy Keys

1. Navigate to your GitHub repository
2. Click on the **Settings** tab
3. In the left sidebar, select **Deploy keys**
4. Click the **Add deploy key** button
5. Enter a descriptive title for your key (e.g., "LimaCharlie Git Sync Integration")
6. Paste your public SSH key into the "Key" field
7. **Important:** Check the box for **Allow write access**
8. Click **Add key** to save

### Step 2: Store SSH Private Key in LimaCharlie

1. Log in to your LimaCharlie account
2. Navigate to the **Secret Manager** section of your Organization
3. Click **Create New Secret**
4. Choose a descriptive name for your secret (e.g., "github-deploy-key")
5. Paste the **private** part of your SSH key into the value field
6. Save the secret

### Step 3: Configure Git Sync in LimaCharlie

1. Navigate to the **Git Sync** section in LimaCharlie
2. Under the **SSH Key** section, select **Secret Manager**
3. From the dropdown menu, select the secret you created in Step 2
4. Set the **user name** to `git`
5. Copy the SSH URL from your GitHub repository (found on the repository's main page, under Code)
6. Paste the SSH URL into the **repository** URL field in LimaCharlie
7. Configure the **branch** name (required)
8. Select the push and pull options which allow you to specify which items to push to or pull from Git configurations.
9. Optionally, select push and pull schedules if you wish to regularly sync or export your Infrastructure as Code configurations to and from LimaCharlie. This will create D&R rules on the backend that kick off the push and pull actions on the selected schedule/interval.
10. Click **save settings**.

### Step 4: Verify Integration

1. Perform a test commit to your GitHub repository by clicking "Push to Git" in the upper right corner.
2. Verify that your configuration has been pushed to Github.

### Troubleshooting

If you encounter synchronization issues:

* Verify that the deploy key has proper write permissions
* Ensure the correct SSH URL format is used (should begin with `git@github.com:`)
* Check that the private key in Secret Manager matches the public key added to GitHub

---

# Govee

The Govee Extension allows you to trigger color changes on your [supported Govee lights](https://developer.govee.com/docs/support-product-model) via a rule response action. It requires you to configure a Govee API key in the extension.

## Setup

1. Request an API key from Govee by following their instructions [here](https://developer.govee.com/reference/apply-you-govee-api-key)
2. Get the Device ID (device) and model (sku) of the device you'd like to target by requesting a list of your supported devices from the Govee API:

```
curl --location 'https://openapi.api.govee.com/router/api/v1/user/devices' --header 'Govee-API-Key: YOUR_GOVEE_API_KEY'
```

3. Decide what RGB color(s) you want to use. By default, the extension will alert with red (`255,0,0`), and revert back to white (`255,255,255`) when the alert `duration` has ended.
4. Add your Govee API key to the extension configuration:
    ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/govee.png)

## Usage

When enabled, you may configure the response of a D&R rule to trigger a Govee event. Consider the following example response rule:

```
- action: extension request
  extension action: run
  extension name: ext-govee
  extension request:
    device_id: '{{ "YOUR_GOVEE_DEVICE" }}'
    device_model: '{{ "YOUR_GOVEE_DEVICE_SKU" }}'
    alert_color: '{{ "255,0,0" }}'
    alert_brightness: '{{ "100" }}'
    revert_color: '{{ "255,255,255" }}'
    revert_brightness: '{{ "10" }}'
    duration: '{{ "30" }}'
  suppression:
    is_global: true
    keys:
      - Govee
    max_count: 1
    period: 1m
```

Note that the only required fields here are the `device_id` and `device_model`. Values supplied in the example are the defaults.

### Parameters

**Required parameters:**

* `device_id`: returned via the Govee API, see example response below
* `device_model`: returned via the Govee API, see example response below

**Optional parameters:**

* `alert_color`: color of the light when alert fires, in [RGB format](https://htmlcolorcodes.com/color-picker/), default `255,0,0` (red)
* `revert_color`: color of the light to return to, after alert fires, in [RGB format](https://htmlcolorcodes.com/color-picker/), default `255,255,255` (white)
* `alert_brightness`: brightness of the light, default `100`
* `revert_brightness`: brightness of the light to return to, after alert fires, default `10`
* `duration`: duration of the alert in seconds, how long the light will remain at `alert_color` before returning to `revert_color`, default `30`

**Govee API sample request and response:**

```
curl --location 'https://openapi.api.govee.com/router/api/v1/user/devices' --header 'Govee-API-Key: YOUR_GOVEE_API_KEY'
```

```
{
    "code": 200,
    "message": "success",
    "data": [
        {
            "sku": "H6008",                           # use in `device_model` parameter
            "device": "AA:BB:00:11:22:33:44:55",      # use in `device_id` parameter
            "deviceName": "DetectionLight",
            "type": "devices.types.light",
            "capabilities": [
                ...
            ]
        }
    ]
}
```

---

# Govee

The Govee Extension allows you to trigger color changes on your [supported Govee lights](https://developer.govee.com/docs/support-product-model) via a rule response action. It requires you to configure a Govee API key in the extension.

## Setup

1. Request an API key from Govee by following their instructions [here](https://developer.govee.com/reference/apply-you-govee-api-key)
2. Get the Device ID (device) and model (sku) of the device you'd like to target by requesting a list of your supported devices from the Govee API:

```
curl --location 'https://openapi.api.govee.com/router/api/v1/user/devices' --header 'Govee-API-Key: YOUR_GOVEE_API_KEY'
```

3. Decide what RGB color(s) you want to use. By default, the extension will alert with red (`255,0,0`), and revert back to white (`255,255,255`) when the alert `duration` has ended.
4. Add your Govee API key to the extension configuration:
    ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/govee.png)

## Usage

When enabled, you may configure the response of a D&R rule to trigger a Govee event. Consider the following example response rule:

```
- action: extension request
  extension action: run
  extension name: ext-govee
  extension request:
    device_id: '{{ "YOUR_GOVEE_DEVICE" }}'
    device_model: '{{ "YOUR_GOVEE_DEVICE_SKU" }}'
    alert_color: '{{ "255,0,0" }}'
    alert_brightness: '{{ "100" }}'
    revert_color: '{{ "255,255,255" }}'
    revert_brightness: '{{ "10" }}'
    duration: '{{ "30" }}'
  suppression:
    is_global: true
    keys:
      - Govee
    max_count: 1
    period: 1m
```

Note that the only required fields here are the `device_id` and `device_model`. Values supplied in the example are the defaults.

### Parameters

**Required parameters:**

* `device_id`: returned via the Govee API, see example response below
* `device_model`: returned via the Govee API, see example response below

**Optional parameters:**

* `alert_color`: color of the light when alert fires, in [RGB format](https://htmlcolorcodes.com/color-picker/), default `255,0,0` (red)
* `revert_color`: color of the light to return to, after alert fires, in [RGB format](https://htmlcolorcodes.com/color-picker/), default `255,255,255` (white)
* `alert_brightness`: brightness of the light, default `100`
* `revert_brightness`: brightness of the light to return to, after alert fires, default `10`
* `duration`: duration of the alert in seconds, how long the light will remain at `alert_color` before returning to `revert_color`, default `30`

**Govee API sample request and response:**

```
curl --location 'https://openapi.api.govee.com/router/api/v1/user/devices' --header 'Govee-API-Key: YOUR_GOVEE_API_KEY'
```

```
{
    "code": 200,
    "message": "success",
    "data": [
        {
            "sku": "H6008",                           # use in `device_model` parameter
            "device": "AA:BB:00:11:22:33:44:55",      # use in `device_id` parameter
            "deviceName": "DetectionLight",
            "type": "devices.types.light",
            "capabilities": [
                ...
            ]
        }
    ]
}
```

---

# Hayabusa

## Hayabusa Extension Pricing

While it is free to enable the Hayabusa extension, pricing is applied to downloaded and processed artifacts -- $0.02/GB for the original artifact, and $0.5/GB for the generation of the Hayabusa artifact.

The [Hayabusa](https://github.com/Yamato-Security/hayabusa) extension allows you to run Hayabusa against a specified event log (.evtx) or a collection of event logs (.zip).

Hayabusa is a Windows event log fast forensics timeline generator and threat hunting tool created by the Yamato Security group in Japan.

LimaCharlie will automatically kick off the analysis based off of the artifact ID provided in a rule action, or you can run it manually via the extension.

## Configuration

When enabled, you may configure the response of a D&R rule to run a Hayabusa analysis against an artifact event. Consider the following example D&R rule:

**Detect:**

```
event: ingest
op: exists
path: /
target: artifact_event
artifact type: wel
```

**Respond:**

```
- action: extension request
  extension action: generate
  extension name: ext-hayabusa
  extension request:
       artifact_id: '{{ .routing.log_id }}'
       send_to_timeline: true
       profile: '{{ "timesketch-verbose" }}'
       min_rule_level: '{{ "informational" }}'
```

Note that the only required field here is the `artifact_id`. The other values supplied in the example are the defaults.

## Results

```
hayabusa update-rules

hayabusa csv-timeline -f /path/to/your/artifact --RFC-3339 -p timesketch-$profile --min-level $min_rule_level --no-wizard --quiet -o $artifact_id.csv -U
```

Upon running Hayabusa, a CSV file is generated. The CSV file will be uploaded as a LimaCharlie artifact.

The resulting CSV is compatible with Timesketch, and can be imported [as a timeline](https://timesketch.org/guides/user/upload-data/).

Outputting your data to Google BigQuery is another option, and is [outlined here](/v2/docs/hayabusa-to-bigquery)

Several events will be pushed to the `ext-hayabusa` Sensor timeline:

* `hayabusa_results`: contains the results summary from the Hayabusa output
* `hayabusa_artifact`: contains the `artifact_id` of the CSV file that was uploaded to LimaCharlie
* `hayabusa_event`: many of these will be sent to the timeline if you check the checkbox or parameter for `Send to timeline`, and it contains the raw contents of the Hayabusa CSV output in JSON format

## Arguments

* `artifact_id`: ID of the LimaCharlie artifact to process
* `profile`: either `minimal`, `standard`, `verbose`, `all-field-info`, `all-field-info-verbose`, `super-verbose`, `timesketch-minimal`, or `timesketch-verbose`
  + Default: `timesketch-verbose`
  + [More details](https://github.com/Yamato-Security/hayabusa?tab=readme-ov-file#7-timesketch-minimal-profile-output)
* `min_rule_level`: `informational`, `low`, `medium`, `high`, or `critical`, [more details](https://github.com/Yamato-Security/hayabusa?tab=readme-ov-file#dfir-timeline-commands-1)
  + Default: `informational`
* `send_to_timeline`: whether or not to ingest the Hayabusa results into the sensor timeline as events, boolean, default `true`

## Usage

If you use the LimaCharlie Velociraptor extension, a good use case of this extension would be to trigger Hayabusa analysis upon ingestion of a Velociraptor KAPE files artifact.

Go to Extensions / Velociraptor, and run Collect Artifact request.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/hayabusa-1.png)

Kick off a `Windows.KapeFiles.Targets` artifact collection in the LimaCharlie Velociraptor extension

**Argument options:**

* `EventLogs=Y`
   ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/hayabusa-2.png)
* `KapeTriage=Y` - this is an option, however the extension will first take all .evtx files out of the triage collection and send them through Hayabusa, and ignore the rest, so there is more overhead involved, versus just using `EventLogs=Y`.

Configure a D&R rule to look for these events upon ingestion, and then trigger the Hayabusa extension:

**Detect:**

```
op: and
target: artifact_event
rules:
    - op: is
      path: routing/log_type
      value: velociraptor
    - op: is
      not: true
      path: routing/event_type
      value: export_complete
```

**Respond:**

```
- action: extension request
  extension action: generate
  extension name: ext-hayabusa
  extension request:
      artifact_id: '{{ .routing.log_id }}'
      send_to_timeline: true    # `false` if you only want the CSV artifact
```

## Generate LC Detections from Hayabusa Output

> **Note**
> 
> This capability depends on setting the parameter to send Hayabusa output to the sensor timeline with `send_to_timeline: true`

Assuming you want Hayabusa detections of a certain `Level` or severity sent directly to your LimaCharlie detections stream, you can use the following D&R rule to accomplish this:

**Detect:**

```
event: hayabusa_event
op: and
rules:
  - op: is
    path: routing/hostname
    value: ext-hayabusa
  - op: matches
    path: event/results/Level
    re: (med|high|crit)
```

**Respond:**

```
- action: report
  name: >-
    Hayabusa - {{ .event.results.Level }} - {{ .event.results.message }}
```

The resulting detection would look something like this:

```
{
  "action": "report",
  "data": {
    "cat": "Hayabusa - med - Failed Logon From Public IP",
    "detect": {
      "event": {
        "artifact_id": "eb39c3b4-6312-41c8-8b6e-e0b46b2f870e",
        "artifact_type": "evtx",
        "event": "hayabusa_event",
        "job_id": "2e904fda-6d3f-4ce1-bf82-ede97f3c0d17",
        "results": {
          "Channel": "Sec",
          "Computer": "windows-server-2022-01304add-3354-4cca-b574-b0a54d7bb6f4-0",
          "Details": "Type: 3 - NETWORK  TgtUser: 4cca  SrcComp: WIN-S2Q2306JU66  SrcIP: 185.161.248.147  AuthPkg: NTLM  Proc: -",
          "EventID": "4625",
          "EvtxFile": "/tmp/triage_1078055872.evtx",
          "ExtraFieldInfo": "FailureReason: BAD USER OR PW  IpPort: 0  KeyLength: 0  LogonProcessName: NtLmSsp  ProcessId: 0  Status: BAD USER OR PW  SubStatus: UNKNOWN USER  SubjectLogonId: 0x0  SubjectUserSid: S-1-0-0  TargetDomainName: windows-server-2022-01304add-3354-4cca-b574-b0a54d7bb6f4-0  TargetUserSid: S-1-0-0",
          "Level": "med",
          "MitreTactics": "InitAccess  Persis",
          "MitreTags": "T1078  T1190  T1133",
          "OtherTags": "",
          "RecordID": "681128",
          "RuleFile": "win_security_susp_failed_logon_source.yml",
          "datetime": "2024-03-20 21:50:55.930385+00:00",
          "message": "Failed Logon From Public IP",
          "timestamp_desc": "hayabusa"
        }
      },
      "routing": {
        "arch": 9,
        "did": "",
        "event_id": "0a6989a1-af71-4583-a8bc-e766bd2a81d8",
        "event_time": 1711071722721,
        "event_type": "hayabusa_event",
        "ext_ip": "internal",
        "hostname": "ext-hayabusa",
        "iid": "bfac2d1f-5d8c-4115-9df2-633a4f1d062b",
        "int_ip": "",
        "moduleid": 6,
        "oid": "01304add-3354-4cca-b574-b0a54d7bb6f4",
        "plat": 2415919104,
        "sid": "3109b3c7-c5ca-4029-b493-4d4e6766c4d3",
        "tags": [
          "ext:ext-hayabusa",
          "lc:system"
        ],
        "this": "76088a58bb99484c82cf9e9065fce1ea"
      },
      "ts": "2024-03-22 01:42:02"
    },
    "detect_id": "90609b8b-c2b8-4537-b17e-5d1665fd8717",
    "gen_time": 1711114007077,
    "link": "https://app.limacharlie.io/orgs/01304add-3354-4cca-b574-b0a54d7bb6f4/sensors/3109b3c7-c5ca-4029-b493-4d4e6766c4d3/timeline?time=1711071722&selected=76088a58bb99484c82cf9e9065fce1ea",
    "mtd": null,
    "routing": {
      "arch": 9,
      "did": "",
      "event_id": "0a6989a1-af71-4583-a8bc-e766bd2a81d8",
      "event_time": 1711071722721,
      "event_type": "hayabusa_event",
      "ext_ip": "internal",
      "hostname": "ext-hayabusa",
      "iid": "bfac2d1f-5d8c-4115-9df2-633a4f1d062b",
      "int_ip": "",
      "moduleid": 6,
      "oid": "01304add-3354-4cca-b574-b0a54d7bb6f4",
      "plat": 2415919104,
      "sid": "3109b3c7-c5ca-4029-b493-4d4e6766c4d3",
      "tags": [
        "ext:ext-hayabusa",
        "lc:system"
      ],
      "this": "76088a58bb99484c82cf9e9065fce1ea"
    },
    "source": "01304add-3354-4cca-b574-b0a54d7bb6f4.bfac2d1f-5d8c-4115-9df2-633a4f1d062b.3109b3c7-c5ca-4029-b493-4d4e6766c4d3.90000000.9",
    "source_rule": "replay-rule"
  }
}
```

---

# Hostname Resolution

The Endpoint Agent reports its hostname to the LimaCharlie cloud where it shows up as the `hostname` field for the Sensor.

The resolution of that hostname is done in a few different ways:

1. The main local interface is detected by looking for the route to `8.8.8.8`.
2. A `getnameinfo()` with `NI_NAMEREQD` is performed to resolve the FQDN of the box.
3. If the above hostname resolved is valid (no failure, and it is not equal to the static hostname of a few VPN and virtualization providers), this is the hostname we use.
4. If the FQDN could not be resolved, the local hostname of the box is used.

This method allows the endpoint agent to better resolve its hostname in large environments where different regions re-use the same hostname.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

---

# Infrastructure

The Infrastructure Extension allows you to perform infrastructure-as-code (IaC) modifications to your Organization. IaC modifications can be made in the web UI or via the LimaCharlie [CLI tool](https://github.com/refractionPOINT/python-limacharlie/#configs-1). Users can create new organizations from known templates or maintain a common configuration across multiple organizations.

> Scaling Organization Management
>
> If you're an managed service company or need to manage a large number of Organizations, consider LimaCharlie's MSSP setup. You can find more information about this [here](https://github.com/refractionPOINT/mssp-demo).

## Enabling the Infrastructure Extension

To enable the Infrastructure extension, navigate to the [Infrastructure extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-infrastructure) in the marketplace. Select the organization you wish to enable the extension for, and select **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-1.png "image(234).png")

After clicking **Subscribe**, the Infrastructure extension should be available almost immediately.

> Where to start?
>
> IaC can be a powerful tool for rapidly deploying and managing Organizations within LimaCharlie. To help you discover more possibilities, we have provided several example templates/configurations [here](https://github.com/refractionPOINT/templates).

## Using the Infrastructure Extension

Once enabled, you will see an Infrastructure as Code option under the **Organization Settings** within the LimaCharlie web UI. The extension also becomes available via the REST API.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-2.png "image(240).png")

Within the Infrastructure As Code module, you can:

* **Apply a New Config** to an existing organization. Changes are made additively, and are good for merging new configuration parameters into your organization.
* **Edit the Entire Configuration** for an existing organization. This is your current configuration, and can be modified directly in the web UI.
* Perform **Fetch**, **Push**, or **Push-from-file** operations.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-3.png "image(241).png")

## Actions via REST API

The REST interface for the Infrastructure extension mimics the CLI tool. The following REST API actions can be sent to interact with the Infrastructure extension:

```
{
  "params": {
    "sync_artifacts": {
      "type": "bool",
      "desc": "applies to artifacts"
    },
    "is_force": {
      "type": "bool",
      "desc": "make the org an exact copy of the configuration provided."
    },
    "is_dry_run": {
      "type": "bool",
      "desc": "do not apply config, just simulate."
    },
    "sync_integrity": {
      "type": "bool",
      "desc": "applies to integrity"
    },
    "action": {
      "is_required": true,
      "values": [
        "push",
        "fetch"
      ],
      "type": "enum",
      "desc": "action to take."
    },
    "sync_org_values": {
      "type": "bool",
      "desc": "applies to org_values"
    },
    "sync_resources": {
      "type": "bool",
      "desc": "applies to resources"
    },
    "config": {
      "type": "str",
      "desc": "configuration to apply."
    },
    "config_source": {
      "type": "str",
      "desc": "ARL where configs to apply are located."
    },
    "ignore_inaccessible": {
      "desc": "ignore resources which are inaccessible like locked or segmented.",
      "type": "bool"
    },
    "sync_fp": {
      "type": "bool",
      "desc": "applies to fp"
    },
    "sync_exfil": {
      "desc": "applies to exfil",
      "type": "bool"
    },
    "sync_dr": {
      "type": "bool",
      "desc": "applies to dr"
    },
    "sync_outputs": {
      "type": "bool",
      "desc": "applies to outputs"
    },
    "config_root": {
      "type": "str",
      "desc": "file name of the root config within config_source to apply."
    }
  }
}
```

---

# Infrastructure

The Infrastructure Extension allows you to perform infrastructure-as-code (IaC) modifications to your Organization. IaC modifications can be made in the web UI or via the LimaCharlie [CLI tool](https://github.com/refractionPOINT/python-limacharlie/#configs-1). Users can create new organizations from known templates or maintain a common configuration across multiple organizations.

> Scaling Organization Management
>
> If you're an managed service company or need to manage a large number of Organizations, consider LimaCharlie's MSSP setup. You can find more information about this [here](https://github.com/refractionPOINT/mssp-demo).

## Enabling the Infrastructure Extension

To enable the Infrastructure extension, navigate to the [Infrastructure extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-infrastructure) in the marketplace. Select the organization you wish to enable the extension for, and select **Subscribe**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-1.png "image(234).png")

After clicking **Subscribe**, the Infrastructure extension should be available almost immediately.

> Where to start?
>
> IaC can be a powerful tool for rapidly deploying and managing Organizations within LimaCharlie. To help you discover more possibilities, we have provided several example templates/configurations [here](https://github.com/refractionPOINT/templates).

## Using the Infrastructure Extension

Once enabled, you will see an Infrastructure as Code option under the **Organization Settings** within the LimaCharlie web UI. The extension also becomes available via the REST API.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-2.png "image(240).png")

Within the Infrastructure As Code module, you can:

* **Apply a New Config** to an existing organization. Changes are made additively, and are good for merging new configuration parameters into your organization.
* **Edit the Entire Configuration** for an existing organization. This is your current configuration, and can be modified directly in the web UI.
* Perform **Fetch**, **Push**, or **Push-from-file** operations.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/infra-3.png "image(241).png")

## Actions via REST API

The REST interface for the Infrastructure extension mimics the CLI tool. The following REST API actions can be sent to interact with the Infrastructure extension:

```
{
  "params": {
    "sync_artifacts": {
      "type": "bool",
      "desc": "applies to artifacts"
    },
    "is_force": {
      "type": "bool",
      "desc": "make the org an exact copy of the configuration provided."
    },
    "is_dry_run": {
      "type": "bool",
      "desc": "do not apply config, just simulate."
    },
    "sync_integrity": {
      "type": "bool",
      "desc": "applies to integrity"
    },
    "action": {
      "is_required": true,
      "values": [
        "push",
        "fetch"
      ],
      "type": "enum",
      "desc": "action to take."
    },
    "sync_org_values": {
      "type": "bool",
      "desc": "applies to org_values"
    },
    "sync_resources": {
      "type": "bool",
      "desc": "applies to resources"
    },
    "config": {
      "type": "str",
      "desc": "configuration to apply."
    },
    "config_source": {
      "type": "str",
      "desc": "ARL where configs to apply are located."
    },
    "ignore_inaccessible": {
      "desc": "ignore resources which are inaccessible like locked or segmented.",
      "type": "bool"
    },
    "sync_fp": {
      "type": "bool",
      "desc": "applies to fp"
    },
    "sync_exfil": {
      "desc": "applies to exfil",
      "type": "bool"
    },
    "sync_dr": {
      "type": "bool",
      "desc": "applies to dr"
    },
    "sync_outputs": {
      "type": "bool",
      "desc": "applies to outputs"
    },
    "config_root": {
      "type": "str",
      "desc": "file name of the root config within config_source to apply."
    }
  }
}
```

---

# Infrastructure as Code

## Overview

LimaCharlie leverages YAML templates to define and manage the configurations for an Organization, enabling a powerful infrastructure-as-code (IaC) approach. These templates capture all the essential security settings and features of an organization, such as:

* **Enabled Add-ons**: Any additional features or modules that have been activated, from advanced detection mechanisms to specialized data analysis tools.
* **Detection & Response Rules**: The automated rules that determine how the system responds to specific threats or events, ensuring immediate and appropriate action.
* **Event & Artifact Collection**: Specifications for what data is collected, how it's processed, and how long it's retained. This can include system logs, endpoint telemetry, or forensic data from incidents.
* **File Integrity Monitoring**: Rules that detect and alert on unauthorized file changes, helping to identify potential breaches or malicious activity.
* **Output Configurations**: Settings that determine how and where data is sent, such as forwarding alerts to SIEMs, sending notifications to Slack, or exporting logs to external storage.

These YAML-based configurations allow you to capture an organization's entire security setup in a standardized format. This not only provides clarity and visibility but also enables efficient scaling. When deploying a new organization, you can simply apply the existing YAML template to instantly replicate the security environmentno need for manual reconfiguration.

The infrastructure-as-code model promotes consistency, as every organization can be configured identically with minimal effort. Additionally, YAML templates can be version-controlled, allowing you to track changes, roll back updates, and ensure auditability over time. This approach is especially beneficial in multi-tenant environments or service providers managing security across multiple clients, as it facilitates rapid, scalable deployment with confidence that security postures remain aligned across organizations.

Another advantage is the flexibility to customize templates for specific use cases. You can create a base template for common settings and extend it with organization-specific rules or modules, giving you the ability to fine-tune security without sacrificing consistency.

Overall, LimaCharlie's YAML templates enable security teams to treat organizations like modular containers, allowing rapid, repeatable deployment and easy maintenance across a large number of environments while minimizing the risks of human error.

## Example

Here's a basic config for an organization in LimaCharlie:

```yaml
version: 3
resources:
  api:
  - insight
  replicant:
  - infrastructure-service
  - integrity
  - reliable-tasking
  - responder
  - sigma
  - soteria-rules
  - logging
  - yara
integrity:
  linux-key:
    patterns:
    - /home/*/.ssh/*
    tags: []
    platforms:
    - linux
artifact:
  linux-logs:
    is_ignore_cert: false
    is_delete_after: false
    days_retention: 30
    patterns:
    - /var/log/syslog.1
    - /var/log/auth.log.1
    tags: []
    platforms:
    - linux
  windows-logs:
    is_ignore_cert: false
    is_delete_after: false
    days_retention: 30
    patterns:
    - wel://system:*
    - wel://security:*
    - wel://application:*
    tags: []
    platforms:
    - windows
```

Applying this would get an org started with some basics:

* Add-ons that enable incident response (`insight`, `reliable-tasking`, `responder`)
* Managed detection & response rulesets (`sigma`, `soteria-rules`)
* Services that add Sensor capabilities (`integrity`, `logging`, `yara`)
* Some basic configurations to monitor file integrity of `*/.ssh` on Linux and collect syslog, auth logs, and Windows event logs

## Generating IaC Configs

There are many ways to produce an IaC config for reuse across multiple deployments. One option is to use our [IaC Generator](https://iac.limacharlie.io/).

![IaC Generator](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28263%29.png)

## Applying Configs

### Methods

* Via web application in 'Templates' (requires `infrastructure-service`)
* Via REST API requests to `infrastructure-service`
* Via CLI (`limacharlie config fetch` / `limacharlie config push`)

The web application offers two main modes of syncing:

* `Apply`: Add new config and apply changes additively
* `Modify`: Edit existing config and apply changes destructively

Apply mode can be especially useful for applying partial configs from online examples and community solutions. LimaCharlie has a [GitHub repository](https://github.com/refractionPOINT/templates) with some starter config templates.

For finer-grained control of config, or updating configs as part of a CI pipeline, we recommend reading the documentation for [infrastructure service](/v2/docs/ext-infrastructure).

---

# Ingesting Linux Audit Logs

One data source of common interest on Linux systems is the `audit.log` file. By default, this file stores entries from the Audit system, which contains information about logins, privilege escalations, and other account-related events. You can find more information about Audit Log files [here](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security_guide/sec-understanding_audit_log_files).

There are a few techniques to ingest Linux Audit logs into LimaCharlie:

1. Pull the raw logs using Artifacts and/or the File System navigator *(EDR sensors only)*
2. Collect the files using **Artifact Collection.**
3. Stream the raw audit log via a `file` adapter.

We will explore these techniques in this tutorial. Adapters can also be configured as syslog listeners; that will be covered in another tutorial.

## File System Browser

Our Windows, Linux, and macOS EDR sensors offer file system navigation capabilities. If you need a single, ad-hoc collection of the `auth.log`, you can use the File System capability to navigate to `/var/log`, and download `auth.log`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/audit-1.png)

## Artifact Collection

If you don't need to stream Linux Audit log(s), but instead want to maintain a copy of them for posterity, Artifact collection would be your best method. This is an automated collection technique, but won't stream the events to your **Timeline**.

**Step 1:** Within the Navigation Pane, select `Artifact Collection`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/audit-2.png)

**Step 2:** Create a simple artifact collection rule for `/var/log/auth.log`. In this example, we chose a retention period of 30 days; however, you should choose the correct retention period for your use case.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/audit-3.png)

click **Save**

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/audit-4.png)

**Step 3:** Saving the artifact rule will then populate to the appropriate sensor(s), and you should see the `auth.log` in the Artifacts menu, once it is collected by the Sensor.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/audit-5(1).png)

> **Want more logs?**
> 
> Want more than just the most recent `auth.log`? Specify a regular expression to capture all archived copies of the log files. However, be careful on retention and make sure you're not unnecessarily duplicating data!

## File Adapter Ingestion

It is also possible to deploy a LimaCharlie [Adapter](/v2/docs/adapters) pointed to `auth.log` to collect and stream the events in directly. Note that Adapters will create a separate telemetry "stream" - thus, it is recommended to combine file types where possible.

**Step 1:** Create an Installation Key for your adapter and download the appropriate binary.

**Step 2:** On the system(s) to collect logs from, deploy the adapter. We recommend utilizing a configuration file for adapter testing, to allow for tracking of changes. The following is a sample file that will ingest `auth.log` events as basic text.

```
file:
  client_options:
    identity:
      installation_key: <installation_key>
      oid: <oid>
    platform: text
    sensor_seed_key: audit-log-events
  file_path: /var/log/auth.log
  no_follow: false
```

More details on configuration files and adapter usage can be found [here](/v2/docs/adapter-usage).

**Step 3:** Run the adapter, providing the `file` option and the appropriate config file.

`$ ./lc_adapter file /tmp/config.yml`

The adapter should load the config and display options to the terminal.

*Note: This is not a persistent install; utilize your operating system's init/systemctl capabilities to create a persistent adapter*

**Step 4:** Returning to the LimaCharlie web UI, you should start to see events flowing in almost instantaneously.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28115%29.png)

Note that a `text` platform will ingest data as basic text, however you could use formatting options to parse the fields respective to your `auth.log` format.

---

# Ingesting MacOS Unified Logs

You can enable real-time MacOS Unified Logs (MUL) ingestion using the LimaCharlie EDR Sensor.

First, navigate to the Exfil Control section of LimaCharlie and ensure that `MUL` events are enabled for your Windows rules.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28258%29.png)

Next, navigate to the `Artifact Collection` section and set up an artifact collection rule for the MacOS Unified Log(s) of interest. To ingest MUL real-time events in the timeline, use the `mul://[Predicate]` format, where the predicate is a standard [MacOS MUL predicate](https://www.macminivault.com/faq/introduction-to-macos-unified-logs/). For example, to ingest the Safari logs, you'd use the following pattern:

`mul://process == "Safari"`

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28259%29.png)

If you ingest MacOS Unified Logs with a `mul://` pattern, they are streamed in real-time as first-class telemetry alongside the native EDR events, and are included in the flat rate price of the sensor.

After you apply those, you should start seeing your MacOS Unified Logs data coming through for your endpoints within 10 minutes. You can verify this by going into the Timeline view and choosing `MUL` event type.

Also see: [Artifacts](/v1/docs/telemetry-artifacts)

---

# Integrity

The Integrity Extension helps you manage all aspects of file or registry integrity monitoring (FIM and RIM, respectively). This extension automates integrity checks of file system and registry values through pattern-based rules.

## Enabling the Integrity Extension

To enable the Integrity extension, navigate to the [Integrity extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-integrity) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

After clicking **Subscribe**, the Infrastructure extension should be available almost immediately.

## Using the Integrity Extension

Once enabled, you will see an **File/Reg Integrity** option under **Automation** within the LimaCharlie web UI.

Selecting this option allows you to customize **File & Registry Integrity Monitoring** rules.

Selecting **Add Monitoring Rule** will allow you to create a FIM or RIM rule, specifying a platform, Tag(s), and pattern(s).

### Rule Patterns

Patterns are file or registry patterns and support wildcards (\*, ?, +). Windows directory separators (backslash, `"\"`) must be escape with a double-slash `"\\"`.

When a FIM or RIM rule is tripped, you will see a `FIM_HIT` event in the Sensor(s) timeline.

### Example Rule Patterns

#### Windows File Monitoring

| **Monitor a specific directory on all drives** | **Monitor a specific file on a specific drive** |
| --- | --- |
| ?:\\Windows\\System32\\drivers | C:\\Windows\\System32\\specialfile.exe |
| ?:\\inetpub\\wwwroot |  |

#### Windows Registry Monitoring

> All registry monitoring patterns MUST begin with **\\REGISTRY**, followed by the hive and then the path or value to monitor.

| Monitor for changes to system Run and RunOnce | Monitor all users for additions to a user's Run |
| --- | --- |
| \\REGISTRY\\MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\* | \\REGISTRY\\USER\S-\*\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\* |
| \\REGISTRY\\MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\* |  |

#### Linux

| **Monitor for changes to root's authorized\_keys** | **Monitor for changes to all user private ssh directories** |
| --- | --- |
| /root/.ssh/authorized\_keys | /home/\*/.ssh/\* |

#### macOS

| Monitor for changes to user keychains | Monitor for changes to system keychains |
| --- | --- |
| /Users/\*/Library/Keychains/\* | /Library/Keychains |

### Linux Support

FIM is supported on Linux systems, however, support may vary based on Linux distribution and software.

#### Linux with eBPF Support

Linux hosts capable of running with [eBPF](https://ebpf.io/) have file notification and FIM capabilities on par with Windows and macOS.

#### Legacy Support

FIM is partially supported on systems without eBPF. Specified file expressions are actively monitored via `inotify` (as opposed to macOS and Windows, which utilize passive kernel monitoring). Due to [inotify](https://man7.org/linux/man-pages/man7/inotify.7.html) limitations, paths with wildcards are less efficient and only support monitoring up to 20 sub-directories covered by the wildcard. In addition to this, the path expressions should specify a final wildcard of *when all files under a directory need to be monitored. Omitting the final* `*` will result in only the top-level directory being monitoring.

## Actions via REST API

The following REST API actions can be sent to interact with the Integrity extension:

**List Rules**

```
{
  "action": "list_rules"
}
```

**Add Rule**

```
{
  "action": "add_rule",
  "name": "linux-root-ssh-configs",
  "patterns": [
    "/root/.ssh/*"
  ],
  "tags": [
    "vip",
    "workstation"
  ],
  "platforms": [
    "linux"
  ]
}
```

**Remove Rule**

```
{
  "action": "remove_rule",
  "name": "linux-ssh-configs"
}
```

---

# Integrity

The Integrity Extension helps you manage all aspects of file or registry integrity monitoring (FIM and RIM, respectively). This extension automates integrity checks of file system and registry values through pattern-based rules.

## Enabling the Integrity Extension

To enable the Integrity extension, navigate to the [Integrity extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-integrity) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

After clicking **Subscribe**, the Infrastructure extension should be available almost immediately.

## Using the Integrity Extension

Once enabled, you will see an **File/Reg Integrity** option under **Automation** within the LimaCharlie web UI.

Selecting this option allows you to customize **File & Registry Integrity Monitoring** rules.

Selecting **Add Monitoring Rule** will allow you to create a FIM or RIM rule, specifying a platform, Tag(s), and pattern(s).

### Rule Patterns

Patterns are file or registry patterns and support wildcards (\*, ?, +). Windows directory separators (backslash, `"\"`) must be escape with a double-slash `"\\"`.

When a FIM or RIM rule is tripped, you will see a `FIM_HIT` event in the Sensor(s) timeline.

### Example Rule Patterns

#### Windows File Monitoring

| **Monitor a specific directory on all drives** | **Monitor a specific file on a specific drive** |
| --- | --- |
| ?:\\Windows\\System32\\drivers | C:\\Windows\\System32\\specialfile.exe |
| ?:\\inetpub\\wwwroot |  |

#### Windows Registry Monitoring

> All registry monitoring patterns MUST begin with **\\REGISTRY**, followed by the hive and then the path or value to monitor.

| Monitor for changes to system Run and RunOnce | Monitor all users for additions to a user's Run |
| --- | --- |
| \\REGISTRY\\MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\* | \\REGISTRY\\USER\S-\*\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\* |
| \\REGISTRY\\MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\RunOnce\* |  |

#### Linux

| **Monitor for changes to root's authorized\_keys** | **Monitor for changes to all user private ssh directories** |
| --- | --- |
| /root/.ssh/authorized\_keys | /home/\*/.ssh/\* |

#### macOS

| Monitor for changes to user keychains | Monitor for changes to system keychains |
| --- | --- |
| /Users/\*/Library/Keychains/\* | /Library/Keychains |

### Linux Support

FIM is supported on Linux systems, however, support may vary based on Linux distribution and software.

#### Linux with eBPF Support

Linux hosts capable of running with [eBPF](https://ebpf.io/) have file notification and FIM capabilities on par with Windows and macOS.

#### Legacy Support

FIM is partially supported on systems without eBPF. Specified file expressions are actively monitored via `inotify` (as opposed to macOS and Windows, which utilize passive kernel monitoring). Due to [inotify](https://man7.org/linux/man-pages/man7/inotify.7.html) limitations, paths with wildcards are less efficient and only support monitoring up to 20 sub-directories covered by the wildcard. In addition to this, the path expressions should specify a final wildcard of * when all files under a directory need to be monitored. Omitting the final `*` will result in only the top-level directory being monitoring.

## Actions via REST API

The following REST API actions can be sent to interact with the Integrity extension:

**List Rules**

```
{
  "action": "list_rules"
}
```

**Add Rule**

```
{
  "action": "add_rule",
  "name": "linux-root-ssh-configs",
  "patterns": [
    "/root/.ssh/*"
  ],
  "tags": [
    "vip",
    "workstation"
  ],
  "platforms": [
    "linux"
  ]
}
```

**Remove Rule**

```
{
  "action": "remove_rule",
  "name": "linux-ssh-configs"
}
```

---

# ChromeOS Support

LimaCharlie provides support for ChromeOS endpoints through its sensor deployment capabilities.

## Overview

The LimaCharlie sensor can be deployed on ChromeOS devices to provide security monitoring and response capabilities for Chrome OS environments.

## Deployment

ChromeOS sensors can be installed and managed through the standard LimaCharlie sensor deployment workflow, allowing organizations to extend their security coverage to ChromeOS endpoints.

## Features

When deployed on ChromeOS, the LimaCharlie sensor provides:

- Real-time security event collection
- Detection and Response (D&R) rule enforcement
- Artifact collection capabilities
- Integration with the broader LimaCharlie security platform

## Use Cases

ChromeOS support is particularly valuable for:

- Organizations with BYOD (Bring Your Own Device) programs including Chromebooks
- Educational institutions deploying Chromebooks at scale
- Enterprises with ChromeOS devices in their fleet
- Security teams requiring unified visibility across diverse endpoint types

---

# Security Monitoring for DevOps

DevOps practices emphasize speed, automation, and continuous delivery, but security often becomes an afterthought. LimaCharlie provides security monitoring capabilities specifically designed to integrate with DevOps workflows, enabling you to maintain both velocity and security.

## DevOps Security Challenges

Traditional security tools aren't built for DevOps environments:

- **Speed vs. Security**: Manual security reviews slow down deployment pipelines
- **Ephemeral Infrastructure**: Containers and cloud instances appear and disappear rapidly
- **Distributed Systems**: Microservices architectures create complex attack surfaces
- **Automation Gaps**: Security processes often remain manual while everything else is automated
- **Visibility**: Traditional monitoring struggles with dynamic, containerized environments

## LimaCharlie's DevOps-Friendly Approach

### API-First Architecture

Everything in LimaCharlie is accessible via API, making it easy to integrate security into CI/CD pipelines:

```bash
# Example: Check sensor health in deployment script
curl -H "Authorization: Bearer ${API_KEY}" \
  https://api.limacharlie.io/v1/orgs/${ORG}/sensors
```

### Infrastructure as Code Support

Security configurations can be version-controlled and deployed alongside your infrastructure:

```yaml
# Example: Detection rule as code
detection:
  target: events
  event: NEW_PROCESS
  op: and
  rules:
    - op: is
      path: event/FILE_PATH
      value: /usr/bin/cryptominer
  response:
    - action: report
      name: cryptocurrency_miner_detected
```

### Container and Kubernetes Monitoring

LimaCharlie sensors work seamlessly in containerized environments:

- Monitor containers without requiring container modifications
- Track container lifecycle events
- Detect malicious activity within containers
- Integrate with Kubernetes for automated deployment

### Automated Response

Create automated security responses that don't require manual intervention:

```yaml
# Example: Auto-isolate compromised hosts
response:
  - action: task
    command: isolated
    investigation: true
  - action: report
    name: host_isolated_automatically
```

## Integration Patterns

### CI/CD Pipeline Integration

**Pre-Deployment Security Checks**:

```bash
# In your CI pipeline
- name: Security Scan
  run: |
    # Deploy to staging with LimaCharlie monitoring
    ./deploy_staging.sh
    
    # Run security baseline check
    ./check_security_baseline.sh
    
    # Promote to production only if checks pass
    if [ $? -eq 0 ]; then
      ./deploy_production.sh
    fi
```

**Post-Deployment Monitoring**:

```python
# Example: Verify deployment security posture
import requests

def check_deployment_security(deployment_tag):
    # Query LimaCharlie for alerts related to new deployment
    response = requests.get(
        f'https://api.limacharlie.io/v1/orgs/{org_id}/detections',
        headers={'Authorization': f'Bearer {api_key}'},
        params={'tag': deployment_tag, 'limit': 100}
    )
    
    alerts = response.json()
    if len(alerts) > threshold:
        # Trigger rollback
        trigger_rollback(deployment_tag)
```

### Infrastructure as Code

**Terraform Integration**:

```hcl
# Example: Deploy LimaCharlie sensors with Terraform
resource "limacharlie_sensor" "web_servers" {
  for_each = aws_instance.web_servers
  
  hostname = each.value.private_dns
  tags     = ["web", "production", each.value.availability_zone]
  
  installation_key = var.lc_installation_key
}
```

### Monitoring as Code

Version control your detection rules and responses:

```yaml
# detections/suspicious_docker_activity.yaml
detection:
  target: events
  event: NEW_PROCESS
  op: and
  rules:
    - op: contains
      path: event/COMMAND_LINE
      value: docker
    - op: is
      path: event/USER
      value: root
    - op: contains
      path: event/COMMAND_LINE
      value: privileged
  response:
    - action: report
      name: privileged_docker_container
      metadata:
        severity: medium
        category: container_security
```

## DevOps Security Best Practices

### 1. Security in the Deployment Pipeline

Integrate security checks at every stage:

- **Build**: Scan container images for vulnerabilities
- **Test**: Run security tests in staging environments with LimaCharlie monitoring
- **Deploy**: Automatically enable monitoring for new instances
- **Monitor**: Continuously track runtime behavior

### 2. Ephemeral Infrastructure Monitoring

Handle short-lived resources effectively:

```yaml
# Auto-tag sensors based on cloud metadata
sensor_config:
  auto_tags:
    - cloud_provider: aws
    - instance_type: {{ ec2_instance_type }}
    - environment: {{ deployment_env }}
    - deployment_id: {{ deployment_id }}
```

### 3. Automated Incident Response

Reduce MTTR with automated responses:

```yaml
# Example: Auto-response for critical alerts
response:
  - action: isolate
    condition: severity >= critical
  - action: snapshot
    command: dump_memory
  - action: notify
    webhook: "{{ pagerduty_webhook }}"
```

### 4. Compliance as Code

Automate compliance checking:

```python
# Example: Continuous compliance validation
def validate_compliance():
    # Check all production sensors have required configurations
    sensors = get_all_sensors(tags=['production'])
    
    for sensor in sensors:
        if not sensor.has_required_rules():
            alert_compliance_violation(sensor)
        if not sensor.has_required_logging():
            alert_compliance_violation(sensor)
```

## Metrics and Reporting

Track security metrics in your DevOps dashboards:

```python
# Example: Security metrics for DevOps dashboard
metrics = {
    'deployment_security_score': calculate_deployment_score(),
    'time_to_detect': calculate_ttd(),
    'time_to_respond': calculate_ttr(),
    'security_coverage': calculate_coverage_percentage(),
    'critical_alerts_by_service': get_alerts_by_service()
}

# Push to monitoring system
push_to_datadog(metrics)
```

## Example: Complete DevOps Security Workflow

```bash
#!/bin/bash
# deploy_with_security.sh

set -e

DEPLOYMENT_ID=$(uuidgen)
ENVIRONMENT="production"

# 1. Deploy infrastructure
echo "Deploying infrastructure..."
terraform apply -auto-approve

# 2. Deploy LimaCharlie sensors
echo "Deploying security monitoring..."
./deploy_lc_sensors.sh \
  --tag "deployment:${DEPLOYMENT_ID}" \
  --tag "env:${ENVIRONMENT}"

# 3. Wait for sensors to come online
echo "Waiting for sensors..."
./wait_for_sensors.sh --tag "deployment:${DEPLOYMENT_ID}"

# 4. Run security baseline
echo "Running security baseline checks..."
./security_baseline.sh --deployment "${DEPLOYMENT_ID}"

# 5. Deploy application
echo "Deploying application..."
./deploy_app.sh

# 6. Monitor for anomalies
echo "Monitoring deployment for 5 minutes..."
./monitor_deployment.sh \
  --duration 300 \
  --tag "deployment:${DEPLOYMENT_ID}" \
  --alert-threshold 5

# 7. If monitoring passes, complete deployment
echo "Deployment complete and verified secure"
```

## Key Benefits for DevOps Teams

1. **No Slowdown**: Security checks run in parallel with deployments
2. **Automated**: Security monitoring deploys automatically with infrastructure
3. **Observable**: Security telemetry flows into existing monitoring tools
4. **Repeatable**: Security configurations are version-controlled and tested
5. **Scalable**: Monitoring scales with your infrastructure automatically

## Getting Started

1. **Install the LimaCharlie CLI**: `pip install limacharlie`
2. **Set up API credentials**: Store your API key securely (e.g., in CI/CD secrets)
3. **Create detection rules as code**: Version control your security policies
4. **Integrate with CI/CD**: Add security checks to your pipeline
5. **Automate sensor deployment**: Use infrastructure-as-code tools
6. **Monitor and iterate**: Track metrics and continuously improve

LimaCharlie enables DevOps teams to maintain velocity while building security into every stage of the development and deployment lifecycle.

---

# File Integrity Monitoring (FIM) Deployments

File Integrity Monitoring (FIM) allows you to track changes to critical files and registry keys on your endpoints. This capability is essential for detecting unauthorized modifications to system files, configuration files, and other sensitive resources.

## Overview

LimaCharlie's FIM implementation monitors specified files and registry locations for changes, reporting events when modifications occur. This enables you to:

- Detect unauthorized changes to critical system files
- Monitor configuration file modifications
- Track registry key changes (Windows)
- Maintain audit trails of file system changes
- Create detection and response rules based on file modifications

## Configuration

FIM is configured through D&R (Detection & Response) rules that specify which files or registry keys to monitor and what actions to trigger when changes are detected.

### Basic FIM Rule Structure

```yaml
detect:
  event: FIM_*
  op: and
  rules:
    - path: /path/to/monitor
      
respond:
  - action: report
    name: file_modification_detected
```

## Monitoring Files

To monitor specific files or directories:

```yaml
detect:
  event: FIM_MODIFIED
  op: and
  rules:
    - path: /etc/passwd
      
respond:
  - action: report
    name: critical_file_modified
```

### Recursive Directory Monitoring

Monitor all files within a directory recursively:

```yaml
detect:
  event: FIM_MODIFIED
  op: and
  rules:
    - path: /etc/
      is_recursive: true
      
respond:
  - action: report
    name: etc_directory_change
```

## Monitoring Registry Keys (Windows)

For Windows systems, monitor registry keys:

```yaml
detect:
  event: FIM_MODIFIED
  op: and
  rules:
    - path: HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Run
      
respond:
  - action: report
    name: autorun_registry_modified
```

## FIM Events

LimaCharlie generates the following FIM events:

- **FIM_MODIFIED**: File or registry key content changed
- **FIM_CREATED**: New file or registry key created
- **FIM_DELETED**: File or registry key deleted

## Best Practices

1. **Start Small**: Begin monitoring critical system files before expanding to broader directories
2. **Use Filters**: Apply appropriate filters to reduce noise from expected changes
3. **Baseline Normal Activity**: Understand normal file modification patterns before alerting
4. **Prioritize Critical Assets**: Focus on files that impact security and system integrity
5. **Combine with Other Rules**: Use FIM events in combination with other detection rules for better context

## Example Use Cases

### Monitor SSH Configuration

```yaml
detect:
  event: FIM_MODIFIED
  op: and
  rules:
    - path: /etc/ssh/sshd_config
      
respond:
  - action: report
    name: ssh_config_modified
  - action: task
    command: report_ssh_config_change
```

### Monitor Windows Startup Programs

```yaml
detect:
  event: FIM_CREATED
  op: and
  rules:
    - path: HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Run
      
respond:
  - action: report
    name: new_autorun_program
  - action: task
    command: investigate_startup_item
```

### Monitor Web Server Configuration

```yaml
detect:
  event: FIM_MODIFIED
  op: and
  rules:
    - path: /etc/nginx/
      is_recursive: true
      
respond:
  - action: report
    name: nginx_config_changed
```

---

# Use Cases

LimaCharlie provides a flexible security infrastructure platform that supports various security operations and detection engineering workflows. Below are common use cases and implementation patterns.

## Detection Engineering

Build and deploy custom detection rules across your infrastructure:

- Create detection & response (D&R) rules using LimaCharlie's rule engine
- Test detections in real-time against live telemetry
- Deploy rules across sensor groups or specific endpoints
- Iterate quickly with immediate feedback

## Threat Hunting

Leverage LimaCharlie's telemetry and querying capabilities:

- Search historical events using structured queries
- Pivot across related events and artifacts
- Export findings for further analysis
- Create detections from hunting discoveries

## Incident Response

Respond to security events with integrated response capabilities:

- Execute remote commands on endpoints
- Collect forensic artifacts (memory dumps, files, logs)
- Isolate compromised systems from the network
- Coordinate response actions through automation

## Security Operations Platform

Build a complete security operations infrastructure:

- Centralize telemetry from multiple sources
- Integrate with external tools via APIs and webhooks
- Create custom dashboards and reporting
- Automate security workflows with outputs and integrations

## Compliance and Auditing

Support compliance requirements through visibility and evidence collection:

- Maintain audit trails of security events
- Generate compliance reports from telemetry
- Monitor configuration changes
- Document security controls and coverage

## Security Research and Development

Develop and test security tools and detections:

- Access raw telemetry streams for analysis
- Prototype detection logic rapidly
- Test against diverse operating systems
- Validate detection efficacy before production deployment

## Managed Security Services

Deliver security services to multiple customers:

- Multi-tenant architecture with organization isolation
- Centralized management across customer environments
- Scalable deployment and configuration
- API-driven automation for service delivery

---

# Security Service Providers (MSSP, MSP, MDR)

LimaCharlie provides a comprehensive platform for Security Service Providers including MSSPs (Managed Security Service Providers), MSPs (Managed Service Providers), and MDR (Managed Detection and Response) providers.

## Multi-Tenancy Support

LimaCharlie is built from the ground up to support multi-tenant architectures, allowing service providers to manage multiple customer organizations from a single platform.

### Key Features for Service Providers:

- **Organization Management**: Create and manage multiple customer organizations from a central interface
- **Role-Based Access Control (RBAC)**: Granular permissions to control access across organizations
- **Centralized Billing**: Single billing interface for all customer organizations
- **White-Label Capabilities**: Customize the platform with your own branding
- **API-First Design**: Automate organization provisioning and management

## Organization Hierarchy

Service providers can create a hierarchical structure:

- **Parent Organization**: Your service provider organization
- **Child Organizations**: Individual customer organizations

This hierarchy enables:
- Centralized policy management
- Bulk configuration deployment
- Aggregated reporting and analytics
- Cross-tenant threat intelligence sharing

## Detection & Response Capabilities

### Detection Engineering
- **Detection & Response Rules**: Create custom detection rules using LimaCharlie's D&R engine
- **Rule Libraries**: Share detection rules across customer organizations
- **YARA Rules**: Deploy file and memory scanning rules
- **Sigma Rules**: Import industry-standard Sigma rules

### Response Actions
- **Automated Response**: Configure automated responses to detections
- **Isolation**: Isolate compromised endpoints
- **Process Control**: Kill processes, delete files, or quarantine threats
- **Custom Actions**: Execute custom scripts and commands

## Service Delivery Models

### MSSP (Managed Security Service Provider)
- 24/7 security monitoring
- Threat hunting and incident response
- Security analytics and reporting
- Compliance monitoring

### MSP (Managed Service Provider)
- Endpoint management
- Security posture monitoring
- Patch management integration
- IT asset visibility

### MDR (Managed Detection and Response)
- Continuous monitoring
- Threat detection and investigation
- Incident response
- Threat intelligence integration

## Automation & Integration

### API Access
- Full REST API for all platform capabilities
- Automate customer onboarding
- Integrate with PSA/RMM tools
- Custom dashboards and reporting

### Webhooks
- Real-time event notifications
- Integration with SIEM platforms
- Ticketing system integration
- Custom workflow triggers

### Add-ons and Extensions
- **VirusTotal Integration**: Automated file reputation checks
- **Threat Intelligence Feeds**: Subscribe to threat intel sources
- **Cloud Storage**: Long-term telemetry retention
- **Advanced Analytics**: Enhanced search and investigation capabilities

## Deployment Options

### Agent Deployment
- Windows, macOS, Linux support
- Docker container monitoring
- Cloud workload protection (AWS, Azure, GCP)
- Kubernetes monitoring

### Deployment Methods
- MSI installers
- Command-line deployment
- Group Policy (GPO)
- Mobile Device Management (MDM)
- Cloud deployment templates

## Pricing for Service Providers

LimaCharlie offers flexible pricing models designed for service providers:

- **Per-Endpoint Pricing**: Predictable costs based on protected endpoints
- **Volume Discounts**: Tiered pricing for larger deployments
- **Usage-Based Options**: Pay for actual data ingestion and storage
- **Custom Enterprise Agreements**: Tailored pricing for large-scale deployments

## Getting Started

1. **Sign Up**: Create a service provider account
2. **Configure Parent Org**: Set up your master organization
3. **Create Child Orgs**: Provision customer organizations
4. **Deploy Agents**: Roll out sensors to customer endpoints
5. **Configure Detection**: Set up detection rules and response actions
6. **Monitor & Respond**: Begin monitoring and responding to threats

## Best Practices

### Organization Structure
- Use consistent naming conventions for customer organizations
- Implement standardized tagging for assets
- Create organization templates for faster onboarding

### Security Operations
- Develop playbooks for common incident types
- Maintain a rule library for consistent detection
- Regularly review and tune detection rules
- Implement tiered escalation procedures

### Scalability
- Automate customer onboarding
- Use templates for common configurations
- Leverage APIs for bulk operations
- Monitor platform usage and optimize costs

## Support Resources

- **Documentation**: Comprehensive technical documentation
- **API Reference**: Complete API documentation
- **Community Slack**: Connect with other service providers
- **Professional Services**: Expert assistance for complex deployments
- **Training Programs**: Security operations training

## Additional Resources

For more information on building your security service practice with LimaCharlie:

- Contact sales for service provider partnerships
- Review the API documentation for automation capabilities
- Join the LimaCharlie community for best practices
- Explore case studies from other service providers

---

# Security Service Providers (MSSP, MSP, MDR)

LimaCharlie is designed to support security service providers, including MSSPs (Managed Security Service Providers), MSPs (Managed Service Providers), and MDR (Managed Detection and Response) providers. The platform provides comprehensive multi-tenancy capabilities and automation features that enable service providers to efficiently manage security operations for multiple clients.

## Multi-Tenancy Architecture

LimaCharlie's architecture is built from the ground up to support multi-tenancy, allowing service providers to manage multiple customer organizations from a single interface while maintaining strict data isolation and security boundaries.

### Organization Management

Service providers can create and manage multiple organizations (tenants), each representing a different customer or business unit. Each organization maintains:

- Separate data storage and processing
- Independent billing and resource tracking
- Isolated security policies and configurations
- Dedicated API keys and access controls

### Hierarchical Access Control

The platform supports hierarchical access control, enabling service providers to:

- Grant different permission levels to different team members
- Provide customers with limited access to their own data
- Maintain administrative control across all managed organizations
- Delegate specific responsibilities without compromising security

## Automation and Orchestration

LimaCharlie provides extensive automation capabilities that enable service providers to scale their operations efficiently.

### Detection & Response Rules

Create and deploy detection and response rules across multiple customer organizations:

```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: contains
      path: event/FILE_PATH
      value: "powershell.exe"
respond:
  - action: report
    name: suspicious_powershell
```

### Infrastructure as Code

Manage your security infrastructure as code using LimaCharlie's REST API and CLI tools. This enables:

- Version-controlled security configurations
- Automated deployment across customer environments
- Consistent policy enforcement
- Rapid onboarding of new customers

## Resource Management

### Resource Limits and Quotas

Set resource limits and quotas for each customer organization to ensure fair usage and predictable costs:

- Event ingestion limits
- Storage quotas
- API rate limits
- Retention policies

### Billing and Cost Tracking

Track resource consumption and costs on a per-organization basis:

- Detailed usage reports
- Cost attribution to specific customers
- Flexible billing models
- Budget alerts and notifications

## Customer Portal and Self-Service

Enable customers to access their own security data and insights through a dedicated portal:

- Read-only or limited-write access to their organization
- Custom dashboards and reports
- Alert notifications and incident management
- Service request workflows

## API and Integration

LimaCharlie's comprehensive REST API enables deep integration with service provider tools and workflows:

- Automated customer provisioning
- Integration with ticketing systems
- SIEM and SOAR connectivity
- Custom reporting and analytics

Example API call to create a new organization:

```bash
curl -X POST "https://api.limacharlie.io/v1/orgs" \
  -H "Authorization: bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "customer-org",
    "template": "base-security-template"
  }'
```

## Best Practices for Service Providers

### Standardization

- Develop standard security baselines for different customer profiles
- Use templates for consistent deployment
- Maintain a library of reusable detection and response rules

### Segregation

- Maintain strict data segregation between customers
- Use dedicated API keys for each customer organization
- Implement role-based access control (RBAC) for your team

### Automation

- Automate repetitive tasks wherever possible
- Use Infrastructure as Code for configuration management
- Implement automated testing for security rules

### Monitoring and Reporting

- Set up centralized monitoring across all customer organizations
- Create standardized reports for different stakeholder levels
- Implement proactive alerting for critical issues

### Scalability

- Design processes that scale as you add customers
- Use automation to reduce per-customer operational overhead
- Leverage LimaCharlie's cloud-native architecture for elastic scaling

## Support and Resources

LimaCharlie provides dedicated support for service providers:

- Technical account management
- Architecture consulting
- Custom training and onboarding
- Priority support channels

For more information on how LimaCharlie can support your service provider business, contact the sales team or consult the documentation.

---

# Security Service Providers (MSSP, MSP, MDR)

LimaCharlie provides a comprehensive platform for security service providers to deliver managed security services at scale. The platform is designed to support MSSPs (Managed Security Service Providers), MSPs (Managed Service Providers), and MDR (Managed Detection and Response) providers.

## Key Capabilities for Service Providers

### Multi-Tenancy

LimaCharlie's architecture is built for multi-tenancy from the ground up, allowing service providers to:

- Manage multiple customer organizations from a single pane of glass
- Maintain complete isolation between customer environments
- Apply consistent security policies across organizations
- Scale operations efficiently as customer base grows

### White-Labeling

Service providers can brand the platform as their own:

- Custom branding and logos
- Custom domain names
- Branded reporting and dashboards
- Client-facing interfaces with provider branding

### Billing and Licensing

Flexible billing options designed for service providers:

- Wholesale pricing for resellers
- Per-seat or consumption-based billing
- Ability to mark up services
- Centralized billing management
- Usage tracking and reporting per organization

### Automation and Orchestration

Build repeatable processes for onboarding and management:

- Automated organization provisioning
- Template-based configurations
- Bulk operations across organizations
- Integration with existing PSA/RMM tools
- API-first architecture for custom workflows

### Monitoring and Alerting

Centralized visibility across all customer environments:

- Aggregated alerting across organizations
- Custom alert routing per customer
- SLA monitoring and reporting
- Real-time threat intelligence
- Cross-tenant threat hunting capabilities

## Getting Started as a Service Provider

1. **Partner Program**: Contact LimaCharlie to discuss partnership opportunities and pricing
2. **Parent Organization**: Set up your master/parent organization
3. **Customer Organizations**: Create child organizations for each customer
4. **Templates**: Define baseline security policies and detection rules
5. **Integration**: Connect to your existing tools and workflows
6. **Onboarding**: Deploy sensors to customer endpoints

## Best Practices

### Organization Structure

- Use a parent organization for your company
- Create child organizations for each customer
- Use naming conventions for easy identification
- Implement consistent tagging strategies

### Access Control

- Use role-based access control (RBAC)
- Limit access to customer organizations
- Implement least-privilege principles
- Audit access regularly

### Standardization

- Create reusable detection rules
- Maintain configuration templates
- Document standard operating procedures
- Version control your configurations

### Automation

- Automate organization provisioning
- Use APIs for bulk operations
- Integrate with ticketing systems
- Automate reporting and compliance checks

## Support for Service Providers

LimaCharlie provides dedicated support for service providers:

- Partner account management
- Technical onboarding assistance
- Access to partner resources
- Co-marketing opportunities
- Training and certification programs

---

# Enterprise SOC

LimaCharlie provides enterprise-grade Security Operations Center (SOC) capabilities designed to scale with your organization's security needs. This documentation covers the key features and capabilities available for enterprise deployments.

---

# SecOps Development

**LimaCharlie's SecOps capabilities allow security teams to build, deploy, and operate custom security tooling at scale.**

## Overview

SecOps Development in LimaCharlie enables you to create custom detection and response (D&R) rules, automate security operations, integrate with third-party tools, and build tailored security workflows.

## Core Components

### Detection & Response (D&R) Rules

D&R rules are the foundation of SecOps development in LimaCharlie. They allow you to:

- Define custom detection logic using event matching
- Trigger automated responses when threats are detected
- Chain multiple actions together for complex workflows
- Test rules in isolation before deploying to production

**Basic D&R Rule Structure:**

```yaml
detect:
  event: NEW_PROCESS
  op: starts with
  path: /tmp/
  
respond:
  - action: report
    name: suspicious_process_execution
  - action: task
    command: deny_tree
    investigation: true
```

### Event Types

LimaCharlie sensors generate dozens of event types that can be used in detection logic:

- Process events (NEW_PROCESS, TERMINATE_PROCESS)
- Network events (NEW_CONNECTION, DNS_REQUEST)
- File events (FILE_CREATE, FILE_MODIFIED, FILE_DELETE)
- Registry events (REGISTRY_CREATE, REGISTRY_WRITE) [Windows]
- User events (USER_LOGIN, USER_LOGOFF)
- And many more...

### Response Actions

When detections trigger, you can execute various response actions:

- **report**: Generate a detection report
- **task**: Execute sensor commands (isolate, deny_tree, etc.)
- **action**: Trigger automation or integrations
- **add_tag**: Apply tags to endpoints
- **service_request**: Call external webhooks or services

### Automation & Integrations

LimaCharlie supports multiple automation methods:

1. **Outputs**: Stream detections to external systems (SIEM, SOAR, ticketing)
2. **Actions**: HTTP callbacks and webhooks
3. **Service Requests**: Call external APIs from D&R rules
4. **FP&A (False Positive & Absence)**: Automated alert enrichment and filtering

## Development Workflow

### 1. Rule Development

- Start with the web interface rule builder
- Test against historical data using replay
- Validate logic with known true/false positives
- Export rules as YAML for version control

### 2. Testing

```bash
# Test a rule against historical events
limacharlie rules test --rule-file my_rule.yaml --org my-org
```

### 3. Deployment

- Deploy individually via UI or CLI
- Bulk deploy using infrastructure-as-code
- Use staging organizations for pre-production testing
- Apply rules selectively using tags and filters

### 4. Monitoring

- Review detection rates and false positives
- Monitor performance impact on endpoints
- Adjust thresholds and tune logic
- Archive or deprecate ineffective rules

## Best Practices

### Rule Development

1. **Be specific**: Narrow detection scope to reduce false positives
2. **Use filters**: Apply rules only to relevant endpoints using tags
3. **Test thoroughly**: Validate against known good and bad activity
4. **Document intent**: Add clear descriptions explaining rule purpose
5. **Version control**: Store rules in git for change tracking

### Performance Optimization

- Avoid overly broad event matching
- Use efficient operators (equals over contains when possible)
- Limit expensive operations (hashing, external lookups)
- Monitor sensor performance metrics

### Security Considerations

- Follow least privilege for API keys and permissions
- Audit rule changes and access logs
- Encrypt sensitive data in transit and at rest
- Test destructive actions in non-production first

## Advanced Topics

### Custom Event Generation

Create synthetic events for tracking custom metrics:

```yaml
respond:
  - action: report
    name: custom_metric
    metadata:
      metric_name: login_count
      value: "{{ .event.user }}"
```

### Multi-Stage Detection

Chain multiple events together for behavioral detection:

```yaml
detect:
  op: and
  rules:
    - event: NEW_PROCESS
      path: cmd.exe
    - event: NEW_CONNECTION
      after: 5s
      
respond:
  - action: report
    name: suspicious_command_with_network
```

### Integration Patterns

Common integration scenarios:

- **Enrichment**: Query threat intelligence APIs during detection
- **Ticketing**: Auto-create tickets in Jira/ServiceNow
- **SOAR**: Trigger playbooks in Cortex XSOAR, Tines, etc.
- **SIEM**: Forward events to Splunk, Sentinel, Chronicle

## Resources

- [D&R Rule Syntax Reference](/docs/detection-and-response)
- [Event Type Documentation](/docs/events)
- [Sensor Commands Reference](/docs/sensor-commands)
- [API Documentation](/docs/api)
- [Community Rules Repository](https://github.com/refractionPOINT/rules)

## Support

For questions or assistance with SecOps development:

- Community Slack: [limacharlie.slack.com](https://limacharlie.slack.com)
- GitHub Issues: [github.com/refractionPOINT/lc-public](https://github.com/refractionPOINT/lc-public)
- Email: support@limacharlie.io

---

# WEL Monitoring

Windows Event Logs (WEL) are a critical source of security and operational data on Windows systems. LimaCharlie provides comprehensive WEL monitoring capabilities to help you detect threats, troubleshoot issues, and maintain compliance.

## Overview

WEL monitoring in LimaCharlie allows you to:

- Collect and analyze Windows Event Logs in real-time
- Create detection rules based on event patterns
- Forward events to external systems
- Investigate security incidents using event data

## Enabling WEL Collection

To enable WEL collection on your Windows sensors:

1. Navigate to your organization in the LimaCharlie web interface
2. Go to **Sensors** and select the target sensor or sensor group
3. Enable WEL collection in the sensor configuration
4. Specify which event log channels to monitor

## Event Log Channels

Common Windows Event Log channels include:

- **Security**: Authentication, authorization, and security events
- **System**: System component events, driver issues
- **Application**: Application-specific events
- **Windows PowerShell**: PowerShell execution events
- **Microsoft-Windows-Sysmon/Operational**: Sysmon events (if installed)

## Creating Detection Rules

You can create D&R (Detection & Response) rules based on WEL events. Rules can match on:

- Event IDs
- Event sources
- Event data fields
- Patterns across multiple events

## Best Practices

- Focus on high-value security events (e.g., Event ID 4624, 4625, 4688)
- Use filtering to reduce noise and storage costs
- Combine WEL monitoring with other telemetry sources
- Regularly review and tune your detection rules

## Example Use Cases

- **Failed Login Detection**: Monitor Event ID 4625 for failed authentication attempts
- **Privilege Escalation**: Track Event ID 4672 for special privilege assignments
- **Process Creation**: Use Event ID 4688 to track new process creation
- **Service Installation**: Monitor Event ID 7045 for new service installations

## Related Resources

- [Detection & Response Rules](/docs/en/detection-and-response-rules)
- [Windows Sensor Configuration](/docs/en/windows-sensor-configuration)
- [Event Forwarding](/docs/en/event-forwarding)

---

# SOAR / Automation

LimaCharlie provides powerful automation capabilities through its Security Orchestration, Automation, and Response (SOAR) features. These capabilities enable you to automate detection, response, and remediation workflows.

## Automation Capabilities

LimaCharlie's automation features allow you to:

- Create detection and response rules
- Automate incident response workflows
- Orchestrate actions across multiple sensors
- Integrate with external services and tools
- Build custom automation logic

## Key Components

### Detection & Response Rules

Create rules that automatically detect threats and take action when specific conditions are met. Rules can:

- Monitor telemetry in real-time
- Match on specific events or patterns
- Trigger automated responses
- Send notifications and alerts

### Outputs

Connect LimaCharlie to external services to send alerts, logs, and data. Outputs support integration with:

- SIEM platforms
- Ticketing systems
- Messaging platforms
- Custom webhooks

### Actions

Execute automated responses when threats are detected:

- Isolate endpoints
- Kill processes
- Quarantine files
- Run custom commands
- Collect additional forensic data

## Getting Started

To begin automating with LimaCharlie:

1. Define your use cases and automation requirements
2. Create detection rules for your threat scenarios
3. Configure outputs for alerting and integration
4. Test your automation workflows
5. Monitor and refine your rules over time

## Resources

For detailed information on implementing automation:

- Review the Detection & Response documentation
- Explore the API documentation for programmatic automation
- Check out example rules and templates
- Join the community for best practices and examples

---

# AWS Documentation

## Articles

### Soteria AWS Rules
**Last Updated: 10 Oct 2025**

[Link to Soteria AWS Rules documentation](/docs/en/soteria-aws-rules)

---

### Amazon S3
**Last Updated: 07 Oct 2025**

[Link to Amazon S3 documentation](/docs/en/outputs-destinations-amazon-s3)

---

### SQS
**Last Updated: 07 Aug 2025**

[Link to SQS adapter documentation](/docs/en/adapter-types-sqs)

---

### S3
**Last Updated: 07 Aug 2025**

[Link to S3 adapter documentation](/docs/en/adapter-types-s3)

---

### AWS GuardDuty
**Last Updated: 06 Jun 2025**

[Link to AWS GuardDuty adapter documentation](/docs/en/adapter-types-aws-guardduty)

---

### AWS CloudTrail
**Last Updated: 01 Nov 2024**

[Link to AWS CloudTrail adapter documentation](/docs/en/adapter-types-aws-cloudtrail)

---

### AWS
**Last Updated: 05 Oct 2024**

[Link to AWS CLI extension documentation](/docs/en/ext-cloud-cli-aws)

---

# Cloud Security

LimaCharlie provides comprehensive cloud security capabilities to help you monitor, detect, and respond to threats across your cloud infrastructure.

## Overview

Cloud security in LimaCharlie enables you to:

- Monitor cloud infrastructure and services
- Detect misconfigurations and security risks
- Track cloud resource changes
- Implement security policies and compliance checks
- Respond to cloud-based security incidents

## Key Features

### Cloud Resource Monitoring

Monitor your cloud resources in real-time across multiple cloud providers. LimaCharlie can track changes to:

- Virtual machines and compute instances
- Storage buckets and databases
- Network configurations
- Identity and access management (IAM) policies
- Security groups and firewall rules

### Threat Detection

Leverage LimaCharlie's detection and response capabilities specifically tailored for cloud environments:

- Detect suspicious cloud API calls
- Identify unauthorized access attempts
- Monitor for data exfiltration
- Track privilege escalation
- Detect cryptomining and resource abuse

### Configuration Management

Continuously assess your cloud infrastructure against security best practices:

- Scan for misconfigurations
- Validate security policies
- Check compliance against frameworks (CIS, NIST, etc.)
- Alert on drift from baseline configurations

### Integration

LimaCharlie integrates with major cloud providers and services:

- AWS (Amazon Web Services)
- Azure (Microsoft Azure)
- GCP (Google Cloud Platform)
- Cloud storage services
- Container orchestration platforms

## Getting Started

To begin using cloud security features in LimaCharlie:

1. Configure your cloud provider credentials
2. Set up cloud resource collection
3. Define detection rules for cloud events
4. Create response automations
5. Configure alerting and notifications

## Best Practices

- Enable comprehensive logging across all cloud services
- Implement least-privilege access policies
- Regularly review and update security configurations
- Use detection rules to identify anomalous behavior
- Automate response actions where possible
- Maintain visibility across multi-cloud environments

---

# Endpoint Protection

LimaCharlie's Endpoint Protection Platform (EPP) provides comprehensive security monitoring and response capabilities for your endpoints.

## Overview

The LimaCharlie EPP extension provides real-time threat detection, prevention, and response capabilities across your fleet of endpoints. It combines behavioral analysis, threat intelligence, and customizable detection rules to protect against malware, ransomware, and other security threats.

## Key Features

- Real-time threat detection and prevention
- Behavioral analysis and anomaly detection
- Integration with threat intelligence feeds
- Customizable detection and response rules
- Cross-platform support (Windows, Linux, macOS)
- Low resource footprint
- Cloud-native architecture

## Installation

To enable EPP protection on your endpoints:

1. Install the LimaCharlie sensor on target endpoints
2. Enable the EPP extension in your organization
3. Configure detection rules and response actions
4. Monitor threats through the web interface or API

## Configuration

### Basic Setup

```yaml
# Example EPP configuration
extension: epp
enabled: true
config:
  protection_level: high
  scan_memory: true
  scan_network: true
```

### Detection Rules

Create custom detection rules using D&R (Detection & Response) rules:

```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: is
      path: event/FILE_PATH
      value: '*.exe'
    - op: contains
      path: event/COMMAND_LINE
      value: 'suspicious_pattern'

respond:
  - action: report
    name: suspicious_process_detected
  - action: task
    command: deny_tree
```

## Integration

The EPP extension integrates with:

- SIEM platforms via API
- Threat intelligence feeds
- Incident response workflows
- Automation and orchestration tools

## Best Practices

1. Start with moderate protection levels and adjust based on your environment
2. Test detection rules in report-only mode before enabling blocking
3. Regularly review and update your detection rules
4. Monitor false positive rates and tune accordingly
5. Integrate with your existing security workflows

## API Access

Access EPP functionality programmatically:

```python
# Example: Query EPP detections
from limacharlie import Manager

manager = Manager(oid='your-org-id', api_key='your-api-key')
detections = manager.detections(limit=100)

for detection in detections:
    print(f"Detection: {detection['detect']['event']}")
```

## Support

For technical support and additional documentation, contact the LimaCharlie support team or visit the documentation portal.

---

# SecOps Development

LimaCharlie provides a comprehensive development environment for security operations teams to build, test, and deploy detection and response capabilities.

## Development Workflow

The typical SecOps development workflow in LimaCharlie follows these stages:

1. **Local Development**: Write and test detection rules locally
2. **Testing**: Validate rules against sample data or test environments
3. **Deployment**: Push rules to production organizations
4. **Monitoring**: Track rule performance and tune as needed

## Detection & Response (D&R) Rules

D&R rules are the foundation of automated security operations in LimaCharlie. They define:

- **Detect**: What events or conditions to look for
- **Respond**: What actions to take when conditions are met

### Rule Structure

```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: is
      path: event/FILE_PATH
      value: cmd.exe
    - op: contains
      path: event/COMMAND_LINE
      value: "/c"

respond:
  - action: report
    name: suspicious_cmd_execution
```

### Testing Rules

Use the LimaCharlie CLI or API to test rules against historical data:

```bash
limacharlie dr test --rule my_rule.yaml --events sample_events.json
```

## Version Control Integration

LimaCharlie supports infrastructure-as-code approaches for managing security operations:

- Store D&R rules in Git repositories
- Use CI/CD pipelines to deploy changes
- Track changes and roll back when needed

### Example CI/CD Workflow

```yaml
# .github/workflows/deploy-rules.yml
name: Deploy D&R Rules

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Deploy to LimaCharlie
        run: |
          limacharlie dr import --file rules/*.yaml
        env:
          LC_API_KEY: ${{ secrets.LC_API_KEY }}
```

## Development Best Practices

### 1. Test Before Deploying

Always validate rules in a test environment before production deployment.

### 2. Use Descriptive Names

Name rules clearly to indicate their purpose:
- `lateral_movement_psexec`
- `credential_dump_lsass`
- `webshell_detection_aspx`

### 3. Add Metadata

Include metadata in rules for documentation and tracking:

```yaml
metadata:
  author: security-team
  severity: high
  mitre: T1021.002
  description: Detects PsExec lateral movement
```

### 4. Version Your Rules

Use semantic versioning for rule sets to track changes over time.

### 5. Monitor Performance

Track rule performance metrics:
- False positive rate
- Detection coverage
- Response time
- Resource usage

## Development Tools

### LimaCharlie CLI

The command-line interface for managing LimaCharlie resources:

```bash
# Install
pip install limacharlie

# Authenticate
limacharlie login

# List rules
limacharlie dr list

# Import rules
limacharlie dr import --file rule.yaml

# Export rules
limacharlie dr export --output rules/
```

### API Access

Programmatic access via REST API:

```python
from limacharlie import Manager

# Initialize
lc = Manager(api_key="your-api-key", oid="your-org-id")

# List D&R rules
rules = lc.rules()

# Add new rule
lc.add_rule({
    "name": "my_rule",
    "detect": {...},
    "respond": [...]
})
```

### SDK Libraries

Official SDKs available for:
- Python
- JavaScript/Node.js
- Go

## Testing Strategies

### Unit Testing

Test individual rule components:

```python
def test_process_detection():
    rule = load_rule("detect_malicious_process.yaml")
    event = create_test_event(process="evil.exe")
    assert rule.matches(event)
```

### Integration Testing

Test rules against realistic data sets:

1. Collect sample events from test environment
2. Run rules against sample data
3. Validate expected detections and responses

### Performance Testing

Ensure rules scale appropriately:

- Test with high event volumes
- Monitor resource consumption
- Optimize expensive operations

## Debugging

### Rule Testing Interface

Use the web interface to test rules interactively:

1. Navigate to D&R Rules
2. Click "Test Rule"
3. Paste sample event data
4. View match results and responses

### Logging

Enable verbose logging for troubleshooting:

```yaml
detect:
  # ... detection logic
  
respond:
  - action: report
    name: my_detection
    metadata:
      debug: true  # Enable debug logging
```

### Event Replay

Replay historical events to test rule changes:

```bash
limacharlie replay --start "2025-01-01" --end "2025-01-02" --rule new_rule.yaml
```

## Deployment Strategies

### Blue/Green Deployment

1. Deploy new rules to test organization
2. Validate performance and accuracy
3. Switch production to new rule set
4. Keep old rules as backup

### Canary Deployment

1. Deploy new rules to subset of sensors
2. Monitor for issues
3. Gradually increase coverage
4. Full rollout once validated

### Feature Flags

Use rule tags to enable/disable rules dynamically:

```yaml
detect:
  # ... detection logic

metadata:
  enabled: true
  tags: [experimental, canary]
```

## Collaboration

### Team Development

- Use separate organizations for dev/staging/prod
- Share rule templates via version control
- Document rule changes in commit messages
- Review rules before merging to main branch

### Knowledge Sharing

- Document detection logic and rationale
- Share threat intelligence context
- Maintain playbooks for response actions
- Create rule libraries for common scenarios

---

# DFIR Tag Index

## Velociraptor
10 Oct 2025
[View Article](/docs/en/ext-velociraptor)

## Plaso
09 Oct 2025
[View Article](/docs/en/ext-plaso)

## Velociraptor to BigQuery
08 Oct 2025
[View Article](/docs/en/velociraptor-to-bigquery)

## YARA
07 Oct 2025
[View Article](/docs/en/ext-yara)

## Hayabusa
07 Oct 2025
[View Article](/docs/en/ext-hayabusa)

## Reference: Endpoint Agent Commands
07 Aug 2025
[View Article](/docs/en/reference-endpoint-agent-commands)

## Incident Response
31 Jul 2025
[View Article](/docs/en/incident-response)

## YARA Manager
31 Jul 2025
[View Article](/docs/en/ext-yara-manager)

## Response Actions
02 May 2025
[View Article](/docs/en/response-actions)

## Sleeper Deployment
25 Nov 2024
[View Article](/docs/en/sleeper)

## Dumper
12 Nov 2024
[View Article](/docs/en/ext-dumper)

## Hayabusa to BigQuery
15 Oct 2024
[View Article](/docs/en/hayabusa-to-bigquery)

---

# Enterprise SOC

LimaCharlie provides enterprise-grade Security Operations Center (SOC) capabilities designed for organizations that need advanced security monitoring, threat detection, and incident response at scale.

## Key Features

### Multi-Tenant Architecture
- **Organization Hierarchy**: Create and manage multiple organizations with centralized visibility
- **Role-Based Access Control (RBAC)**: Granular permissions and access controls across your organization
- **Cross-Organization Search**: Query and correlate data across multiple tenants from a single interface

### Centralized Management
- **Unified Dashboard**: Monitor security posture across all organizations
- **Centralized Logging**: Aggregate logs and telemetry from multiple sources
- **Policy Management**: Deploy and manage detection rules, response actions, and configurations at scale

### Advanced Security Operations
- **Custom Detection Rules**: Create sophisticated detection logic using D&R (Detection & Response) rules
- **Automated Response**: Configure automated responses to security events
- **Threat Intelligence Integration**: Integrate with threat intelligence feeds and platforms
- **Case Management**: Built-in incident response and case management workflow

### Compliance & Reporting
- **Audit Logging**: Comprehensive audit trails for compliance requirements
- **Custom Reports**: Generate security reports for stakeholders
- **Data Retention Controls**: Configure retention policies to meet regulatory requirements

## Enterprise Deployment Options

### Cloud-Hosted
LimaCharlie's cloud infrastructure provides immediate deployment with:
- Global data centers for low-latency access
- Automatic scaling and high availability
- Managed infrastructure and updates

### Private Cloud
For organizations with specific compliance or data residency requirements:
- Deploy in your own cloud environment
- Maintain control over data location and access
- Full feature parity with cloud-hosted option

## Getting Started with Enterprise Features

1. **Contact Sales**: Reach out to discuss your enterprise requirements
2. **Architecture Planning**: Work with LimaCharlie team to design your deployment
3. **Deployment**: Set up your enterprise environment
4. **Integration**: Connect your security tools and data sources
5. **Training**: Onboard your SOC team with LimaCharlie platform training

## Support

Enterprise customers receive:
- Dedicated support channels
- Priority response times
- Custom integrations assistance
- Regular architecture reviews

For more information about enterprise features and pricing, contact the LimaCharlie sales team.

---

# Endpoint Protection

**Endpoint Protection Platform (EPP)** provides automated threat detection and response capabilities for your endpoints.

## Overview

The EPP extension continuously monitors endpoints for malicious activity and automatically takes protective actions based on detected threats.

## Key Features

- **Real-time Threat Detection**: Monitors endpoint activity for indicators of compromise
- **Automated Response**: Takes immediate action when threats are detected
- **Threat Intelligence Integration**: Leverages up-to-date threat intelligence feeds
- **Behavioral Analysis**: Detects anomalous behavior patterns
- **File Reputation**: Checks file hashes against known malware databases

## Configuration

To enable EPP, configure the extension in your organization settings with the desired protection policies and response actions.

## Detection Capabilities

The EPP extension provides detection for:

- Malware execution
- Ransomware activity
- Credential dumping
- Lateral movement
- Suspicious PowerShell usage
- Living-off-the-land (LOLBin) abuse
- Process injection
- Persistence mechanisms

## Response Actions

Automated responses include:

- Process termination
- Network isolation
- File quarantine
- Alert generation
- Custom remediation actions

## Best Practices

1. **Tune Detection Rules**: Adjust sensitivity based on your environment
2. **Test Response Actions**: Validate automated responses in a test environment first
3. **Monitor Alerts**: Regularly review EPP alerts for false positives
4. **Update Threat Intelligence**: Keep threat intelligence feeds current
5. **Document Exceptions**: Maintain a list of approved exceptions and whitelists

---

# GCP Documentation

This documentation collection covers LimaCharlie's integration with Google Cloud Platform (GCP) services.

## Articles

### Data Pipeline & Analysis

#### Velociraptor to BigQuery
Learn how to stream Velociraptor telemetry data to Google BigQuery for analysis and long-term storage.

[Read more: Velociraptor to BigQuery](/docs/en/velociraptor-to-bigquery)

#### Building Reports with BigQuery + Looker Studio
Create comprehensive security reports and dashboards using BigQuery as a data source and Google Looker Studio for visualization.

[Read more: Building Reports with BigQuery + Looker Studio](/docs/en/tutorials-reporting-building-reports-with-bigquery-looker-studio)

#### Hayabusa to BigQuery
Pipeline Hayabusa Windows event log analysis results to BigQuery for centralized threat hunting and incident response.

[Read more: Hayabusa to BigQuery](/docs/en/hayabusa-to-bigquery)

### Data Ingestion

#### Tutorial: Ingesting Google Cloud Logs
Step-by-step guide to ingesting logs from Google Cloud Platform into LimaCharlie for centralized monitoring and detection.

[Read more: Tutorial: Ingesting Google Cloud Logs](/docs/en/tutorial-ingesting-google-cloud-logs)

#### Google Workspace
Ingest logs and events from Google Workspace (Gmail, Drive, Calendar, etc.) for security monitoring and compliance.

[Read more: Google Workspace](/docs/en/adapter-types-google-workspace)

### Output Destinations

#### Google Cloud BigQuery
Configure BigQuery as an output destination to send LimaCharlie telemetry and detection data for analysis.

[Read more: Google Cloud BigQuery](/docs/en/outputs-destinations-google-cloud-bigquery)

#### Google Cloud Storage
Send data to Google Cloud Storage buckets for long-term archival and compliance requirements.

[Read more: Google Cloud Storage](/docs/en/outputs-destinations-google-cloud-storage)

#### Google Cloud Pubsub
Stream data to Google Cloud Pub/Sub topics for real-time event processing and integration with other GCP services.

[Read more: Google Cloud Pubsub](/docs/en/outputs-destinations-google-cloud-pubsub)

### Adapters

#### Google Cloud Pubsub Adapter
Configure the Pub/Sub adapter to receive data from Google Cloud Platform into LimaCharlie.

[Read more: Google Cloud Pubsub](/docs/en/adapter-types-google-cloud-pubsub)

#### Google Cloud Storage Adapter
Configure the Cloud Storage adapter to ingest data from GCS buckets into LimaCharlie.

[Read more: Google Cloud Storage](/docs/en/adapter-types-google-cloud-storage)

### CLI Extensions

#### Google Cloud CLI Extension
Use the LimaCharlie CLI with Google Cloud-specific commands and integrations.

[Read more: Google Cloud](/docs/en/ext-cloud-cli-google-cloud)

---

# Google Workspace

*Content to be extracted from the linked article at `/docs/en/adapter-types-google-workspace`*

---

# ChromeOS with Google Chrome Enterprise

*Content to be extracted from the linked article at `/docs/en/chrome-enterprise`*

---

**Note**: This appears to be a tag/category index page listing articles tagged with "google workspace". To properly clean the documentation, I would need access to the actual article content from the linked pages:

1. `/docs/en/adapter-types-google-workspace` 
2. `/docs/en/chrome-enterprise`

If you need these specific articles cleaned, please provide the raw markdown content from those individual documentation pages.

---

# Linux

This tag contains 14 articles related to Linux in LimaCharlie documentation:

## Articles

* [Ingesting Linux Audit Logs](/docs/en/ingesting-linux-audit-logs) - 10 Oct 2025
* [Reference: EDR Events](/docs/en/reference-edr-events) - 22 Sep 2025
* [Syslog](/docs/en/adapter-types-syslog) - 20 Aug 2025
* [Reference: Endpoint Agent Commands](/docs/en/reference-endpoint-agent-commands) - 07 Aug 2025
* [Kubernetes Pods Logs](/docs/en/adapter-types-kubernetes-pods-logs) - 16 Jul 2025
* [Docker Agent Installation](/docs/en/docker-agent-installation) - 12 Feb 2025
* [Linux Agent Installation](/docs/en/linux-agent-installation) - 06 Feb 2025
* [Payloads](/docs/en/payloads) - 05 Oct 2024
* [Endpoint Agent Uninstallation](/docs/en/endpoint-agent-uninstallation) - 05 Oct 2024
* [Container Clusters](/docs/en/container-clusters) - 05 Oct 2024
* [Endpoint Agent Installation](/docs/en/endpoint-agent-installation) - 07 Jul 2025
* [Endpoint Agent Commands](/docs/en/endpoint-agent-commands) - 18 Apr 2025
* [Endpoint Agent](/docs/en/endpoint-agent) - 01 Nov 2024
* [Endpoint Agent Events Overview](/docs/en/endpoint-agent-events-overview) - 10 Dec 2024

---

# Lookup Manager

The Lookup Manager is a powerful feature in LimaCharlie that allows you to create and manage lookup tables for enriching detections, automating responses, and correlating security data.

## Overview

Lookups are key-value stores that can be used in Detection & Response (D&R) rules, output destinations, and other automation workflows. They enable you to:

- Maintain allowlists and blocklists
- Store threat intelligence indicators
- Create asset inventories
- Manage configuration data
- Enrich events with contextual information

## Creating a Lookup

To create a lookup table:

1. Navigate to the **Lookups** section in your organization
2. Click **Add Lookup**
3. Provide a name for your lookup (use lowercase, numbers, and hyphens only)
4. Optionally add a description
5. Choose the lookup type:
   - **Simple**: Basic key-value pairs
   - **Resource**: Links to external resources
   - **List**: Simple list of values

## Lookup Types

### Simple Lookups

Simple lookups store key-value pairs where both keys and values are strings.

**Example use case**: Store IP addresses with associated metadata

```yaml
key: 192.168.1.100
value: {"owner": "IT Department", "location": "HQ"}
```

### Resource Lookups

Resource lookups reference external URLs or resources that can be fetched and cached.

**Example use case**: Pull threat intelligence feeds from external sources

### List Lookups

List lookups store simple lists of values without keys.

**Example use case**: Maintain a list of known good executable hashes

## Using Lookups in D&R Rules

You can reference lookups in your Detection & Response rules using the `lookup()` function.

### Basic Lookup Syntax

```yaml
detect:
  op: lookup
  path: event/IP_ADDRESS
  lookup: my_blocklist
  case-sensitive: false
```

### Example: Allowlist Detection

```yaml
detect:
  op: and
  rules:
    - op: is
      path: event/COMMAND_LINE
      value: powershell.exe
    - op: not
      rule:
        op: lookup
        path: event/HASH
        lookup: known_good_hashes
```

This rule detects PowerShell execution where the hash is NOT in the `known_good_hashes` lookup.

## Managing Lookup Data

### Adding Entries

You can add entries to lookups through:

- **Web UI**: Manually add entries one at a time
- **REST API**: Programmatically manage lookup data
- **CLI**: Use the LimaCharlie CLI for bulk operations

### Bulk Import

To import data in bulk, use the REST API or CLI:

```bash
limacharlie lookup add --name my_lookup --key "example_key" --value "example_value"
```

### Updating Entries

Update existing entries by submitting a new value with the same key:

```bash
limacharlie lookup add --name my_lookup --key "existing_key" --value "new_value"
```

### Deleting Entries

Remove entries from a lookup:

```bash
limacharlie lookup del --name my_lookup --key "key_to_delete"
```

## Lookup Performance

Lookups are optimized for fast access:

- Cached at the sensor level for minimal latency
- Updated automatically when changes are made
- Supports thousands of entries with minimal performance impact

## Best Practices

1. **Naming Convention**: Use descriptive names like `ip_allowlist` or `threat_indicators`
2. **Documentation**: Add descriptions to explain the purpose of each lookup
3. **Regular Maintenance**: Review and update lookup data regularly
4. **Version Control**: Keep backups of critical lookup data
5. **Access Control**: Limit who can modify lookup tables
6. **Size Management**: Keep lookups focused and avoid storing unnecessary data

## API Reference

### Add or Update Entry

```
POST /v2/orgs/{oid}/lookup/{lookup_name}
Content-Type: application/json

{
  "key": "example_key",
  "value": "example_value"
}
```

### Get Entry

```
GET /v2/orgs/{oid}/lookup/{lookup_name}/{key}
```

### Delete Entry

```
DELETE /v2/orgs/{oid}/lookup/{lookup_name}/{key}
```

### List All Entries

```
GET /v2/orgs/{oid}/lookup/{lookup_name}
```

## Common Use Cases

### Threat Intelligence

Store indicators of compromise (IOCs) and check events against them:

```yaml
detect:
  op: lookup
  path: event/FILE_HASH
  lookup: malware_hashes
```

### Asset Management

Maintain an inventory of critical assets:

```yaml
detect:
  op: and
  rules:
    - op: lookup
      path: event/IP_ADDRESS
      lookup: critical_assets
    - op: is
      path: event/EVENT_TYPE
      value: NETWORK_CONNECTIONS
```

### User Allowlisting

Exclude known admin activity from alerts:

```yaml
detect:
  op: not
  rule:
    op: lookup
    path: event/USER_NAME
    lookup: admin_users
```

---

# Soteria M365 Rules

[Documentation content appears to be missing from the provided page]

# Microsoft 365

[Documentation content appears to be missing from the provided page]

# Microsoft 365

[Documentation content appears to be missing from the provided page]

---

**Note**: The provided HTML appears to be a tag index/listing page rather than actual documentation content. This page shows 3 articles tagged with "m365":

1. Soteria M365 Rules (09 Oct 2025)
2. Microsoft 365 - Adapter Types (07 Aug 2025)  
3. Microsoft 365 - Cloud CLI Extension (10 Oct 2024)

To extract documentation content, you would need to provide the actual article pages (e.g., `/docs/en/soteria-m365-rules`, `/docs/en/adapter-types-microsoft-365`, or `/docs/en/ext-cloud-cli-microsoft365`) rather than this tag listing page.

---

# Tag: macos

## Articles

* [Ingesting MacOS Unified Logs](/docs/en/ingesting-macos-unified-logs)
* [Reference: EDR Events](/docs/en/reference-edr-events)
* [Reference: Endpoint Agent Commands](/docs/en/reference-endpoint-agent-commands)
* [Mac Unified Logging](/docs/en/adapter-types-mac-unified-logging)
* [macOS Agent Installation - Latest Versions (macOS 15 Sequoia and newer)](/docs/en/clone-macos-agent-installation-latest-versions-macos-15-sequoia-and-newer)
* [macOS Agent Installation - Older Versions (macOS 10.15 Catalina to macOS 14 Sonoma)](/docs/en/macos-agent-installation-latest-os-versions)
* [macOS Agent Installation - MDM Configuration Profiles](/docs/en/macos-agent-installation-mdm-configuration-profiles)
* [macOS Agent Installation - Older Versions (macOS 10.14 and prior)](/docs/en/macos-agent-installation-older-versions)
* [macOS Agent Installation via Jamf Now](/docs/en/installing-macos-agents-via-jamf-now)
* [Payloads](/docs/en/payloads)
* [Endpoint Agent Uninstallation](/docs/en/endpoint-agent-uninstallation)
* [Endpoint Agent Installation](/docs/en/endpoint-agent-installation)
* [macOS Agent Installation](/docs/en/macos-agent-installation)
* [Endpoint Agent Commands](/docs/en/endpoint-agent-commands)
* [Endpoint Agent](/docs/en/endpoint-agent)
* [Endpoint Agent Events Overview](/docs/en/endpoint-agent-events-overview)

---

# M&A Cyber Due Diligence

## Overview

Mergers and acquisitions (M&A) represent critical inflection points where cyber risk assessment can make or break deal value. Traditional due diligence often treats cybersecurity as a checkbox exercise, but modern M&A demands deep technical visibility into an acquisition target's actual security posture.

LimaCharlie provides rapid deployment capabilities that enable acquirers to gain genuine visibility into a target's environment during due diligence, transforming cybersecurity from a theoretical risk assessment into an evidence-based evaluation.

## The M&A Cyber Risk Challenge

Traditional M&A cyber due diligence relies on:

- Self-reported security questionnaires
- Compliance certifications (SOC2, ISO 27001)
- Interviews with target's IT/security teams
- Review of policies and procedures documents
- Third-party assessment reports

**The Problem**: These methods reveal what the target *claims* about their security, not what *actually exists* in their environment.

**Real M&A Cyber Risks**:
- Unpatched critical vulnerabilities
- Shadow IT and unknown assets
- Compromised systems (active breaches)
- Excessive privileged access
- Poor configuration management
- Inadequate logging and monitoring
- Legacy systems with no support

These risks directly impact:
- **Deal valuation**: Significant remediation costs
- **Integration timeline**: Security must be addressed before integration
- **Post-acquisition liability**: Breaches discovered after close
- **Regulatory exposure**: Compliance violations

## LimaCharlie for M&A Due Diligence

LimaCharlie enables technical due diligence through rapid deployment and immediate visibility.

### Key Capabilities

**1. Rapid Deployment**
- Deploy sensors across target environment in hours/days, not weeks
- Minimal infrastructure impact
- Cross-platform support (Windows, Linux, macOS, containers)
- Cloud and on-premise coverage

**2. Immediate Visibility**
- Real-time asset inventory
- Running processes and services
- Network connections and communications
- Installed software and versions
- User accounts and privileges
- Security tool coverage gaps

**3. Evidence-Based Risk Assessment**
- Detect unpatched vulnerabilities
- Identify lateral movement risks
- Discover shadow IT
- Find indicators of compromise (IoCs)
- Assess logging and monitoring capabilities
- Validate security controls

**4. Compliance Validation**
- Verify claimed security controls actually exist
- Validate endpoint protection deployment
- Confirm logging and retention practices
- Check patch management effectiveness

### Deployment Models for Due Diligence

**Option 1: Pre-LOI Technical Assessment**
- Deploy LimaCharlie sensors during exclusive negotiation period
- Requires target cooperation and data room access
- Provides deepest technical visibility
- Informs final valuation and deal terms

**Option 2: Post-LOI, Pre-Close Deep Dive**
- Deploy after Letter of Intent signed
- Part of confirmatory due diligence
- Validates representations and warranties
- Identifies deal-breaker issues before close

**Option 3: Day-One Post-Acquisition Baseline**
- Deploy immediately after acquisition closes
- Establishes security baseline
- Guides integration planning
- Enables continuous monitoring during integration

## Implementation Approach

### Phase 1: Planning (1-2 days)

**Define Scope**
- Which systems/networks to assess
- Data collection boundaries
- Access requirements
- Timeline constraints

**Coordinate with Target**
- Explain deployment approach
- Address target's privacy/security concerns
- Obtain necessary approvals
- Establish technical contacts

**Configure LimaCharlie**
- Set up dedicated organization
- Define collection profiles
- Configure detection rules
- Establish data retention

### Phase 2: Deployment (1-3 days)

**Sensor Installation**
- Deploy sensors to representative systems
- Validate connectivity and data flow
- Confirm minimal performance impact
- Document coverage achieved

**Initial Collection**
- System inventory
- Process baselines
- Network mapping
- Software inventory

### Phase 3: Assessment (3-7 days)

**Technical Analysis**
- Vulnerability identification
- Security control validation
- Threat hunting
- Configuration assessment

**Risk Scoring**
- Categorize findings by severity
- Estimate remediation costs
- Assess integration complexity
- Identify deal risks

**Documentation**
- Technical findings report
- Risk assessment summary
- Remediation roadmap
- Cost estimates

### Phase 4: Reporting (1-2 days)

**Executive Summary**
- High-level risk profile
- Critical findings
- Valuation impact
- Remediation timeline

**Technical Report**
- Detailed findings
- Evidence and screenshots
- Remediation recommendations
- Tool/resource requirements

**Deal Recommendations**
- Valuation adjustments
- Contract provisions (reps/warranties)
- Escrow considerations
- Post-close conditions

## Key Risk Areas to Assess

### 1. Asset Discovery & Inventory

**What to Look For:**
- Unknown/shadow IT systems
- Unmanaged endpoints
- Legacy systems
- Cloud resources

**LimaCharlie Detection:**
- Comprehensive endpoint inventory
- Network communication analysis
- Cloud workload visibility
- Container/Kubernetes discovery

### 2. Vulnerability Management

**What to Look For:**
- Unpatched critical vulnerabilities
- End-of-life software
- Unsupported operating systems
- Missing security updates

**LimaCharlie Detection:**
- Software version inventory
- Known vulnerable software identification
- Patch level assessment
- OS support status

### 3. Threat Presence

**What to Look For:**
- Active malware infections
- Command & control communications
- Lateral movement indicators
- Data exfiltration attempts

**LimaCharlie Detection:**
- Behavioral threat detection
- Network IOC matching
- Process analysis
- File hash reputation

### 4. Access Control

**What to Look For:**
- Excessive administrative privileges
- Shared/generic accounts
- Weak authentication
- Dormant accounts with access

**LimaCharlie Detection:**
- User account enumeration
- Privilege level identification
- Login pattern analysis
- Service account discovery

### 5. Security Tooling Gaps

**What to Look For:**
- Missing endpoint protection
- No EDR/XDR coverage
- Inadequate logging
- No SIEM/monitoring

**LimaCharlie Detection:**
- Security tool inventory
- Coverage gap identification
- Logging capability assessment
- Detection/response capability validation

## Case Study Examples

### Example 1: Hidden Compromise Discovered

**Situation**: Mid-market SaaS acquisition, $50M valuation

**LimaCharlie Findings**:
- Active Cobalt Strike beacon on internal server
- Lateral movement to 12 additional systems
- Exfiltration of customer database
- Breach estimated at 4-6 months old

**Outcome**: 
- Deal paused pending incident response
- Valuation reduced by $8M
- Mandatory cyber insurance as condition of close
- 12-month escrow for potential breach liability

### Example 2: Infrastructure Reality Check

**Situation**: Healthcare technology acquisition, $30M valuation

**Target Claims**:
- "Fully patched environment"
- "24/7 security monitoring"
- "EDR on all endpoints"

**LimaCharlie Findings**:
- 67% of servers running unsupported OS versions
- No EDR on 40% of workstations
- Security monitoring limited to firewall logs
- 200+ critical vulnerabilities

**Outcome**:
- Valuation reduced by $4M
- $2M remediation budget required
- 6-month delay in integration timeline
- Dedicated security team to be retained post-close

### Example 3: Shadow IT Discovery

**Situation**: Financial services acquisition, $100M valuation

**LimaCharlie Findings**:
- 45 AWS accounts unknown to IT
- Personal Dropbox/Google Drive in use for customer data
- 30+ SaaS applications with no oversight
- Production databases with public internet exposure

**Outcome**:
- Shadow IT remediation plan required
- Data governance program implementation mandatory
- Additional compliance audit required
- Post-close security integration extended by 6 months

## Integration with Deal Process

### Term Sheet / LOI Stage

**Consideration**: Include cybersecurity assessment rights in term sheet

**Language Example**:
> "Buyer shall have the right to deploy monitoring and assessment tools within Target's IT environment for the purpose of cybersecurity due diligence, subject to reasonable confidentiality and data privacy protections."

### Due Diligence Stage

**Timeline Integration**:
- Week 1-2: Deploy LimaCharlie sensors
- Week 2-3: Data collection and analysis
- Week 3-4: Risk assessment and reporting
- Week 4+: Findings review and negotiation

### Purchase Agreement Stage

**Findings Impact**:
- Representations and warranties specific to findings
- Indemnification carve-outs for known issues
- Escrow provisions for remediation costs
- Post-close security requirements

**Example Rep/Warranty**:
> "Except as disclosed in Schedule X [LimaCharlie findings], Seller represents that: (a) no Systems contain material vulnerabilities or malware, (b) all Systems are running supported software versions, (c) no unauthorized access to Systems has occurred in the past 24 months."

### Post-Close Integration

**Continued Use**:
- Maintain LimaCharlie deployment
- Monitor during integration
- Track remediation progress
- Validate security improvements

## Best Practices

### 1. Start Early
Deploy LimaCharlie as soon as you have access to the target environment. Every day of visibility helps.

### 2. Be Transparent
Explain to the target what you're doing and why. Frame it as protecting both parties.

### 3. Focus on Material Risks
Prioritize findings that impact valuation, timeline, or deal viability. Don't get lost in minor issues.

### 4. Quantify Everything
Translate technical findings into business impact: remediation costs, integration delays, breach liability.

### 5. Document Thoroughly
Maintain detailed evidence of all findings. This documentation protects you post-close.

### 6. Plan for Remediation
Don't just identify problemsprovide realistic remediation roadmaps with timelines and costs.

### 7. Consider Retention
Keep key target security personnel identified during assessment to support remediation.

### 8. Maintain Deployment
Don't tear down LimaCharlie after due diligence. Use it for post-close integration monitoring.

## ROI Calculation

### Direct Value

**Risk Mitigation**:
- Avoided post-close breach costs: $500K - $5M+
- Prevented overpayment due to hidden risks: 5-15% of deal value
- Reduced integration costs through early identification: $200K - $2M

**Deal Intelligence**:
- Evidence-based valuation adjustments
- Informed negotiation leverage
- Accurate remediation budgeting

### Indirect Value

**Integration Acceleration**:
- Clear security baseline from day one
- Pre-planned remediation roadmap
- Faster path to unified security posture

**Risk Transfer**:
- Document known issues for reps/warranties
- Establish escrow terms based on evidence
- Reduce post-close surprises

### Cost Structure

**LimaCharlie Costs**:
- Endpoint licensing: ~$1-3 per endpoint/month
- Professional services (optional): $10K - $50K
- Total typical engagement: $5K - $75K

**Typical ROI**: 10x - 100x investment through risk mitigation and better deal terms

## Getting Started

### Step 1: Contact LimaCharlie

Reach out to LimaCharlie M&A team:
- Email: sales@limacharlie.io
- Schedule consultation call
- Discuss specific deal requirements

### Step 2: Scope Definition

Define assessment scope:
- Target environment size and complexity
- Access availability and timeline
- Key risk areas to assess
- Reporting requirements

### Step 3: Deployment Planning

Work with LimaCharlie team to plan:
- Sensor deployment approach
- Data collection configuration
- Detection rule customization
- Timeline coordination with deal process

### Step 4: Execute Assessment

Deploy and analyze:
- Install sensors
- Collect telemetry
- Analyze findings
- Generate reports

### Step 5: Act on Intelligence

Use findings to:
- Inform valuation
- Negotiate terms
- Plan remediation
- Structure deal protections

## Conclusion

M&A cyber due diligence is no longer optionalit's a critical component of deal success. Traditional approaches based on questionnaires and compliance reports provide false comfort, while technical assessment with tools like LimaCharlie delivers genuine visibility into cyber risk.

By deploying LimaCharlie during due diligence, acquirers gain:
- **Evidence-based risk assessment** rather than self-reported claims
- **Material findings** that impact valuation and deal terms
- **Remediation roadmaps** that inform integration planning
- **Continuous monitoring** that extends beyond close

The investment in technical due diligence is minimal compared to the risks of post-close security surprises, regulatory exposure, and breach liability. Every significant M&A transaction should include hands-on cybersecurity assessment using platforms like LimaCharlie.

For organizations engaged in M&A activitywhether as serial acquirers or occasional buyersestablishing a repeatable technical due diligence process with LimaCharlie provides competitive advantage through better risk understanding, more accurate valuation, and faster post-close integration.

---

# Network Monitoring

Network monitoring in LimaCharlie provides visibility into network traffic, connections, and communications across your infrastructure. This enables detection of malicious network activity, unauthorized connections, and data exfiltration attempts.

## Overview

LimaCharlie's network monitoring capabilities allow you to:

- Monitor network connections in real-time
- Detect suspicious network patterns
- Track data flows and communications
- Identify unauthorized network access
- Monitor DNS queries and responses
- Analyze network protocols and traffic

## Configuration

Network monitoring can be configured through Detection & Response (D&R) rules and collection policies to capture relevant network events from your sensors.

## Key Network Events

LimaCharlie captures various network-related events including:

- **NEW_CONNECTION**: New network connections established
- **DNS_REQUEST**: DNS query requests
- **HTTP_REQUEST**: HTTP/HTTPS requests
- **NETWORK_SUMMARY**: Periodic network activity summaries
- **CONNECTION_CLOSED**: Network connections terminated

## Detection Capabilities

Network monitoring enables detection of:

- Command and control (C2) communications
- Data exfiltration attempts
- Lateral movement
- Suspicious DNS queries
- Unauthorized outbound connections
- Port scanning and reconnaissance
- Protocol anomalies

## Best Practices

1. **Focus on anomalies**: Monitor for unusual network patterns rather than all traffic
2. **Baseline normal behavior**: Understand typical network activity before alerting on deviations
3. **Correlate events**: Combine network events with process and file events for context
4. **Use allowlists**: Reduce noise by allowlisting known-good connections
5. **Monitor DNS**: DNS is often an early indicator of compromise

---

# Security Monitoring for DevOps

Security monitoring for DevOps integrates security practices into the continuous integration and continuous deployment (CI/CD) pipeline, enabling teams to identify and respond to security issues throughout the software development lifecycle.

## Overview

LimaCharlie provides security monitoring capabilities that integrate seamlessly into DevOps workflows:

- Runtime security monitoring for containers and cloud workloads
- CI/CD pipeline security
- Infrastructure as code (IaC) security scanning
- Continuous compliance monitoring
- API and webhook integrations for automation

## DevOps Integration

### CI/CD Pipeline Integration

LimaCharlie can be integrated into CI/CD pipelines to:

- Scan container images for vulnerabilities
- Monitor build processes for suspicious activity
- Validate security configurations
- Enforce security policies before deployment
- Generate security attestations

### Container Security

Monitor containerized environments with:

- Runtime container monitoring
- Image vulnerability scanning
- Container escape detection
- Kubernetes security monitoring
- Docker security monitoring

### Cloud Security

Monitor cloud infrastructure including:

- EC2 instances
- Lambda functions
- Cloud storage access
- API gateway activity
- Cloud configuration compliance

## Automation

LimaCharlie's API and webhook capabilities enable:

- Automated incident response
- Security orchestration
- Custom integrations with DevOps tools
- Automated remediation workflows
- Security metrics and reporting

## Best Practices

1. **Shift left**: Integrate security early in the development process
2. **Automate security checks**: Build security into CI/CD pipelines
3. **Monitor runtime behavior**: Don't rely solely on static analysis
4. **Use infrastructure as code**: Version control and audit security configurations
5. **Implement least privilege**: Limit access and permissions by default
6. **Continuous monitoring**: Monitor applications and infrastructure continuously, not just at deployment

## Key Capabilities

- **Real-time monitoring**: Detect threats as they occur in production
- **Automated response**: Configure automated responses to security events
- **Compliance**: Maintain continuous compliance with security standards
- **Visibility**: Gain comprehensive visibility across development and production environments
- **Scalability**: Monitor security across distributed, cloud-native architectures

---

# Observability Pipeline

LimaCharlie provides a comprehensive observability pipeline that allows you to collect, process, and route security telemetry data across your infrastructure.

## Overview

The observability pipeline enables you to:

- Collect data from multiple sources
- Transform and enrich data in real-time
- Route data to various destinations
- Apply detection and response rules
- Maintain full visibility across your security stack

## Key Features

### Data Collection

LimaCharlie can ingest data from various sources including:

- EDR sensors
- Cloud infrastructure
- Network devices
- Third-party security tools
- Custom applications via API

### Data Processing

The pipeline supports:

- Real-time data transformation
- Enrichment with threat intelligence
- Normalization across different data sources
- Filtering and sampling

### Data Routing

Route processed data to:

- SIEM platforms
- Data lakes
- Security analytics tools
- Incident response platforms
- Custom webhooks

## Getting Started

To set up your observability pipeline:

1. Configure your data sources
2. Define your processing rules
3. Set up your destinations
4. Deploy and monitor

For detailed implementation guidance, refer to the specific integration documentation for your use case.

---

# Observability Pipeline

The Observability Pipeline in LimaCharlie allows you to process, transform, and route telemetry data in real-time. This powerful feature enables you to normalize data from multiple sources, enrich events, filter noise, and send processed data to various destinations.

## Overview

The pipeline operates on streaming telemetry data before it's stored or forwarded. You can chain multiple processing steps to transform data as it flows through the system.

## Key Capabilities

- **Data Transformation**: Modify event structure, add/remove fields, normalize formats
- **Enrichment**: Add context from external sources or internal lookups
- **Filtering**: Drop unwanted events or sample high-volume data
- **Routing**: Send specific events to different outputs based on conditions
- **Aggregation**: Combine multiple events into summaries
- **Parsing**: Extract structured data from unstructured logs

## Pipeline Components

### Processors

Processors are the building blocks of your pipeline. Each processor performs a specific transformation:

- **Filter**: Include/exclude events based on criteria
- **Parse**: Extract fields using regex, JSON parsing, or other methods
- **Enrich**: Add fields from lookups or external APIs
- **Transform**: Modify field values or event structure
- **Aggregate**: Combine events over time windows
- **Sample**: Reduce event volume by sampling

### Outputs

Define where processed events should be sent:

- LimaCharlie storage
- External SIEMs
- Data lakes
- Custom webhooks
- Other cloud services

## Configuration

Pipelines are configured using YAML or through the web interface. Example pipeline configuration:

```yaml
pipeline:
  - processor: filter
    config:
      include:
        event_type: NETWORK_CONNECTIONS
  
  - processor: parse
    config:
      field: event.COMMAND_LINE
      pattern: '(?P<command>\w+)\s+(?P<args>.*)'
  
  - processor: enrich
    config:
      lookup: threat_intel
      field: event.IP_ADDRESS
      output_field: threat_info
  
  - processor: output
    config:
      destination: splunk
      format: cef
```

## Best Practices

1. **Start Simple**: Begin with basic filtering and gradually add complexity
2. **Test Thoroughly**: Use pipeline testing tools before deploying to production
3. **Monitor Performance**: Watch for pipeline latency and processing errors
4. **Document Logic**: Comment complex transformations for maintainability
5. **Handle Errors**: Include error handling for parsing and enrichment failures

## Performance Considerations

- Keep pipelines efficient to avoid latency
- Use sampling for high-volume event types
- Cache enrichment lookups when possible
- Monitor pipeline metrics and alerts

## Related Documentation

- Output destinations and integrations
- Detection & Response rules
- Artifact collection
- Data retention policies

---

# Table Top Exercises

LimaCharlie provides a comprehensive framework for conducting security tabletop exercises to test and improve your organization's incident response capabilities.

## Overview

Tabletop exercises are simulated security scenarios that allow teams to practice their response procedures in a controlled environment. LimaCharlie's platform enables you to design, execute, and analyze these exercises effectively.

## Key Features

- **Scenario Creation**: Design custom security scenarios based on real-world threats
- **Team Coordination**: Facilitate communication and decision-making among response teams
- **Real-time Monitoring**: Track exercise progress and team responses
- **Post-Exercise Analysis**: Review decisions and outcomes to identify improvement areas

## Benefits

1. **Risk-Free Practice**: Test response procedures without impacting production systems
2. **Team Readiness**: Ensure all team members understand their roles during incidents
3. **Process Improvement**: Identify gaps in current response procedures
4. **Documentation**: Create records of exercises for compliance and training purposes

## Best Practices

- Schedule regular exercises to maintain team readiness
- Vary scenario complexity and types
- Include stakeholders from different departments
- Document lessons learned and update procedures accordingly
- Use realistic scenarios based on your threat landscape

---

# Windows

## Articles

### Endpoint Agent & Installation
- [Endpoint Agent](/docs/en/endpoint-agent) - 01 Nov 2024
- [Endpoint Agent Installation](/docs/en/endpoint-agent-installation) - 07 Jul 2025
- [Windows Agent Installation](/docs/en/windows-agent-installation) - 05 Oct 2024
- [Agent Deployment via Microsoft Intune](/docs/en/agent-deployment-microsoft-intune) - 05 Oct 2024
- [Building a custom MSI installer for Windows](/docs/en/building-a-custom-msi-installer-for-windows) - 05 Oct 2024
- [Endpoint Agent Uninstallation](/docs/en/endpoint-agent-uninstallation) - 05 Oct 2024
- [VDI & Virtual Machine Templates](/docs/en/vdi-virtual-machine-templates) - 01 Nov 2024

### Agent Commands & Reference
- [Endpoint Agent Commands](/docs/en/endpoint-agent-commands) - 18 Apr 2025
- [Reference: Endpoint Agent Commands](/docs/en/reference-endpoint-agent-commands) - 07 Aug 2025
- [Payloads](/docs/en/payloads) - 05 Oct 2024

### Event Logs & Monitoring
- [Ingesting Windows Event Logs](/docs/en/ingesting-windows-event-logs) - 07 Oct 2025
- [Ingesting Sysmon Event Logs](/docs/en/ingesting-sysmon-event-logs) - 09 Oct 2025
- [Ingesting Defender Event Logs](/docs/en/ingesting-defender-event-logs) - 01 Nov 2024
- [Windows Event Logs](/docs/en/adapter-examples-windows-event-logs) - 28 May 2025
- [Sysmon Comparison](/docs/en/sysmon-comparison) - 12 Nov 2024
- [Reference: EDR Events](/docs/en/reference-edr-events) - 22 Sep 2025

### Adapters
- [Windows Event Log](/docs/en/adapter-types-windows-event-log) - 30 Jul 2025
- [EVTX](/docs/en/adapter-types-evtx) - 07 Aug 2025
- [IIS Logs](/docs/en/adapter-types-iis) - 06 Dec 2024

### Protection
- [Endpoint Protection](/docs/en/ext-epp) - 28 May 2025

---

# Clean Documentation Task

I'll extract the clean documentation content from this page, removing all UI chrome and navigation elements.

However, I notice this input appears to be **only the website's header navigation and sidebar** - there's no actual documentation content visible in what you've provided. The page shows:

- Logo and header
- Navigation filters (v1/v2, date ranges)
- A sidebar list of recently updated articles with links

But there's **no main documentation content** to clean.

## What I Need

Could you please provide the **full page content** that includes:
- The navigation/header (which you've shown)
- **The main documentation content** (which is missing)
- Any footer elements

## How to Get the Full Content

If you're using a tool to convert HTML to markdown, make sure it's capturing the entire page body, not just the navigation elements. The main content area should contain the actual documentation text, code examples, explanations, etc.

Once you provide the complete page with the actual documentation content, I'll clean it according to the specifications - removing all the navigation chrome while preserving the core documentation.

---

# SOAR / Automation

LimaCharlie's SOAR (Security Orchestration, Automation, and Response) capabilities enable you to automate security operations and incident response workflows.

## Overview

The platform provides powerful automation features that allow you to:

- Create automated response workflows
- Orchestrate complex security operations
- Integrate with external tools and services
- Build custom detection and response logic
- Scale security operations efficiently

## Key Features

### Detection & Response (D&R) Rules

D&R rules are the foundation of LimaCharlie's automation capabilities. They allow you to:

- Define detection logic based on telemetry events
- Trigger automated responses when threats are detected
- Create complex multi-stage detection scenarios
- Respond to threats in real-time

### Outputs and Integrations

LimaCharlie supports numerous output integrations to connect your security workflows:

- SIEM integrations (Splunk, Elastic, etc.)
- Ticketing systems (Jira, ServiceNow, etc.)
- Communication platforms (Slack, Microsoft Teams, etc.)
- Custom webhooks for any external service

### Automation Service

The Automation Service provides advanced capabilities for:

- Running scheduled tasks
- Executing complex multi-step workflows
- Managing long-running operations
- Coordinating actions across multiple systems

### Response Actions

When detections occur, you can automatically:

- Isolate compromised endpoints
- Kill malicious processes
- Collect forensic evidence
- Block network connections
- Execute custom remediation scripts

## Best Practices

1. **Start Simple**: Begin with basic detection rules and gradually increase complexity
2. **Test Thoroughly**: Use LimaCharlie's testing features to validate rules before deployment
3. **Monitor Performance**: Track rule execution and adjust as needed
4. **Document Workflows**: Maintain clear documentation of your automation logic
5. **Review Regularly**: Periodically audit and update automation rules

## Use Cases

- **Threat Hunting**: Automate searches for indicators of compromise
- **Incident Response**: Orchestrate response actions across your infrastructure
- **Compliance**: Automate evidence collection and reporting
- **Threat Intelligence**: Integrate threat feeds and automate enrichment
- **User Behavior Analytics**: Detect anomalous activities automatically

## Getting Started

To begin using LimaCharlie's automation capabilities:

1. Define your security objectives and use cases
2. Create D&R rules for common threats
3. Configure output integrations for alerting
4. Test your automation workflows
5. Monitor and refine based on results

---

# Soteria AWS Rules

**Last Updated:** 10 Oct 2025

AWS-specific detection rules for cloud security monitoring.

---

# Amazon S3

**Last Updated:** 07 Oct 2025

Configure Amazon S3 as an output destination for LimaCharlie data.

---

# SQS

**Last Updated:** 07 Aug 2025

AWS Simple Queue Service (SQS) adapter for ingesting events into LimaCharlie.

---

# S3

**Last Updated:** 07 Aug 2025

AWS S3 adapter for ingesting data from S3 buckets into LimaCharlie.

---

# AWS GuardDuty

**Last Updated:** 06 Jun 2025

AWS GuardDuty adapter for ingesting security findings into LimaCharlie.

---

# AWS CloudTrail

**Last Updated:** 01 Nov 2024

AWS CloudTrail adapter for ingesting API activity logs into LimaCharlie.

---

# AWS

**Last Updated:** 05 Oct 2024

AWS integration and CLI tools for LimaCharlie cloud security.

---

# Cloud

No documentation content found for this tag.

---

I can see you've shared a screenshot or page content that appears to be from the LimaCharlie documentation system, but it seems to be showing a "No Articles found for defender Tag" message rather than actual documentation content to clean.

This appears to be a navigation/tag browsing page rather than a documentation article. According to the cleaning guidelines you provided, this type of page would be:

**What this page contains:**
- Logo/header (UI chrome - should be removed)
- Navigation elements (UI chrome - should be removed)  
- A "No Articles found" message (not actual documentation content)

**Result:**
Since there is no actual documentation content on this page (it's just showing that no articles exist for the "defender" tag), there's nothing to extract or clean. This is purely UI chrome with no substantive documentation content.

Would you like me to:
1. Process a different documentation page that has actual content?
2. Help you find documentation pages that need cleaning?
3. Set up a batch process to clean multiple documentation files?

If you have a specific documentation file you'd like me to clean, please share it and I'll extract only the substantive documentation content according to your specifications.

---

# SecOps Development

LimaCharlie provides a comprehensive development environment for building, testing, and deploying security operations workflows and detection logic.

## Development Workflow

The typical SecOps development workflow in LimaCharlie follows these stages:

1. **Local Development**: Write and test detection rules, response actions, and automation logic
2. **Testing**: Validate rules against test data and simulated events
3. **Staging**: Deploy to a staging organization for integration testing
4. **Production**: Roll out verified logic to production environments

## Detection & Response (D&R) Rules

D&R rules are the core of LimaCharlie's detection capabilities. They combine detection logic with automated response actions.

### Rule Structure

```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: is
      path: event/FILE_PATH
      value: "powershell.exe"
    - op: contains
      path: event/COMMAND_LINE
      value: "-encodedcommand"

respond:
  - action: report
    name: suspicious_powershell
  - action: task
    command: deny_tree
```

### Testing Rules

Use the LimaCharlie CLI or web interface to test rules against:

- Historical event data
- Synthetic test events
- Live event streams in test mode

## Automation & Orchestration

LimaCharlie supports multiple automation approaches:

### Output Streams

Forward detections and events to external systems:

- SIEM platforms
- SOAR tools
- Custom webhooks
- Message queues

### Service Extensions

Extend LimaCharlie functionality with custom code:

- Python-based extensions
- Serverless function integrations
- Custom API endpoints

## API & SDK Development

Build custom integrations and tools using LimaCharlie's REST API:

### Python SDK

```python
from limacharlie import Manager

lc = Manager(oid="your-org-id", secret_api_key="your-api-key")

# Query sensors
sensors = lc.sensors()

# Execute commands
sensor = lc.sensor("sensor-id")
sensor.task("os_processes")
```

### REST API

```bash
curl -X POST https://api.limacharlie.io/v1/org/YOUR_OID/sensors \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json"
```

## Version Control & CI/CD

Manage LimaCharlie configurations as code:

### Infrastructure as Code

Export and version control:

- D&R rules
- Output configurations
- Organization settings
- Service extensions

### CI/CD Integration

Automate deployment using:

- GitHub Actions
- GitLab CI
- Jenkins
- CircleCI

Example GitHub Actions workflow:

```yaml
name: Deploy LimaCharlie Rules

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Deploy rules
        env:
          LC_API_KEY: ${{ secrets.LC_API_KEY }}
          LC_OID: ${{ secrets.LC_OID }}
        run: |
          python deploy_rules.py
```

## Best Practices

### Rule Development

- Start with broad detection logic, then refine based on false positives
- Use meaningful rule names and descriptions
- Document expected behavior and test cases
- Version control all rule changes

### Testing

- Test rules against diverse event datasets
- Validate both positive and negative test cases
- Monitor performance impact of complex rules
- Use staging environments before production deployment

### Performance

- Optimize rule efficiency to minimize sensor impact
- Use appropriate event types for detections
- Leverage caching for repeated lookups
- Monitor rule execution metrics

### Security

- Rotate API keys regularly
- Use least-privilege access controls
- Audit configuration changes
- Encrypt sensitive data in transit and at rest

## Development Tools

### LimaCharlie CLI

Command-line tool for managing LimaCharlie resources:

```bash
# Install
pip install limacharlie

# Configure
limacharlie login

# Deploy rules
limacharlie dr push ./rules/

# Test detections
limacharlie dr test --rule suspicious_powershell --event test_event.json
```

### VS Code Extension

LimaCharlie provides VS Code integration for:

- Syntax highlighting for D&R rules
- Rule validation and linting
- Integrated testing
- Deployment shortcuts

## Resources

- [API Documentation](https://api.limacharlie.io/static/swagger/)
- [Python SDK Reference](https://github.com/refractionPOINT/python-limacharlie)
- [D&R Rule Examples](https://github.com/refractionPOINT/rules)
- [Community Resources](https://community.limacharlie.io)

---

# DFIR

*This page appears to be a tag/category index page that currently contains no articles. The page indicates it's meant to organize documentation related to DFIR (Digital Forensics and Incident Response) topics.*

**Note:** This appears to be an empty category page. No actual documentation content is present beyond the tag label "dfir" and a message indicating no articles are currently tagged with this category.

---

# Enterprise SOC

LimaCharlie provides enterprise-grade Security Operations Center (SOC) capabilities designed for organizations that need to manage security at scale. The platform offers centralized management, multi-tenancy support, and advanced orchestration features that enable security teams to efficiently monitor and protect distributed environments.

## Multi-Tenancy Architecture

LimaCharlie's enterprise features are built on a robust multi-tenancy model that allows organizations to:

- Manage multiple organizations from a single pane of glass
- Implement role-based access control (RBAC) across tenants
- Apply configurations and policies consistently across multiple environments
- Segregate data and operations while maintaining centralized visibility

## Organization Management

### Parent-Child Relationships

Organizations can be structured hierarchically, enabling:

- Centralized billing and license management
- Policy inheritance from parent to child organizations
- Delegated administration with appropriate access controls
- Consolidated reporting and analytics across all child organizations

### Cross-Organization Operations

Administrators can perform operations across multiple organizations simultaneously:

- Deploy detection and response rules
- Push configuration changes
- Execute bulk operations
- Generate cross-organizational reports

## Enterprise Features

### Centralized Policy Management

- Define baseline security policies at the parent organization level
- Automatically propagate policies to child organizations
- Override policies at the child level when necessary
- Track policy compliance across all managed organizations

### Advanced Access Controls

- Integration with enterprise identity providers (SSO/SAML)
- Granular permission management
- API key management with scope restrictions
- Audit logging of all administrative actions

### Scalability

LimaCharlie's architecture is designed to handle enterprise-scale deployments:

- Support for hundreds of thousands of endpoints
- Horizontal scaling capabilities
- High-availability infrastructure
- Global data centers for optimal performance

## Compliance and Governance

Enterprise customers benefit from:

- SOC 2 Type II compliance
- Data residency options
- Retention policy management
- Comprehensive audit trails
- Export capabilities for compliance reporting

## Support and SLAs

Enterprise plans include:

- Dedicated support channels
- Priority response times
- Technical account management
- Custom SLA agreements
- Professional services for deployment and optimization

For more information about enterprise features and pricing, contact the LimaCharlie sales team.

---

# Endpoint Protection

## Overview

The Endpoint Protection Platform (EPP) extension provides traditional anti-malware capabilities within LimaCharlie. It combines signature-based detection with behavioral analysis to protect endpoints from known and emerging threats.

## Features

- Real-time file scanning
- Signature-based malware detection
- Integration with LimaCharlie's Detection & Response (D&R) rules
- Automated response actions
- Centralized management and reporting

## Installation

To enable EPP for your organization:

1. Navigate to the **Add-ons** section in the LimaCharlie web interface
2. Find **Endpoint Protection** in the available extensions
3. Click **Subscribe** to activate the extension
4. Configure your EPP policies and rules

## Configuration

### Basic Setup

EPP works by scanning files as they are accessed on protected endpoints. You can configure scanning behavior through D&R rules that trigger EPP scans.

### Creating EPP Scan Rules

Create Detection & Response rules to specify when and what to scan:

```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: exists
      path: event/FILE_PATH
    - op: matches
      path: event/FILE_PATH
      re: '.*\.exe$'

respond:
  - action: epp_scan
    metadata:
      path: event/FILE_PATH
```

### Response Actions

When malware is detected, EPP can trigger automated responses:

- **quarantine**: Isolate the malicious file
- **delete**: Remove the file from the system
- **report**: Log the detection without taking action
- **block**: Prevent the file from executing

Example response configuration:

```yaml
respond:
  - action: epp_scan
    metadata:
      path: event/FILE_PATH
      on_detection: quarantine
```

## Signature Updates

EPP signatures are automatically updated to protect against the latest threats. Updates occur in the background without requiring manual intervention or endpoint restarts.

## Integration with D&R

EPP seamlessly integrates with LimaCharlie's Detection & Response engine. You can:

- Create rules that trigger EPP scans based on behavioral indicators
- Combine EPP results with other telemetry for enriched detection logic
- Chain EPP scans with other response actions

## Monitoring and Alerts

EPP detections appear in your organization's detections feed and can trigger alerts through configured notification channels. Each detection includes:

- File hash (MD5, SHA1, SHA256)
- File path and name
- Detection signature name
- Threat severity level
- Timestamp and endpoint identifier

## Best Practices

1. **Layered Defense**: Use EPP alongside behavioral D&R rules for comprehensive protection
2. **Test Before Deploying**: Validate EPP rules in report-only mode before enabling automated responses
3. **Monitor False Positives**: Review detections regularly and tune rules to reduce false positives
4. **Regular Review**: Periodically audit your EPP configuration and update scanning policies as needed

## API Integration

EPP functionality is available through the LimaCharlie API, allowing you to:

- Trigger on-demand scans
- Retrieve detection results
- Manage EPP configurations programmatically

Example API call for on-demand scan:

```bash
curl -X POST https://api.limacharlie.io/v1/org/{oid}/extension/epp/scan \
  -H "Authorization: Bearer {jwt_token}" \
  -d '{
    "sid": "sensor_id",
    "path": "/path/to/file"
  }'
```

## Troubleshooting

### Scans Not Triggering

- Verify EPP extension is active for your organization
- Check that D&R rules targeting EPP are enabled
- Ensure sensors have network connectivity to receive signature updates

### High False Positive Rate

- Review detection signatures and adjust sensitivity
- Create allowlist rules for known-good files
- Consider using file reputation data to supplement EPP decisions

### Performance Impact

- EPP scanning is optimized for minimal performance impact
- Adjust scan triggers to balance security and performance
- Consider scanning only high-risk file types or locations

## Support

For EPP-related issues or questions, contact LimaCharlie support through the web interface or email support@limacharlie.io.

---

# GCP

## Articles

* [Velociraptor to BigQuery - 08 Oct 2025](/docs/velociraptor-to-bigquery)
* [Building Reports with BigQuery + Looker Studio - 07 Oct 2025](/docs/tutorials-reporting-building-reports-with-bigquery-looker-studio)
* [Google Cloud Pubsub - 07 Aug 2025](/docs/adapter-types-google-cloud-pubsub)
* [Google Cloud Storage - 07 Aug 2025](/docs/adapter-types-google-cloud-storage)
* [Tutorial: Ingesting Google Cloud Logs - 25 Apr 2025](/docs/tutorial-ingesting-google-cloud-logs)
* [Google Cloud BigQuery - 10 Dec 2024](/docs/outputs-destinations-google-cloud-bigquery)
* [Google Workspace - 31 Oct 2024](/docs/adapter-types-google-workspace)
* [Hayabusa to BigQuery - 15 Oct 2024](/docs/hayabusa-to-bigquery)
* [Google Cloud - 05 Oct 2024](/docs/ext-cloud-cli-google-cloud)
* [Google Cloud Storage - 05 Oct 2024](/docs/outputs-destinations-google-cloud-storage)
* [Google Cloud Pubsub - 05 Oct 2024](/docs/outputs-destinations-google-cloud-pubsub)

---

# Google Workspace Integration

## Google Workspace

**Last Updated: 31 Oct 2024**

[Documentation content for Google Workspace adapter would be found at: `/docs/adapter-types-google-workspace`]

## ChromeOS with Google Chrome Enterprise

**Last Updated: 05 Oct 2024**

[Documentation content for ChromeOS with Google Chrome Enterprise would be found at: `/docs/chrome-enterprise`]

---

**Note**: This appears to be a tag/category page listing articles rather than a full documentation page. To clean the actual documentation content, I would need access to the individual article pages:
- `/docs/adapter-types-google-workspace`
- `/docs/chrome-enterprise`

---

# Incident Response Documentation

This page contains articles related to incident response (IR) capabilities and workflows in LimaCharlie.

Currently, there are no articles tagged with "ir" in this documentation version.

*Note: This appears to be a tag index page. You may want to check the v2 documentation or browse other sections for incident response content.*

---

# LimaCharlie Tag

No articles found for this tag.

---

# Linux Tag - Documentation Articles

## Linux-Related Documentation

### Endpoint Agent

* [Endpoint Agent](/docs/endpoint-agent)
* [Endpoint Agent Events Overview](/docs/endpoint-agent-events-overview)
* [Endpoint Agent Commands](/docs/endpoint-agent-commands)
* [Endpoint Agent Installation](/docs/endpoint-agent-installation)
* [Endpoint Agent Uninstallation](/docs/endpoint-agent-uninstallation)

### Linux-Specific Installation

* [Linux Agent Installation](/docs/linux-agent-installation)
* [Docker Agent Installation](/docs/docker-agent-installation)

### Reference Documentation

* [Reference: EDR Events](/docs/reference-edr-events)
* [Reference: Endpoint Agent Commands](/docs/reference-endpoint-agent-commands)

### Integration & Logging

* [Ingesting Linux Audit Logs](/docs/ingesting-linux-audit-logs)
* [Syslog](/docs/adapter-types-syslog)
* [Kubernetes Pods Logs](/docs/adapter-types-kubernetes-pods-logs)

### Advanced Topics

* [Container Clusters](/docs/container-clusters)
* [Payloads](/docs/payloads)

---

# Lookup Manager

The Lookup Manager extension provides a centralized way to manage lookup tables in LimaCharlie. Lookup tables allow you to store reference data that can be used in Detection & Response (D&R) rules, pipelines, and other parts of the platform.

## Overview

Lookup tables are key-value stores that can be referenced during event processing. Common use cases include:

- Allowlists/denylists (IP addresses, domains, file hashes)
- Asset inventories (mapping hostnames to departments, criticality levels)
- Threat intelligence feeds
- Configuration data
- User/employee directories

## Creating a Lookup Table

1. Navigate to **Lookup Manager** in the LimaCharlie web interface
2. Click **Add Lookup**
3. Provide a unique name for your lookup table
4. (Optional) Add a description
5. Choose the data format:
   - **Key-Value**: Simple key-value pairs
   - **CSV**: Tabular data with columns
6. Click **Create**

## Adding Data to Lookups

### Via Web Interface

1. Open your lookup table
2. Click **Add Entry** or **Import**
3. For key-value lookups: enter key and value
4. For CSV lookups: upload a CSV file or paste CSV data
5. Click **Save**

### Via API

You can manage lookups programmatically using the LimaCharlie API:

```python
from limacharlie import Manager

# Initialize manager
lc = Manager(oid='YOUR_ORG_ID', secret_api_key='YOUR_API_KEY')

# Add entries to a lookup
lc.lookup_add('my-lookup', {
    'key1': 'value1',
    'key2': 'value2'
})
```

## Using Lookups in D&R Rules

Reference lookup tables in your Detection & Response rules using the `lookup()` function:

```yaml
detect:
  event: NEW_PROCESS
  op: lookup
  path: event/FILE_PATH
  resource: 'lcr://lookup/known-bad-hashes'

respond:
  - action: report
    name: known_malware_execution
```

### Lookup Operators

- `lookup`: Check if a value exists in a lookup table
- `lookup_with_default`: Return a default value if key not found
- `lookup_many`: Check multiple values against a lookup

Example with default value:

```yaml
detect:
  event: NEW_PROCESS
  op: lookup_with_default
  path: event/USER_NAME
  resource: 'lcr://lookup/user-departments'
  default: 'unknown'
  value: 'finance'
```

## Lookup Resources

Lookups are referenced using the LimaCharlie Resource (LCR) syntax:

```
lcr://lookup/LOOKUP_NAME
```

For example:
- `lcr://lookup/allowed-ips`
- `lcr://lookup/critical-assets`
- `lcr://lookup/threat-indicators`

## Managing Lookup Data

### Updating Entries

- Via UI: Edit individual entries in the lookup table interface
- Via API: Use `lookup_add()` to update existing keys (will overwrite)

### Deleting Entries

- Via UI: Select entries and click **Delete**
- Via API: Use `lookup_delete()` to remove specific keys

```python
# Delete specific keys
lc.lookup_delete('my-lookup', ['key1', 'key2'])
```

### Clearing a Lookup

To remove all entries from a lookup table:

```python
# Clear all entries
lc.lookup_clear('my-lookup')
```

## Best Practices

1. **Naming Convention**: Use descriptive, lowercase names with hyphens (e.g., `known-bad-domains`)
2. **Size Limits**: Keep lookup tables reasonably sized for performance. Very large datasets may be better suited for external integrations
3. **Updates**: Regularly update threat intelligence lookups to maintain accuracy
4. **Documentation**: Add descriptions to lookup tables explaining their purpose and data format
5. **Access Control**: Use appropriate permissions to control who can modify lookup tables
6. **Testing**: Test D&R rules that reference lookups to ensure proper behavior

## Automation

Automate lookup updates using the API in scheduled scripts:

```python
import requests
from limacharlie import Manager

def update_threat_feed():
    # Fetch latest threat intel
    response = requests.get('https://threat-feed.example.com/indicators.json')
    indicators = response.json()
    
    # Update lookup
    lc = Manager(oid='YOUR_ORG_ID', secret_api_key='YOUR_API_KEY')
    
    # Convert to key-value format
    lookup_data = {indicator['value']: indicator['type'] for indicator in indicators}
    
    # Update lookup (replaces all entries)
    lc.lookup_clear('threat-indicators')
    lc.lookup_add('threat-indicators', lookup_data)

# Run periodically via cron or similar
update_threat_feed()
```

## Performance Considerations

- Lookup operations are very fast (typically < 1ms)
- Lookups are cached in memory for quick access
- Large lookups (>100k entries) may impact memory usage
- Consider using external integrations for very large datasets (>1M entries)

## Troubleshooting

### Lookup Not Found Error

If you get a "lookup not found" error in a D&R rule:
1. Verify the lookup name is correct (case-sensitive)
2. Check that the lookup exists in your organization
3. Ensure proper LCR syntax: `lcr://lookup/LOOKUP_NAME`

### Performance Issues

If lookups are causing performance problems:
1. Check the size of your lookup tables
2. Consider splitting large lookups into smaller, more specific ones
3. Review how frequently lookups are being updated
4. Contact support for optimization guidance

---

# macOS

## Articles

### Ingesting MacOS Unified Logs
07 Oct 2025

### Reference: EDR Events
22 Sep 2025

### Reference: Endpoint Agent Commands
07 Aug 2025

### Mac Unified Logging
16 Jul 2025

### macOS Agent Installation - Latest Versions (macOS 15 Sequoia and newer)
11 Jul 2025

### macOS Agent Installation - Older Versions (macOS 10.15 Catalina to macOS 14 Sonoma)
11 Jul 2025

### macOS Agent Installation - MDM Configuration Profiles
16 Mar 2025

### macOS Agent Installation - Older Versions (macOS 10.14 and prior)
10 Dec 2024

### macOS Agent Installation via Jamf Now
01 Nov 2024

### Payloads
05 Oct 2024

### Endpoint Agent Uninstallation
05 Oct 2024

### Endpoint Agent Installation
07 Jul 2025

### macOS Agent Installation
01 Nov 2024

### Endpoint Agent Commands
18 Apr 2025

### Endpoint Agent
01 Nov 2024

### Endpoint Agent Events Overview
10 Dec 2024

---

# Security Service Providers (MSSP, MSP, MDR)

LimaCharlie was purpose-built for Security Service Providers like MSSPs, MSPs, and MDR providers. The platform's multi-tenant architecture, flexible pricing, and automation capabilities make it an ideal foundation for building and scaling security services.

## Key Capabilities for Service Providers

### Multi-Tenant Architecture

- **Organization Hierarchy**: Manage multiple client organizations from a single interface
- **Cross-Organization Views**: Monitor and manage security across all clients
- **Isolated Environments**: Each client organization maintains complete data isolation
- **Granular Access Control**: Define precise permissions for team members across organizations

### Flexible Business Models

LimaCharlie supports various service provider business models:

- **Reseller Model**: Purchase capacity and resell to clients with your own markup
- **White-Label**: Brand the platform as your own service
- **Hybrid Approach**: Combine multiple models based on client needs

### Automation at Scale

- **Infrastructure as Code**: Define security policies, detection rules, and configurations as code
- **Replicator**: Deploy configurations across multiple organizations with a single command
- **CI/CD Integration**: Automate deployment and updates through your existing pipelines
- **API-First Design**: Programmatically manage all aspects of the platform

### Cost-Effective Pricing

- **Usage-Based**: Only pay for what you use - no per-seat or per-sensor licensing
- **Predictable Margins**: Transparent pricing helps you forecast costs and margins
- **Volume Discounts**: Benefit from economies of scale as you grow

## Getting Started as a Service Provider

### 1. Account Structure

Create a hierarchical organization structure:

```
Your Company (Root Org)
 Client A
    Production Environment
    Development Environment
 Client B
    Production Environment
 Internal/Demo
     Testing Environment
```

### 2. Define Your Service Offering

Determine what capabilities you'll provide:

- **EDR/XDR**: Endpoint detection and response
- **SIEM**: Log collection and analysis
- **Threat Hunting**: Proactive threat detection
- **Incident Response**: Automated response actions
- **Compliance Monitoring**: Continuous compliance validation
- **Custom Integrations**: Connect to client-specific tools

### 3. Create Standard Configurations

Build reusable templates for:

- Detection & Response rules
- Output destinations (SIEM, ticketing, etc.)
- Sensor configurations
- Retention policies
- Alert routing

### 4. Implement Automation

Use the Replicator to deploy your standard configurations:

```yaml
# replicator.yaml
organizations:
  - name: client-a-prod
    detection_rules:
      - ./rules/standard-edr.yaml
      - ./rules/ransomware-detection.yaml
    outputs:
      - ./outputs/client-siem.yaml
    retention:
      events: 90d
      detections: 365d
```

Deploy across clients:

```bash
python -m limacharlie replicator apply -f replicator.yaml
```

### 5. Set Up Monitoring and Alerting

Configure centralized monitoring:

- **Outputs**: Route detections to your SOC tools
- **Webhooks**: Integrate with your ticketing system
- **Slack/Teams**: Real-time notifications
- **Custom Dashboards**: Build views for your analysts

## Best Practices

### Security Isolation

- Never share API keys between client organizations
- Use service accounts with least privilege access
- Enable audit logging for all administrative actions
- Implement multi-factor authentication for all users

### Configuration Management

- Store all configurations in version control
- Use CI/CD to deploy changes consistently
- Test changes in a dev/staging organization first
- Document your standard operating procedures

### Client Onboarding

Create a standardized onboarding process:

1. Create client organization
2. Deploy sensors to endpoints
3. Apply standard detection rules
4. Configure outputs and integrations
5. Set up custom rules (if needed)
6. Validate monitoring coverage
7. Provide client access (if applicable)

### Performance Optimization

- Use appropriate retention periods for each data type
- Leverage sampling for high-volume events
- Implement efficient detection rules
- Monitor your usage and costs regularly

## Advanced Features for Service Providers

### Custom Integrations

Build integrations using:

- **REST API**: Programmatic access to all platform features
- **Webhooks**: Real-time event notifications
- **Outputs**: Stream data to any destination
- **Extensions**: Custom capabilities using serverless functions

### Threat Intelligence

- **Built-in Feeds**: Access to LimaCharlie's threat intelligence
- **Custom Feeds**: Import your own threat intelligence
- **Sharing**: Share indicators across client organizations (with permission)

### Hunting and Investigation

- **Cross-Organization Queries**: Hunt across multiple clients
- **Historical Search**: Query retained telemetry
- **Live Terminal**: Interactive access to endpoints
- **Artifacts**: Collect forensic evidence

### Compliance and Reporting

- **Audit Logs**: Complete audit trail of all actions
- **Compliance Rules**: Automated compliance validation
- **Custom Reports**: Generate client-specific reports
- **Data Retention**: Configure retention per compliance requirements

## Pricing for Service Providers

LimaCharlie offers flexible pricing for service providers:

- **Volume Discounts**: Lower per-unit costs as you scale
- **Commit Discounts**: Save by committing to usage levels
- **Custom Agreements**: Tailored pricing for large deployments

Contact the sales team for service provider pricing: [https://limacharlie.io/contact](https://limacharlie.io/contact)

## Support and Resources

### Documentation

- [Getting Started Guide](/docs/getting-started)
- [API Reference](/docs/api)
- [Detection & Response Rules](/docs/detection-and-response)
- [Replicator Documentation](/docs/replicator)

### Community

- [Slack Community](https://slack.limacharlie.io): Connect with other service providers
- [GitHub](https://github.com/refractionpoint): Open-source tools and examples
- [Blog](https://limacharlie.io/blog): Best practices and updates

### Professional Services

LimaCharlie offers professional services for service providers:

- **Onboarding Assistance**: Get help setting up your practice
- **Custom Development**: Build custom integrations and features
- **Training**: Train your team on the platform
- **Architecture Review**: Optimize your deployment

## Example Use Cases

### MSSP: 24/7 Monitoring

An MSSP uses LimaCharlie to provide 24/7 security monitoring:

- Deploy EDR sensors to client endpoints
- Implement standard detection rules across all clients
- Route alerts to SOC's ticketing system
- Provide custom dashboards for each client
- Bill clients based on endpoint count

### MSP: Security Add-On

An MSP adds security to their service offering:

- Integrate LimaCharlie with RMM tools
- Provide ransomware protection to clients
- Monitor for software vulnerabilities
- Include security in existing monthly fees
- Scale offering as client base grows

### MDR: Managed Detection and Response

An MDR provider builds their service on LimaCharlie:

- Use advanced detection rules and threat intelligence
- Leverage artifact collection for investigations
- Implement automated response actions
- Provide 24/7 incident response
- Offer tiered service levels

## Next Steps

1. [Sign up for a LimaCharlie account](https://app.limacharlie.io/signup)
2. Review the [Getting Started Guide](/docs/getting-started)
3. Explore the [API Documentation](/docs/api)
4. Join the [Slack Community](https://slack.limacharlie.io)
5. Contact sales to discuss service provider pricing

---

# Tag: mergers and acquisitions

No Articles found for mergers and acquisitions Tag

---

# Multi-Tenancy and Service Provider Operations

## Security Service Providers (MSSP, MSP, MDR)

LimaCharlie's multi-tenancy architecture enables Managed Security Service Providers (MSSPs), Managed Service Providers (MSPs), and Managed Detection and Response (MDR) providers to manage multiple client organizations efficiently from a single platform.

### Key Capabilities

**Organization Management**
- Create and manage multiple client organizations from a parent account
- Centralized billing and license management
- Segregated data and access controls per organization
- White-label deployment options

**Access Control**
- Role-based access control (RBAC) across organizations
- Granular permissions at organization and resource levels
- API keys with scoped permissions
- Service accounts for automation

**Automation and Scale**
- Infrastructure-as-Code support via REST API and Terraform
- Bulk operations across multiple organizations
- Automated onboarding and configuration
- Standardized detection and response rules deployment

**Monitoring and Reporting**
- Unified dashboard view across all client organizations
- Per-organization analytics and reporting
- Centralized alerting and case management
- Custom reporting for client deliverables

### Architecture Patterns

**Parent-Child Organization Model**
Service providers operate a parent organization that can create and manage child organizations for each client. This enables:
- Centralized administration
- Standardized security configurations
- Isolated client data and access
- Consolidated billing

**Template-Based Deployment**
Create standardized security configurations and deploy them across multiple client organizations using:
- Detection and Response (D&R) rule templates
- Artifact collection templates
- Integration configurations
- Custom output configurations

### Best Practices

1. **Standardization**: Develop reusable templates for common configurations
2. **Automation**: Use APIs and Infrastructure-as-Code for consistent deployments
3. **Monitoring**: Implement centralized monitoring across all client organizations
4. **Access Control**: Apply principle of least privilege with RBAC
5. **Documentation**: Maintain configuration documentation for each client

---

# Observability

No articles found for the observability tag.

---

**Note:** This appears to be an empty tag/category page from the LimaCharlie documentation. There is no actual documentation content to extract - just the UI chrome indicating that no articles are currently tagged with "observability".

---

# Observability Pipeline

The Observability Pipeline is a feature that allows you to ingest, transform, and route observability data (logs, metrics, traces) through LimaCharlie.

## Overview

The Observability Pipeline enables you to:

- Ingest data from various sources
- Transform and enrich data in flight
- Route data to multiple destinations
- Apply filtering and sampling rules
- Normalize data formats

## Key Concepts

### Data Sources

The pipeline can accept data from multiple sources including:

- Syslog
- HTTP/HTTPS endpoints
- Cloud provider logs (AWS, Azure, GCP)
- Application logs
- Infrastructure metrics

### Transformations

Data can be transformed using:

- Field extraction and parsing
- Data enrichment
- Format conversion
- Filtering and sampling
- Aggregation

### Destinations

Processed data can be routed to:

- SIEM systems
- Log management platforms
- Data lakes
- Analytics tools
- Custom endpoints

## Configuration

Configure your pipeline through the LimaCharlie web interface or API. Define:

1. **Input sources** - where data comes from
2. **Processing rules** - how to transform data
3. **Output destinations** - where to send processed data

## Benefits

- **Cost Optimization**: Filter and sample data before sending to expensive destinations
- **Data Normalization**: Convert data to consistent formats
- **Enrichment**: Add context and metadata to raw events
- **Flexibility**: Route different data types to appropriate destinations
- **Scalability**: Handle high-volume data streams efficiently

---

# SecOps Development

SecOps Development in LimaCharlie provides a comprehensive platform for building, testing, and deploying security operations capabilities. This guide covers the key concepts and workflows for developing security solutions.

## Overview

LimaCharlie's SecOps development environment enables security teams to create custom detection and response rules, automate security workflows, and integrate with external tools and services.

## Key Components

### Detection & Response (D&R) Rules

D&R rules are the foundation of automated security operations in LimaCharlie. They allow you to:

- Detect security events in real-time
- Automatically respond to threats
- Create custom alert conditions
- Implement automated remediation

### FDR (Fast Detection & Response)

FDR provides high-performance event processing for security telemetry:

- Process events in real-time
- Apply complex filtering logic
- Route events to different destinations
- Enrich event data with context

### Output Streams

Output streams enable integration with external systems:

- Send events to SIEM platforms
- Forward alerts to ticketing systems
- Stream data to data lakes
- Integrate with SOAR platforms

## Development Workflow

### 1. Rule Development

Start by creating and testing D&R rules in your organization:

- Write rules using the D&R rule syntax
- Test rules against sample events
- Validate detection logic
- Refine response actions

### 2. Testing

Test your security logic before deployment:

- Use replay functionality to test against historical data
- Validate rule performance
- Check for false positives
- Verify response actions

### 3. Deployment

Deploy your security capabilities:

- Apply rules to specific sensors or organization-wide
- Configure output streams
- Set up integrations
- Monitor rule performance

## Best Practices

### Rule Design

- Keep rules focused on specific detection scenarios
- Use appropriate event types for efficiency
- Implement proper error handling
- Document rule logic and purpose

### Performance

- Optimize rule filters for performance
- Avoid overly broad detection criteria
- Use FDR for high-volume event processing
- Monitor resource usage

### Maintenance

- Regularly review and update rules
- Track false positive rates
- Document changes and versions
- Test rule modifications before deployment

## Development Resources

### Testing Tools

- Event replay for historical testing
- Rule simulator for logic validation
- Performance profiling tools
- Debugging capabilities

### Documentation

- D&R rule reference documentation
- Event schema definitions
- API documentation
- Integration guides

## Integration Capabilities

LimaCharlie supports various integration methods:

- REST API for programmatic access
- Webhooks for event notifications
- Output streams for data forwarding
- SDK libraries for custom development

## Collaboration

SecOps development in LimaCharlie supports team collaboration:

- Share rules across organizations
- Version control for rules
- Role-based access controls
- Audit logging for changes

---

I notice this appears to be a navigation/search results page showing "No Articles found for wel Tag" rather than an actual documentation page with content to clean.

This looks like it might be:
1. A tag search results page that returned no results
2. A navigation page
3. An error/empty state page

There's no actual documentation content to extract here - it's all UI chrome (logo, navigation menu, breadcrumbs, version switchers, and an empty state message).

**Output:**

```markdown
# No Content Found

This page does not contain documentation content to extract.
```

---

**However**, if you intended to clean a different documentation page, please provide the actual documentation content and I'll clean it according to the specifications above.

If you're running a pipeline that processes multiple pages, you might want to:
1. Skip pages with "No Articles found" messages
2. Filter out tag/category listing pages
3. Only process pages that contain actual documentation content

Would you like help with:
- Finding the correct documentation pages to process?
- Setting up filters to skip empty/navigation pages?
- Debugging why this particular page was selected?

---

# Windows

## Articles

### Ingesting Sysmon Event Logs
09 Oct 2025
[Link](/docs/ingesting-sysmon-event-logs)

### Ingesting Windows Event Logs
07 Oct 2025
[Link](/docs/ingesting-windows-event-logs)

### Reference: EDR Events
22 Sep 2025
[Link](/docs/reference-edr-events)

### EVTX
07 Aug 2025
[Link](/docs/adapter-types-evtx)

### Reference: Endpoint Agent Commands
07 Aug 2025
[Link](/docs/reference-endpoint-agent-commands)

### Windows Event Log
30 Jul 2025
[Link](/docs/adapter-types-windows-event-log)

### Endpoint Protection
28 May 2025
[Link](/docs/ext-epp)

### Windows Event Logs
28 May 2025
[Link](/docs/adapter-examples-windows-event-logs)

### IIS Logs
06 Dec 2024
[Link](/docs/adapter-types-iis)

### Sysmon Comparison
12 Nov 2024
[Link](/docs/sysmon-comparison)

### Ingesting Defender Event Logs
01 Nov 2024
[Link](/docs/ingesting-defender-event-logs)

### VDI & Virtual Machine Templates
01 Nov 2024
[Link](/docs/vdi-virtual-machine-templates)

### Payloads
05 Oct 2024
[Link](/docs/payloads)

### Endpoint Agent Uninstallation
05 Oct 2024
[Link](/docs/endpoint-agent-uninstallation)

### Agent Deployment via Microsoft Intune
05 Oct 2024
[Link](/docs/agent-deployment-microsoft-intune)

### Building a custom MSI installer for Windows
05 Oct 2024
[Link](/docs/building-a-custom-msi-installer-for-windows)

### Endpoint Agent Installation
07 Jul 2025
[Link](/docs/endpoint-agent-installation)

### Windows Agent Installation
05 Oct 2024
[Link](/docs/windows-agent-installation)

### Endpoint Agent Commands
18 Apr 2025
[Link](/docs/endpoint-agent-commands)

### Endpoint Agent
01 Nov 2024
[Link](/docs/endpoint-agent)

---

# Release Notes

**13 Oct 2025**

## Platform Updates

This page contains the latest updates, improvements, and new features for the LimaCharlie platform.

## Recent Changes

### October 13, 2025

**Platform Improvements:**
- Enhanced billing system with improved usage tracking
- Updated documentation structure for better navigation
- Performance optimizations across the platform

**Bug Fixes:**
- Resolved issues with billing calculations
- Fixed display issues in the web interface

### Security Updates

- Improved authentication mechanisms
- Enhanced API security measures
- Updated encryption protocols

## Previous Releases

For historical release notes and changelog information, please refer to the version-specific documentation in the sidebar navigation.

---

# Billing

**13 Oct 2025**

## Overview

LimaCharlie provides flexible billing options based on your usage and organizational needs.

## Billing Model

The platform uses a consumption-based billing model where you pay for what you use.

### Key Components

**Data Ingestion:**
- Charged based on the volume of data ingested into the platform
- Measured in gigabytes (GB)

**Sensor Deployments:**
- Billing calculated per active sensor
- Different tiers available based on sensor count

**Storage:**
- Retention policies affect storage costs
- Configurable retention periods

**API Usage:**
- API calls and requests contribute to overall usage
- Rate limits apply based on subscription tier

## Usage Tracking

Monitor your usage through the billing dashboard:
- Real-time usage metrics
- Historical usage data
- Cost projections
- Detailed breakdown by service

## Payment Methods

Supported payment options:
- Credit card
- Enterprise invoicing
- Annual prepayment options

## Billing Cycle

- Monthly billing by default
- Custom billing cycles available for enterprise customers
- Invoices generated at the end of each billing period

## Support

For billing inquiries or account-specific questions, contact the support team through the platform or via email.

---

# Release Notes

## October 2025

### Extension: Yara

**New behavior added**
- Added ability to manage Yara signatures from the Extension management page

### Ingestion

**New behavior added**
- Added ability to specify a `batch_size` payload for the REST, GCS, and S3 Adapters.

### Platform

**New behavior added**
- Log message stating rate limiting is enabled now only shows up when rate limiting is happening.

### Sensor

**New sensor release: 5.46.0**
- Improved parsing and cleanup of macOS `es_file_t` structures to reduce memory usage.
- Added support for Windows 11 24H2 DnsQuery telemetry.
- Limited event types sent from macOS sensors are now properly set.
- Starting with this version, you will need to manually enable the "limited event type" function through the sensor_config.

## September 2025

### Sigma Rules

**New behavior added**
- Sigma Rules now support user-provided configuration yamls to configure Sigma pipelines

### Extension: VirusTotal

**New behavior added**
- All lookups now have the ability to ingest in the "VT" stream for retention

### Soteria Rules

**New rules deployed:**
- `rules-soteria/windows/ransomware/bitlocker_encryption_detected.yaml`
- `rules-soteria/windows/ransomware/windows_shadow_copy_delete.yaml`
- `rules-soteria/windows/persistence/windows_firewall_rule.yaml`

### Extension: Artifact Collection

**Bug fix**
- Fixed an issue preventing the use of compiled Velociraptor artifacts in artifact collection

### Extension: Chronicler

**Bug fix**
- Fixed an issue where Chronicler was sending incomplete Replicant artifact results

## August 2025

### Platform

**New behavior added**
- New `get_tags` function added to Detection & Response rules to help retrieve tags from the Insight Service

### Extension: Net Inspect

**Bug fix**
- Fixed an issue preventing HTTPS traffic inspection when using a custom certificate

### Sensor

**New sensor release: 5.45.0**
- Enhanced macOS ES client authorization check
- Windows process tracking bug fixes
- Improved sensor disconnection handling
- Added support for Google Cloud Storage (GCS) payloads
- Add optional parameters to SERVICE_CHANGE event
- Fix Registry value caching
- Improved performance when using `deny_tree` functionality
- Fixed issue where DNS lookups were not correctly parsed on newer Windows versions

## July 2025

### Extension: Artifact Collection

**New behavior added**
- Can now run multiple artifact collections in parallel with scheduling support
- Artifact Collection results now available in the Replicant section of sensor pages

### Platform

**New behavior added**
- New organizations now default to sensor version 5.44.0
- Improved REST Adapter error messages

### Sensor

**New sensor release: 5.44.0**
- Add Linux eBPF support (preview)
- Enhanced memory usage tracking
- Added ability to list loaded kernel modules (Linux)
- macOS file event improvements
- Added ability to set sensor group through installation key

## June 2025

### Extension: Managed Detection and Response (MDR)

**New behavior added**
- Reduced false positive rate with improved detection logic
- Added automated response actions for high-severity threats

### Platform

**New behavior added**
- New API endpoints for managing detection rules
- Added support for webhooks in automated responses
- Improved search performance in historical data

### Extension: Cloud Sensors (AWS, GCP, Azure)

**New behavior added**
- Added support for Azure VM metadata ingestion
- GCP Cloud Function logs now automatically ingested
- AWS CloudTrail improvements for S3 and IAM events

## May 2025

### Soteria Rules

**New rules deployed:**
- `rules-soteria/linux/privilege_escalation/sudo_privilege_escalation.yaml`
- `rules-soteria/cloud/aws/aws_iam_policy_changed.yaml`
- `rules-soteria/cloud/gcp/gcp_service_account_key_created.yaml`

### Platform

**New behavior added**
- Added ability to export detection alerts to CSV
- Improved timeline visualization in Investigation view
- New keyboard shortcuts for common actions

### Sensor

**New sensor release: 5.43.0**
- Reduced CPU usage during high-volume event periods
- Added support for monitoring Docker containers (Linux)
- Improved network connection tracking accuracy
- Fixed issue with process tree reconstruction on macOS

## April 2025

### Extension: Replay

**New behavior added**
- Added ability to replay historical events through D&R rules for testing
- Support for replay speed control (1x, 2x, 5x, 10x)

### Platform

**Bug fix**
- Fixed issue where large timeline queries could timeout
- Corrected organization quota calculations

### Extension: Integrity

**New behavior added**
- Added support for monitoring Windows registry keys
- Integrity checks can now be scheduled at custom intervals

## March 2025

### Platform

**New behavior added**
- New Insight Service tags available: `malicious_ip`, `tor_exit_node`, `known_miner`
- Added ability to create custom dashboards
- Organization settings now support 2FA enforcement

### Extension: VirusTotal Livehunt

**New behavior added**
- Livehunt rules now support Yara modules
- Added notification options for Livehunt matches

### Sensor

**New sensor release: 5.42.0**
- Added Chrome extension monitoring capability
- Improved file integrity monitoring performance
- Fixed issue with DNS event collection on Ubuntu 24.04
- Added support for detecting RunDLL32 process injection

## February 2025

### Soteria Rules

**New rules deployed:**
- `rules-soteria/windows/credential_access/lsass_memory_dump.yaml`
- `rules-soteria/linux/execution/suspicious_cron_job.yaml`
- `rules-soteria/macos/persistence/launch_agent_creation.yaml`

### Platform

**New behavior added**
- Added bulk sensor management capabilities
- Improved API rate limiting with more granular controls
- New retention policies for Artifact Collection results

### Extension: Carbon Black

**New behavior added**
- Added support for Carbon Black Cloud Enterprise EDR
- Automatic alert synchronization with LimaCharlie detections

## January 2025

### Platform

**New behavior added**
- New Data Residency options: EU-Central, Asia-Pacific
- Added support for custom SSL certificates in Adapters
- Improved error messages across all API endpoints

### Sensor

**New sensor release: 5.41.0**
- Added Apple Silicon native support (M1/M2/M3)
- Improved Windows kernel driver stability
- Added support for monitoring Kubernetes pods
- Fixed issue with network event deduplication

### Extension: SIEM Adapters

**New behavior added**
- Added native Splunk HEC adapter
- Google Chronicle adapter now supports custom parsers
- Improved performance for high-volume log forwarding

---

# YARA

LimaCharlie supports YARA for pattern matching and malware detection through both sensor commands and extensions.

## YARA Sensor Command

The YARA sensor command allows you to scan files on endpoints using YARA rules.

### Usage

```
yara_scan <rule_name> <target_path>
```

**Parameters:**
- `rule_name`: The name of the YARA rule to use for scanning
- `target_path`: The file or directory path to scan

### Example

```
yara_scan malware_detection C:\Users\*\Downloads\*
```

## YARA Extension

LimaCharlie provides a third-party YARA extension for more advanced scanning capabilities.

### Features

- Automated YARA scanning across your fleet
- Integration with YARA rule repositories
- Custom rule management
- Continuous monitoring and alerting

### Configuration

The YARA extension can be configured through the LimaCharlie web interface or API to:

- Define scanning schedules
- Specify target paths
- Set up alerting rules
- Manage YARA rule sets

## Building Reports with BigQuery + Looker Studio

You can create comprehensive reports by exporting LimaCharlie data to BigQuery and visualizing it with Looker Studio.

### Setup Steps

1. Configure BigQuery export in LimaCharlie
2. Connect BigQuery to Looker Studio
3. Create custom dashboards and reports
4. Set up automated report delivery

### Use Cases

- Security metrics and KPIs
- Threat detection trends
- Compliance reporting
- Incident response analytics

## Ingesting Windows Event Logs

LimaCharlie can ingest and process Windows Event Logs for enhanced visibility and detection.

### Configuration

Enable Windows Event Log collection through:

```yaml
event_logs:
  - channel: Security
    enabled: true
  - channel: System
    enabled: true
  - channel: Application
    enabled: true
```

### Supported Event Channels

- Security
- System
- Application
- PowerShell
- Windows Defender
- Custom channels

### Detection and Response

Use D&R rules to:
- Detect suspicious authentication events
- Monitor privilege escalation
- Track application crashes
- Identify malware activity

## Ingesting MacOS Unified Logs

LimaCharlie supports ingesting MacOS Unified Logs for comprehensive endpoint visibility on MacOS systems.

### Configuration

Enable MacOS Unified Log collection:

```yaml
unified_logs:
  enabled: true
  predicates:
    - 'eventType == "logEvent"'
    - 'processImagePath CONTAINS "suspicious"'
```

### Log Filtering

Use predicates to filter logs by:
- Process name
- Event type
- Subsystem
- Category
- Log level

### Detection Capabilities

Create D&R rules to detect:
- Suspicious process execution
- Authentication events
- System modifications
- Application behavior

---

# LimaCharlie CLI

LimaCharlie CLI Extension allows you to issue [LimaCharlie CLI commands](/v2/docs/limacharlie-sdk) using extension requests.

Repo - <https://github.com/refractionPOINT/python-limacharlie>

You may use a rule to trigger a LimaCharlie CLI event. For example the following rule response actions:

```
- action: extension request
  extension action: run
  extension name: limacharlie-cli
  extension request:
    command_line: '{{ "limacharlie configs push --dry-run --oid" }}'
    credentials: '{{ "hive://secret/secret-name" }}'
```

## Command-line Interface

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

---

# LimaCharlie Core Concepts

### Sensors

#### Endpoint Agents

The LimaCharlie endpoint agent is a cross platform endpoint Sensor. It is a low-level, light-weight sensor which executes detection and response functionality in real-time.

The sensor provides a wide range of advanced capability.

* Flight Data Recorder (FDR) type functionality like Processes, Network Connections, Domain Name requests etc.
* Host isolation, automated response rules, intelligent local caching of events for in-depth Incident Response (IR) as well as some forensic features like dumping memory.

Sensors are designed to limit the potential for abuse resulting from unauthorized access to the LimaCharlie platform. This is achieved by limiting open-ended commands which might enable an attacker to covertly upload malicious software to your hosts. This means the LimaCharlie sensor is extremely powerful but also keeps its "read-only" qualities on your infrastructure. Of course, all access and interactions with the hosts are also logged for audit both within the cloud and tamper-proof forwarding to your own infrastructure.

Full commands list is in the [Endpoint Agent Commands](/v2/docs/endpoint-agent-commands) section.

#### Adapters

The LimaCharlie Adapter allows for real-time ingestion of any structured data, such as logs or telemetry, into the LimaCharlie platform, treating it as a first-class data source. This enables users to apply detection and response rules or send data to other outputs. Adapters support formats like JSON, Syslog, and CEFL, and can be deployed on-premise or cloud-to-cloud, either with or without the EDR sensor. For known sources like cloud platforms or Windows Event Logs, built-in mappings simplify data ingestion. Text-based Adapters allow for custom mapping and automation of any structured text. Additionally, pre-defined Adapters offer guided setups for common data sources like AWS CloudTrail and GuardDuty, while specialized connectors like Office 365 and Slack are supported with detailed configuration guidance. Some cloud-to-cloud Adapters, such as AWS S3, delete data after ingestion, so dedicated buckets with proper permissions are recommended.

### Installation Keys

Installation Keys are used to install a sensor. By specifying a key during installation the sensor can cryptographically be tied to your account.

Get more details in the [Installation Keys section](/v2/docs/installation-keys).

### Tags

Sensors can have Tags associated with them. Tags are added during creation or dynamically through the UI, API or Detection & Response Rules.

Get more information in the [Sensor tags section](/v2/docs/sensor-tags).

### Detection & Response Rules

The Detection & Response Rules act as an automation engine. The Detection component is a rule that either matches an event or not. If the Detection component matches, the Response component of the rule is actioned. This can be used to automatically investigate, mitigate or apply Tags.

Detailed explanation in the [Detection & Response section](/v2/docs/detection-and-response).

### Insight

Insight is our built-in data retention and search feature. It is included within our 2 sensor free tier as well.

When you enable Insight, we configure everything for you so that you get access to one year of your data for visualization and searching.

You don't *have to* use the built-in data retention; you can forward data directly to your infrastructure if preferred. However, it is generally much simpler and a better experience to use Insight. If you prefer not to use Insight, go through the next section (Outputs).

### Outputs

If you are using Insight (data retention) this section is optional.

LimaCharlie can relay the data somewhere for longer term storage and analysis. Where that data is sent depends on which Outputs are activated. You can have as many Output modules active as you want, so you can send it to multiple syslog destinations using the Syslog Output module and then send it to some cold storage over an Scp Output module.

Output is also split between four categories:

* event
* detect
* audit
* deployment

Selecting a Stream when creating an Output will select the relevant type of data to flow through it.

More details and exact configuration possibilities in the [Outputs section](/v2/docs/outputs).

### API Keys

The API keys are represented as UUIDs. They are linked to your specific organization and enable you to programmatically acquire authorization tokens that can be used on our REST API. See the [API key section](/v2/docs/api-keys) for more details.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Lookup Manager

The Lookup Manager Extension allows you to create, maintain & automatically refresh lookups in the Organization to then reference them in Detection & Response Rules.

The saved Lookup Configurations can be managed across tenants using Infrastructure as Code extension. To manage lookup versions across all of your tenants, update the file under the original Authenticated Resource Locator.

Every 24 hours, LimaCharlie will sync all of the lookups in the configuration. Lookups can also be manually synced by clicking the `Manual Sync` button on the extension page. When a lookup configuration is added, it will **not** be automatically synced immediately, unless you click on `Manual Sync`.

Lookup sources can be either direct links (URLs) to a given lookup or [ARLs](/v2/docs/reference-authentication-resource-locator).

Example JSON lookup: [link](https://loldrivers.io/api/drivers.json)

## Usage

### Option 1: Preconfigured Lookups

LimaCharlie provides a curated list of several publicly available JSON lookups for use within your organization. These are provided in the lookup manager GUI.

More details and the contents of each of these lookups can be found [here](https://github.com/refractionpoint/lc-public-lookups).

### Option 2: Publicly available Lookups

Giving the lookup configuration a name, the URL *or* [ARL](/v2/docs/reference-authentication-resource-locator), and clicking the Save button will create the new lookup source to sync to your lookups.

`[github,my-org/my-repo-name/path/to/lookup]`

### Option 3: Private Lookup Repository

To use a lookup from a private Github repository you will need to make use of an [Authentication Resource Locator](/v2/docs/reference-authentication-resource-locator).

**Step 1: Create a token in GitHub**

In GitHub go to *Settings* and click *Developer settings* in the left hand side bar.

Next click *Personal access token* followed by *Generate new token*. Select repo permissions and finally *Generate token*.

**Step 2: Connect LimaCharlie to your GitHub Repository**

Inside of LimaCharlie, click on *Lookup Manager* in the left hand menu. Then click *Add New Lookup Configuration*.

Give your lookup a name and then use the token you generated with the following format linked to your repository.

`[github,my-org/my-repo-name/path/to/lookup,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

## Infrastructure as Code

Example:

```
hives:
    extension_config:
        ext-lookup-manager:
            data:
                lookup_manager_rules:
                    - arl: ""
                      format: json
                      name: alienvault
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/alienvault-ip-reputation.json]'
                      tags:
                        - alienvault
                    - arl: ""
                      format: json
                      name: tor
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/tor-ips.json]'
                      tags:
                        - tor
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: ""
```

---

# Lookup Manager

The Lookup Manager Extension allows you to create, maintain & automatically refresh lookups in the Organization to then reference them in Detection & Response Rules.

The saved Lookup Configurations can be managed across tenants using Infrastructure as Code extension. To manage lookup versions across all of your tenants, update the file under the original Authenticated Resource Locator.

Every 24 hours, LimaCharlie will sync all of the lookups in the configuration. Lookups can also be manually synced by clicking the `Manual Sync` button on the extension page. When a lookup configuration is added, it will **not** be automatically synced immediately, unless you click on `Manual Sync`.

Lookup sources can be either direct links (URLs) to a given lookup or [ARLs](/v2/docs/reference-authentication-resource-locator).

Example JSON lookup: [link](https://loldrivers.io/api/drivers.json)

## Usage

### Option 1: Preconfigured Lookups

LimaCharlie provides a curated list of several publicly available JSON lookups for use within your organization. These are provided in the lookup manager GUI.

More details and the contents of each of these lookups can be found [here](https://github.com/refractionpoint/lc-public-lookups).

### Option 2: Publicly available Lookups

Giving the lookup configuration a name, the URL *or* [ARL](/v2/docs/reference-authentication-resource-locator), and clicking the Save button will create the new lookup source to sync to your lookups.

`[github,my-org/my-repo-name/path/to/lookup]`

### Option 3: Private Lookup Repository

To use a lookup from a private Github repository you will need to make use of an [Authentication Resource Locator](/v2/docs/reference-authentication-resource-locator).

**Step 1: Create a token in GitHub**

In GitHub go to *Settings* and click *Developer settings* in the left hand side bar.

Next click *Personal access token* followed by *Generate new token*. Select repo permissions and finally *Generate token*.

**Step 2: Connect LimaCharlie to your GitHub Repository**

Inside of LimaCharlie, click on *Lookup Manager* in the left hand menu. Then click *Add New Lookup Configuration*.

Give your lookup a name and then use the token you generated with the following format linked to your repository.

`[github,my-org/my-repo-name/path/to/lookup,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

## Infrastructure as Code

Example:

```
hives:
    extension_config:
        ext-lookup-manager:
            data:
                lookup_manager_rules:
                    - arl: ""
                      format: json
                      name: alienvault
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/alienvault-ip-reputation.json]'
                      tags:
                        - alienvault
                    - arl: ""
                      format: json
                      name: tor
                      predefined: '[https,storage.googleapis.com/lc-lookups-bucket/tor-ips.json]'
                      tags:
                        - tor
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: ""
```

---

# Lookups

Creating a lookup enables you to create a list that you can use as part of [detection and response rules](/v2/docs/detection-and-response). Once in place, you can refer to it using the `op: lookup` rule with a reference to your add-on looking like `resource: hive://lookup/my-lookup-name`.

Lookups support a few structures:

* Newline-separated values.
* JSON dictionary where keys are the elements of the lookup and the values are the metadata associated.
* YAML dictionary where keys are the elements of the lookup and the values are the metadata associated.
* OTX JSON Pulse.
* MISP JSON Feed.

Here is an example of this complex format:

```
evil.com: some evil website, definitely bad
example.com:
  source: my-threat-intel
  risk: high
  contact: email threatintel@mycorp.com immediately if spotted
```

When uploaded, the data for the lookup can be provided in three different ways:

1. As data literal in the upload API.
2. As a URL callback, where your data is a URL like https://www.my.data.
3. As an [Authenticated Resource Locator](/v2/docs/reference-authentication-resource-locator).

The maximum size of a lookup is 15MB through the REST API and 512KB through the web interface.

## Optimized Format

When creating a lookup, you may want to include correct metadata for each element of the lookup. However, adding metadata may result in issues due to the maximum size. In cases where there is a lot of metadata repetition, you may use an Optimized Format that will allow you to associate large pieces of metadata with a high number of Lookup items.

To accomplish this, you will need to split up your metadata from your lookup values like:

```
{
  "_LC_METADATA": [
    {
      "some": "metadata",
      ...
    }, {
      "some": "moremetadata",
      ...
    }, {
      "somemore": "metadata",
      ...
    }
  ],
  "_LC_INDICATORS: {
    "evil.exe": 0,
    "another.exe": 0,
    "more.exe": 1,
    "vals.exe": 2,
    ...
  }
}
```

The `_LC_METADATA` key has as a value, a list of all the pieces of metadata you want to include.

The `_LC_INDICATORS` is the normal list of indicators, but instead of having the metadata directly associated with each indicator as the value, it uses an integer that refers to the `_LC_METADATA` list's index where the metadata can be found.

The above example is equivalent to the non-optimized:

```
{
  "evil.exe": {
      "some": "metadata",
      ...
    },
  "another.exe": {
      "some": "metadata",
      ...
    },
  "more.exe": {
      "some": "moremetadata",
      ...
    },
  "vals.exe": {
      "somemore": "metadata",
      ...
    },
}
```

As you can see, this optimization is useful to reduce the repeated metadata. This is particularly useful if, for example, you have large numbers of IoCs for a given actor. In that case, every IoC in the lookup would be associated with the same metadata (information about the actor).

### From MISP

When creating an add-on from MISP content, LimaCharlie expects the data to be a JSON document to have the following structure:

```
{
  "Event": {
    "uuid": "fa781e8e-4332-4ff7-8286-f44445fb6f3a",
    "Attribute": [
      {
        "uuid": "e9e6840a-ff90-4fbd-8ef1-f5b766adbbce",
        "value": "evil.com"
      },
      ...
    ]
  }
}
```

The MISP event above once ingested in LC will be transformed to a Lookup like:

```
{
  "evil.com": {
    "misp_event": "fa781e8e-4332-4ff7-8286-f44445fb6f3a",
    "attribute": "e9e6840a-ff90-4fbd-8ef1-f5b766adbbce"
  },
  ...
}
```

LimaCharlie understand the MISP format, regardless of how it is ingested. That being said, the classic way of ingesting it would be to ingest the MSIP Events use an [ARL](https://github.com/refractionPOINT/authenticated_resource_locator) on a MISP REST API with one of the supported ARL authentication types like `basic`.

For example: `[https,misp.my.corp.com/events/1234,basic,myuser:mypassword]`.

### Reference D&R Rules

To put a Lookup "into effect", you need a [detection and response rule](/v2/docs/detection-and-response). The Lookup is a list of elements while the rule describes what you want to look for in that list.

Below is a list of D&R rules describing how to lookup various common Indicators of Compromise:

**Hashes**

```
op: lookup
event: CODE_IDENTITY
path: event/HASH
resource: 'hive://lookup/my-hash-lookup'
```

**Domain Names**

```
op: lookup
event: DNS_REQUEST
path: event/DOMAIN_NAME
resource: 'hive://lookup/my-dns-lookup'
```

**IP Addresses**

```
op: lookup
event: NETWORK_CONNECTIONS
path: event/NETWORK_ACTIVITY/?/IP_ADDRESS
resource: 'hive://lookup/my-ip-lookup'
```

---

# M&A Cyber Due Diligence

LimaCharlie makes merger and acquisition (M&A) cyber due diligence cost-effective and easy to deploy, allowing you to only pay for the resources you need. However, our extensible platform allows you to add as many tools as needed, all within a single agent, streamlining the assessment process.

## M&A problems

* **Inefficiencies in evaluating risks and vulnerabilities:** During the M&A process, it's crucial to identify and assess the cybersecurity risks associated with the target company's digital assets, including intellectual property, customer data, and financial information.
* **Complex infrastructures:** Merging two organizations' IT infrastructures can be a complex and time-consuming process which requires a smooth and secure integration of the acquired company's network into the parent company's infrastructure.
* **Limited visibility and control:** After the M&A process is complete, it's essential to continuously monitor and manage the merged organization's cybersecurity posture.

## LimaCharlie's solution

* **Cost-effective and scalable:** LimaCharlie allows you to be a cost-effective, scalable solution to perform a compromise assessment on to-be-acquired networks, and evaluate pre-existing threats.
* **Continuous visibility and control:** Automated, detection-as-code integration with powerful rulesets like Sigma and YARA rules provide continuous visibility into the merged organization's infrastructure, enabling you to detect and respond to potential threats in real-time.
* **Integrate with any tool:** Our extensible platform allows you to customize your additional tools, all through a single agent, achieving maximum performance with minimal footprint.
* **Centralized telemetry:** LimaCharlie's centralized platform provides a single pane of glass for monitoring and managing the merged infrastructure, simplifying the integration process and reducing the risk of security gaps.

---

# MCP Server

## Overview

The Model Context Protocol (MCP) is a standardized protocol used by AI Agents to access and leverage external tools and resources.

Note that MCP itself is still experimental and cutting edge.

LimaCharlie offers an MCP server at <https://mcp.limacharlie.io> which enables AI agents to:

* **Query and analyze** historical telemetry from any sensor
* **Actively investigate** endpoints using the LimaCharlie Agent (EDR) in real-time
* **Take remediation actions** like isolating endpoints, killing processes, and managing tags
* **Generate content** using AI-powered tools for LCQL queries, D&R rules, playbooks, and detection summaries
* **Manage platform configuration** including rules, outputs, adapters, secrets, and more
* **Access threat intelligence** through IOC searches and MITRE ATT&CK mappings

This opens up the entire LimaCharlie platform to AI agents, regardless of their implementation or location.

## Transport Modes

The server supports two transport modes based on the PUBLIC_MODE environment variable:

### STDIO Mode (PUBLIC_MODE=false, default)

Used for local MCP clients like Claude Desktop or Claude Code:

* Communication through stdin/stdout using JSON-RPC
* Uses LimaCharlie SDK's default authentication
* Reads credentials from environment variables or config files

### HTTP Mode (PUBLIC_MODE=true)

Used when deploying as a public service:

* Server runs as a stateless HTTP API with JSON responses
* Authentication via HTTP headers
* Supports multiple organizations concurrently
* Run with: `uvicorn server:app`

## Requirements & Authentication

### For HTTP Mode

The server requires authentication headers:

1. **Authorization header** in one of these formats:
   * `Authorization: Bearer <jwt>` (OID must be in x-lc-oid header)
   * `Authorization: Bearer <jwt>:<oid>` (combined format)
   * `Authorization: Bearer <api_key>:<oid>` (API key with OID)

2. **x-lc-oid header** (if not included in Authorization):
   * `x-lc-oid: <organization_id>`

### For STDIO Mode

Set environment variables:

* `LC_OID`: Your LimaCharlie Organization ID
* `LC_API_KEY`: Your LimaCharlie API key
* `GOOGLE_API_KEY`: For AI-powered generation features (optional)

## Capabilities

The LimaCharlie MCP server exposes over 100 tools organized by category:

### Investigation & Telemetry

* **Process inspection**: `get_processes`, `get_process_modules`, `get_process_strings`, `yara_scan_process`
* **System information**: `get_os_version`, `get_users`, `get_services`, `get_drivers`, `get_autoruns, get_packages`
* **Network analysis**: `get_network_connections`, `is_online`, `get_online_sensors`
* **File operations**: `find_strings`, `yara_scan_file`, `yara_scan_directory`, `yara_scan_memory`
* **Registry access**: `get_registry_keys`
* **Historical data**: `get_historic_events`, `get_historic_detections`, `get_time_when_sensor_has_data`

### Threat Response & Remediation

* **Network isolation**: `isolate_network`, `rejoin_network`, `is_isolated`
* **Sensor management**: `add_tag`, `remove_tag`, `delete_sensor`
* **Reliable tasking**: `reliable_tasking`, `list_reliable_tasks`

### AI-Powered Generation (requires GOOGLE_API_KEY)

* **Query generation**: `generate_lcql_query` - Create LCQL queries from natural language
* **Rule creation**: `generate_dr_rule_detection`, `generate_dr_rule_respond` - Generate D&R rules
* **Automation**: `generate_python_playbook` - Create Python playbooks
* **Analysis**: `generate_detection_summary` - Summarize detection data
* **Sensor selection**: `generate_sensor_selector` - Generate sensor selectors

### Platform Configuration

* **Detection & Response**: `get_detection_rules`, `set_dr_general_rule`, `set_dr_managed_rule`, `delete_dr_general_rule`
* **False Positive Management**: `get_fp_rules`, `set_fp_rule`, `delete_fp_rule`
* **YARA Rules**: `list_yara_rules`, `set_yara_rule`, `validate_yara_rule`, `delete_yara_rule`
* **Outputs & Adapters**: `list_outputs`, `add_output`, `delete_output`, `list_external_adapters`, `set_external_adapter`
* **Extensions**: `list_extension_configs`, `set_extension_config`, `delete_extension_config`
* **Playbooks**: `list_playbooks`, `set_playbook`, `delete_playbook`
* **Secrets Management**: `list_secrets`, `set_secret`, `delete_secret`
* **Saved Queries**: `list_saved_queries`, `set_saved_query`, `run_saved_query`
* **Lookups**: `list_lookups`, `set_lookup`, `query_lookup`, `delete_lookup`

### Threat Intelligence

* **IOC Search**: `search_iocs`, `batch_search_iocs`
* **Host Search**: `search_hosts`
* **MITRE ATT&CK**: `get_mitre_report`

### Administrative

* **API Keys**: `list_api_keys`, `create_api_key`, `delete_api_key`
* **Installation Keys**: `list_installation_keys`, `create_installation_key`, `delete_installation_key`
* **Cloud Sensors**: `list_cloud_sensors`, `set_cloud_sensor`, `delete_cloud_sensor`
* **Organization Info**: `get_org_info`, `get_usage_stats`
* **Artifacts**: `list_artifacts`, `get_artifact`

### Schema & Documentation

* **Event Schemas**: `get_event_schema`, `get_event_schemas_batch`, `get_event_types_with_schemas`
* **Platform Support**: `get_platform_names`, `list_with_platform`, `get_event_types_with_schemas_for_platform`

## Advanced Features

### Large Result Handling

The server automatically handles large responses by uploading them to Google Cloud Storage (if configured):

* Set `GCS_BUCKET_NAME` for the storage bucket
* Configure `GCS_TOKEN_THRESHOLD` (default: 1000 tokens)
* Results are returned as signed URLs valid for 24 hours

### LCQL Query Execution

The `run_lcql_query` tool supports:

* Streaming results for real-time monitoring
* Flexible time windows and limits
* Output formatting options

## Examples

### Claude Desktop/Code Configuration (STDIO)

```json
{
  "mcpServers": {
    "limacharlie": {
      "command": "python3",
      "args": ["/path/to/server.py"],
      "env": {
        "LC_OID": "your-org-id",
        "LC_API_KEY": "your-api-key",
        "GOOGLE_API_KEY": "your-google-api-key"
      }
    }
  }
}
```

### HTTP Service Usage

```bash
claude mcp add --transport http limacharlie https://mcp.limacharlie.io/mcp \
--header "Authorization: Bearer API_KEY:OID" \
--header "x-lc-oid: OID"
```

## Environment Variables

* `PUBLIC_MODE`: Set to true for HTTP mode, false for STDIO (default: false)
* `GOOGLE_API_KEY`: API key for AI-powered features
* `GCS_BUCKET_NAME`: Google Cloud Storage bucket for large results
* `GCS_SIGNER_SERVICE_ACCOUNT`: Service account for GCS URL signing
* `GCS_TOKEN_THRESHOLD`: Token count threshold for GCS upload (default: 1000)
* `GCS_URL_EXPIRY_HOURS`: Hours until GCS URLs expire (default: 24)
* `LC_OID`: Organization ID (STDIO mode only)
* `LC_API_KEY`: API key (STDIO mode only)

## Notes

* The server is stateless when running in HTTP mode
* HTTP mode uses JSON responses (not Server-Sent Events)
* No OAuth flow is used - authentication is via bearer tokens only
* If you encounter missing capabilities, contact <https://community.limacharlie.com> for quick additions

---

# MCP Server

## Overview

The Model Context Protocol (MCP) is a standardized protocol used by AI Agents to access and leverage external tools and resources.

Note that MCP itself is still experimental and cutting edge.

LimaCharlie offers an MCP server at <https://mcp.limacharlie.io> which enables AI agents to:

* **Query and analyze** historical telemetry from any sensor
* **Actively investigate** endpoints using the LimaCharlie Agent (EDR) in real-time
* **Take remediation actions** like isolating endpoints, killing processes, and managing tags
* **Generate content** using AI-powered tools for LCQL queries, D&R rules, playbooks, and detection summaries
* **Manage platform configuration** including rules, outputs, adapters, secrets, and more
* **Access threat intelligence** through IOC searches and MITRE ATT&CK mappings

This opens up the entire LimaCharlie platform to AI agents, regardless of their implementation or location.

## Transport Modes

The server supports two transport modes based on the PUBLIC_MODE environment variable:

### STDIO Mode (PUBLIC_MODE=false, default)

Used for local MCP clients like Claude Desktop or Claude Code:

* Communication through stdin/stdout using JSON-RPC
* Uses LimaCharlie SDK's default authentication
* Reads credentials from environment variables or config files

### HTTP Mode (PUBLIC_MODE=true)

Used when deploying as a public service:

* Server runs as a stateless HTTP API with JSON responses
* Authentication via HTTP headers
* Supports multiple organizations concurrently
* Run with: `uvicorn server:app`

## Requirements & Authentication

### For HTTP Mode

The server requires authentication headers:

1. **Authorization header** in one of these formats:

* `Authorization: Bearer <jwt>` (OID must be in x-lc-oid header)
* `Authorization: Bearer <jwt>:<oid>` (combined format)
* `Authorization: Bearer <api_key>:<oid>` (API key with OID)

2. **x-lc-oid header** (if not included in Authorization):

* `x-lc-oid: <organization_id>`

### For STDIO Mode

Set environment variables:

* `LC_OID`: Your LimaCharlie Organization ID
* `LC_API_KEY`: Your LimaCharlie API key
* `GOOGLE_API_KEY`: For AI-powered generation features (optional)

## Capabilities

The LimaCharlie MCP server exposes over 100 tools organized by category:

### Investigation & Telemetry

* **Process inspection**: `get_processes`, `get_process_modules`, `get_process_strings`, `yara_scan_process`
* **System information**: `get_os_version`, `get_users`, `get_services`, `get_drivers`, `get_autoruns`, `get_packages`
* **Network analysis**: `get_network_connections`, `is_online`, `get_online_sensors`
* **File operations**: `find_strings`, `yara_scan_file`, `yara_scan_directory`, `yara_scan_memory`
* **Registry access**: `get_registry_keys`
* **Historical data**: `get_historic_events`, `get_historic_detections`, `get_time_when_sensor_has_data`

### Threat Response & Remediation

* **Network isolation**: `isolate_network`, `rejoin_network`, `is_isolated`
* **Sensor management**: `add_tag`, `remove_tag`, `delete_sensor`
* **Reliable tasking**: `reliable_tasking`, `list_reliable_tasks`

### AI-Powered Generation (requires GOOGLE_API_KEY)

* **Query generation**: `generate_lcql_query` - Create LCQL queries from natural language
* **Rule creation**: `generate_dr_rule_detection`, `generate_dr_rule_respond` - Generate D&R rules
* **Automation**: `generate_python_playbook` - Create Python playbooks
* **Analysis**: `generate_detection_summary` - Summarize detection data
* **Sensor selection**: `generate_sensor_selector` - Generate sensor selectors

### Platform Configuration

* **Detection & Response**: `get_detection_rules`, `set_dr_general_rule`, `set_dr_managed_rule`, `delete_dr_general_rule`
* **False Positive Management**: `get_fp_rules`, `set_fp_rule`, `delete_fp_rule`
* **YARA Rules**: `list_yara_rules`, `set_yara_rule`, `validate_yara_rule`, `delete_yara_rule`
* **Outputs & Adapters**: `list_outputs`, `add_output`, `delete_output`, `list_external_adapters`, `set_external_adapter`
* **Extensions**: `list_extension_configs`, `set_extension_config`, `delete_extension_config`
* **Playbooks**: `list_playbooks`, `set_playbook`, `delete_playbook`
* **Secrets Management**: `list_secrets`, `set_secret`, `delete_secret`
* **Saved Queries**: `list_saved_queries`, `set_saved_query`, `run_saved_query`
* **Lookups**: `list_lookups`, `set_lookup`, `query_lookup`, `delete_lookup`

### Threat Intelligence

* **IOC Search**: `search_iocs`, `batch_search_iocs`
* **Host Search**: `search_hosts`
* **MITRE ATT&CK**: `get_mitre_report`

### Administrative

* **API Keys**: `list_api_keys`, `create_api_key`, `delete_api_key`
* **Installation Keys**: `list_installation_keys`, `create_installation_key`, `delete_installation_key`
* **Cloud Sensors**: `list_cloud_sensors`, `set_cloud_sensor`, `delete_cloud_sensor`
* **Organization Info**: `get_org_info`, `get_usage_stats`
* **Artifacts**: `list_artifacts`, `get_artifact`

### Schema & Documentation

* **Event Schemas**: `get_event_schema`, `get_event_schemas_batch`, `get_event_types_with_schemas`
* **Platform Support**: `get_platform_names`, `list_with_platform`, `get_event_types_with_schemas_for_platform`

## Advanced Features

### Large Result Handling

The server automatically handles large responses by uploading them to Google Cloud Storage (if configured):

* Set `GCS_BUCKET_NAME` for the storage bucket
* Configure `GCS_TOKEN_THRESHOLD` (default: 1000 tokens)
* Results are returned as signed URLs valid for 24 hours

### LCQL Query Execution

The `run_lcql_query` tool supports:

* Streaming results for real-time monitoring
* Flexible time windows and limits
* Output formatting options

## Examples

### Claude Desktop/Code Configuration (STDIO)

```
{
  "mcpServers": {
    "limacharlie": {
      "command": "python3",
      "args": ["/path/to/server.py"],
      "env": {
        "LC_OID": "your-org-id",
        "LC_API_KEY": "your-api-key",
        "GOOGLE_API_KEY": "your-google-api-key"
      }
    }
  }
}
```

### HTTP Service Usage

```
claude mcp add --transport http limacharlie https://mcp.limacharlie.io/mcp \
--header "Authorization: Bearer API_KEY:OID" \
--header "x-lc-oid: OID"
```

## Environment Variables

* `PUBLIC_MODE`: Set to true for HTTP mode, false for STDIO (default: false)
* `GOOGLE_API_KEY`: API key for AI-powered features
* `GCS_BUCKET_NAME`: Google Cloud Storage bucket for large results
* `GCS_SIGNER_SERVICE_ACCOUNT`: Service account for GCS URL signing
* `GCS_TOKEN_THRESHOLD`: Token count threshold for GCS upload (default: 1000)
* `GCS_URL_EXPIRY_HOURS`: Hours until GCS URLs expire (default: 24)
* `LC_OID`: Organization ID (STDIO mode only)
* `LC_API_KEY`: API key (STDIO mode only)

## Notes

* The server is stateless when running in HTTP mode
* HTTP mode uses JSON responses (not Server-Sent Events)
* No OAuth flow is used - authentication is via bearer tokens only
* If you encounter missing capabilities, contact <https://community.limacharlie.com> for quick additions

---

# Microsoft 365

The CLI for Microsoft 365 is a tool created to help manage Microsoft 365 tenant(s) and SharePoint framework projects. With this component of the Cloud CLI Extension, you can interact with a Microsoft 365 tenant(s) directly from LimaCharlie.

This extension makes use of the PnP Microsoft 365 CLI, which can be found [here](https://github.com/pnp/cli-microsoft365).

## Example

The following example disables the user account with the provided user ID.

```
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    cloud: '{{ "m365" }}'
    command_tokens:
      - entra
      - user
      - set
      - '--id'
      - '{{ .event.user_id  }}'
      - '--accountEnabled'
      - false
    credentials: '{{ "hive://secret/secret-name" }}'
```

## Credentials

* Per the Microsoft 365 CLI documentation, there are multiple login or authentication mechanisms available. The current LimaCharlie implementation utilizes a client secret for authentication. More information on provisioning client secrets can be found [here](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app).
* Upon invocation, LimaCharlie will first run the `m365 login` command with the credentials provided.
* Create a secret in the secrets manager in the following format:

  ```
  appID/clientSecret/tenantID
  ```

---

# Microsoft 365

The CLI for Microsoft 365 is a tool created to help manage Microsoft 365 tenant(s) and SharePoint framework projects. With this component of the Cloud CLI Extension, you can interact with a Microsoft 365 tenant(s) directly from LimaCharlie.

This extension makes use of the PnP Microsoft 365 CLI, which can be found [here](https://github.com/pnp/cli-microsoft365).

## Example

The following example disables the user account with the provided user ID.

```yaml
- action: extension request
  extension action: run
  extension name: ext-cloud-cli
  extension request:
    cloud: '{{ "m365" }}'
    command_tokens:
      - entra
      - user
      - set
      - '--id'
      - '{{ .event.user_id  }}'
      - '--accountEnabled'
      - false
    credentials: '{{ "hive://secret/secret-name" }}'
```

## Credentials

* Per the Microsoft 365 CLI documentation, there are multiple login or authentication mechanisms available. The current LimaCharlie implementation utilizes a client secret for authentication. More information on provisioning client secrets can be found [here](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app).
* Upon invocation, LimaCharlie will first run the `m365 login` command with the credentials provided.
* Create a secret in the secrets manager in the following format:

  ```
  appID/clientSecret/tenantID
  ```

---

# NIMS

Notion Incident Management System (NIMS) helps SOC/IR teams streamline their incident collaboration. While not a replacement for advanced SIEM or SOAR case management systems, it offers a practical alternative for teams that don't have access to these tools.

The Notion template uses interconnected relational databases to enable effective incident tracking and case management.

The LimaCharlie NIMS extension allows you to send detections from LimaCharlie to NIMS via the Notion API.

Once you subscribe an org to the extension, it creates a D&R rule that sends all detections from your org to your NIMS alert database. Because Notion databases do have a limit on the number of records, the extension also has the ability to purge old alerts that are 1) not associated with any incidents, and 2) older than the specified number of days. A D&R rule is also created to perform this cleanup automatically (or not) based on your configuration.

More information about NIMS, including the template and corresponding docs, can be found [here](https://nims-template.notion.site/).

## Configuration

In order to use this extension, you will need 3 pieces of data:

* Notion authentication token
* NIMS Alert database ID
* NIMS Asset database ID

### Find your database IDs

1. Navigate to the Alert database within NIMS under `Databases`
2. Right click on the database and click `Copy link`
3. Locate the database ID in the URL
   * The database ID is the long string of letters and numbers in the URL after the last `/` and before the `?` or `#` if present
   * Example:
     + Link: `https://www.notion.so/184cdc5a1ef3710badc2d2b1271aeb81?v=174cdc3a1ef181719981000cab12bf54&pvs=4`
     + ID: `184cdc5a1ef3710badc2d2b1271aeb81`
4. Copy the ID
5. Repeat the above for the Asset database

### Generate an auth token

This will walk you through creating a Notion integration, getting the auth token, and adding the integration to the proper NIMS databases.

While completing the following steps, be sure to add the connection to all 3 databasesAlert, Asset, and Incident. Incident is only necessary in order to perform the alerts cleanup to see whether or not the alert is tied to an incident.

1. Go to `Manage connections` in Notion
2. Click `Develop or manage integrations`
3. Click `New integration`
4. Configure the new integration
   * Give it a name, ex: `nims_template`
   * Choose the workspace
   * Type: `Internal`
   * Click `Save`
5. Click `Configure integration settings`
6. Copy the `Internal Integration Secret`-- this is your auth token
   * Click `Save`
7. Navigate to your `Alert Database`
   * Click the 3-dot menu and find `Connections`
   * Click on your newly created integration
8. Click `Confirm`
9. Repeat steps 7 and 8 for the `Asset Database` and the `Incident Database`

## Example D&R rule

**Detect:**

```
op: exists
path: cat
target: detection
```

**Respond:**

```
- action: extension request
  extension action: push_detections
  extension name: ext-nims
  extension request:
    cat: '{{ .cat }}'
    detection: '{{json .detect }}'
    event_time: '{{ .routing.event_time }}'
    hostname: '{{ .routing.hostname }}'
    int_ip: '{{ .routing.int_ip }}'
    link: '{{ .link }}'
    metadata: '{{json .detect_mtd }}'
```

---

# Network Monitoring

LimaCharlie's SecOps Cloud Platform, through its integration with Zeek, revolutionizes network security monitoring by providing scalable semantic analysis, seamless artifact ingestion, and powerful detection and response capabilities. By automating threat detection and enabling efficient incident investigation and response, LimaCharlie helps organizations stay ahead of evolving network threats and maintain a robust security posture.

## Network monitoring problems

* **Scalability issues:** Traditional monitoring tools struggle to keep up with the increased volume and variety of network traffic, leading to performance bottlenecks and reduced visibility.
* **Lack of semantic analysis:** Many network monitoring solutions focus primarily on capturing and storing network traffic data without providing deep, semantic analysis of the content, making it difficult to identify and understand sophisticated threats.
* **Limited automation and response capabilities:** Monitoring tools often lack advanced detection, automation, and response features, requiring manual intervention and slowing down incident response times.

## LimaCharlie's solution

* **Scalable semantic security monitoring:** By leveraging Zeek's robust platform, LimaCharlie enables organizations to perform semantic security monitoring at scale. The Zeek service automatically analyzes ingested PCAP files, extracting rich, structured data that provides deep insights into network activity and potential security threats.
* **Seamless integration with Artifact Ingestion:** LimaCharlie's Zeek extension seamlessly integrates with the platform's Artifact Ingestion system. As PCAP files are ingested, the Zeek service automatically processes them, generating detailed log files that are then re-ingested into the Artifact Ingestion system for further analysis and action.
* **Customizable Detection & Response (D&R) rules:** With the Zeek log files available as artifacts within LimaCharlie, security teams can create sophisticated D&R rules to automate threat detection and response. These rules can be customized to match an organization's specific security requirements, enabling rapid identification and mitigation of potential threats.
* **Efficient incident investigation and response:** LimaCharlie's integration with Zeek empowers security teams to perform efficient incident investigations by providing rich, contextual data about network activity. The platform's powerful search capabilities allow security teams to quickly identify relevant artifacts and take appropriate actions to contain and remediate threats.

---

# Observability Pipeline

The SecOps Cloud Platform (SCP) creates a scalable, versatile, and actionable observability pipeline by collecting and standardizing telemetry from the full security stack. Stream data from any input, route it to any output. The SCP provides visibility into telemetry sources and empowers users to create automated responses to actionable events in the pipeline.

## Observability pipeline problems

Creating an observability pipeline can be a daunting task as users try to integrate a complex and diverse technological environment into a single pipeline solution. When successful, ingesting, managing, and storing data can create significant costs, including:

* **Data costs:** Collecting and storing telemetry can be extremely expensive. As your business grows, so does its data, leading to escalating data storage costs as well.
* **Infrastructure demands:** Creating, managing, and monitoring the infrastructure required to operate an observability pipeline requires system engineers. As this infrastructure grows to accommodate your business, so does the headcount needed to maintain operations.
* **Delayed responsiveness:** Traditional observability pipelines collect and route data. If something appears in the pipeline that warrants concern, it must be routed to a destination for further analysis before action occurs.
* **High SIEM costs:** Data ingestion adds considerable costs to SIEM operations. As an organization expands its digital footprint these costs can increase rapidly.
* **Vendor lock-in constraints:** Many organizations find themselves trapped with security vendors who deliberately create dependencies through restrictive contracts, proprietary data formats, and closed ecosystems  limiting flexibility, driving up costs, and forcing security decisions based on vendor limitations rather than actual security needs.

## LimaCharlie's solutions

The SecOps Cloud Platform unifies telemetry collection by using an API-first approach for integrating the security stack. It creates a natural observability pipeline that scales without limit, facilitates automated responses, and greatly reduces data costs across the board. With the SCP you get a fully interactive observability pipeline that can facilitate countless other critical security operations as well.

* **Free data retention:** LimaCharlie offers a rolling year of free data storage.
* **Infrastructure-as-a-Service:** LimaCharlie provides a scalable, cloud-native infrastructure on an API-first platform. This gives our users maximum flexibility, scalability, and integration capabilities across the full security stack, including the observability pipeline.
* **Instant, bi-directional response:** LimaCharlie supports bi-directionality which allows automated responses sent directly to the source of a detection. For example, if the SecOps Cloud Platform receives a suspicious login alert from O365 it can immediately send a response to suspend the account before telemetry is sent for further processing.
* **Reduce SIEM spend:** LimaCharlie makes it easy to send only relevant telemetry to your SIEM, while still retaining all of your data in storage. This instantly reduces the costs of operating your SIEM while also accommodating any regulatory compliance requirements involving your data.
* **No vendor lock-in:** The API-first nature of LimaCharlie allows you to integrate and use whatever security solutions, services, and resources you prefer. There are no contracts or artificial barriers put in place to restrict your choices.

---

# Observability Pipeline

The SecOps Cloud Platform (SCP) creates a scalable, versatile, and actionable observability pipeline by collecting and standardizing telemetry from the full security stack. Stream data from any input, route it to any output. The SCP provides visibility into telemetry sources and empowers users to create automated responses to actionable events in the pipeline.

## Observability pipeline problems

Creating an observability pipeline can be a daunting task as users try to integrate a complex and diverse technological environment into a single pipeline solution. When successful, ingesting, managing, and storing data can create significant costs, including:

* **Data costs:** Collecting and storing telemetry can be extremely expensive. As your business grows, so does its data, leading to escalating data storage costs as well.
* **Infrastructure demands:** Creating, managing, and monitoring the infrastructure required to operate an observability pipeline requires system engineers. As this infrastructure grows to accommodate your business, so does the headcount needed to maintain operations.
* **Delayed responsiveness:** Traditional observability pipelines collect and route data. If something appears in the pipeline that warrants concern, it must be routed to a destination for further analysis before action occurs.
* **High SIEM costs:** Data ingestion adds considerable costs to SIEM operations. As an organization expands its digital footprint these costs can increase rapidly.
* **Vendor lock-in constraints:** Many organizations find themselves trapped with security vendors who deliberately create dependencies through restrictive contracts, proprietary data formats, and closed ecosystems  limiting flexibility, driving up costs, and forcing security decisions based on vendor limitations rather than actual security needs.

## LimaCharlie's solutions

The SecOps Cloud Platform unifies telemetry collection by using an API-first approach for integrating the security stack. It creates a natural observability pipeline that scales without limit, facilitates automated responses, and greatly reduces data costs across the board. With the SCP you get a fully interactive observability pipeline that can facilitate countless other critical security operations as well.

* **Free data retention:** LimaCharlie offers a rolling year of free data storage.
* **Infrastructure-as-a-Service:** LimaCharlie provides a scalable, cloud-native infrastructure on an API-first platform. This gives our users maximum flexibility, scalability, and integration capabilities across the full security stack, including the observability pipeline.
* **Instant, bi-directional response:** LimaCharlie supports bi-directionality which allows automated responses sent directly to the source of a detection. For example, if the SecOps Cloud Platform receives a suspicious login alert from O365 it can immediately send a response to suspend the account before telemetry is sent for further processing.
* **Reduce SIEM spend:** LimaCharlie makes it easy to send only relevant telemetry to your SIEM, while still retaining all of your data in storage. This instantly reduces the costs of operating your SIEM while also accommodating any regulatory compliance requirements involving your data.
* **No vendor lock-in:** The API-first nature of LimaCharlie allows you to integrate and use whatever security solutions, services, and resources you prefer. There are no contracts or artificial barriers put in place to restrict your choices.

---

# Payload Manager

[Payloads](/v2/docs/payloads), such as scripts, pre-built binaries, or other files, can be deployed to LimaCharlie sensors for any reason necessary.

One method of adding payloads to an Organization is via the web UI on the payloads screen. This is suitable for ad-hoc payload needs, however does not scale past a handful of payloads, or for multiple organizations requiring access the same payload(s).

The payload manager allows you to create, maintain, and automatically create/update payloads within your organization(s). Furthermore, payload configurations can be saved and utilized across multiple organizations using LimaCharlie's Infrastructure as Code capabilities.

Payloads added in the payload manager will be synced once every 24 hours per org.

## Key Concepts

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Infrastructure as Code (IaC)**: Infrastructure as Code automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Payloads

## Overview

Payloads are executables or scripts that can be delivered and executed through LimaCharlie's Endpoint Agent.

Those payloads can be any executable or script natively understood by the endpoint. The main use case is to run something with specific functionality not available in the main LimaCharlie functionality. For example: custom executables provided by another vendor to cleanup a machine, forensic utilities or firmware-related utilities.

We encourage you to look at LimaCharlie native functionality first as it has several advantages:

* Usually has better performance.
* Data returned is always well structured JSON.
* Can be tasked automatically and [Detection & Response Rules](/v2/docs/detection-and-response) can be created from their data.
* Data returned is indexed and searchable.

It is possible to set the Payload's file extension on the endpoint by making the Payload name end with that extension. For example, naming a Payload `extract_everything.bat`, the Payload will be sent as a batch file (`.bat`) and executed as such. This is also true for PowerShell files (`.ps1`).

## Lifecycle

Payloads are uploaded to the LimaCharlie platform and given a name. The task `run` can then be used with the `--payload-name MY-PAYLOAD --arguments "-v EulaAccepted"` can be used to run the payload with optional arguments.

The STDOUT and STDERR data will be returned in a related `RECEIPT` event, up to ~10 MB. If your payload generates more data, we recommend to pipe the data to a file on disk and use the `log_get` command to retrieve it.

The payload is retrieved by the endpoint agent over HTTPS to the Ingestion API DNS endpoint. This DNS entry is available from the Sensor Download section of the web app if you need to allow it.

## Upload / Download via REST

Creating and getting Payloads is done asynchronously. The relevant REST APIs will return specific signed URLs instead of the actual Payload. In the case of a retrieving an existing payload, simply doing an HTTP GET using the returned URL will download the payload content. When creating a Payload the returned URL should be used in an HTTP PUT using the URL like:

```
curl -X PUT "THE-SIGNED-URL-HERE" -H "Content-Type: application/octet-stream" --upload-file your-file.exe
```

Note that the signed URLs are only valid for a few minutes.

## Permissions

Payloads are managed with two permissions:

* `payload.ctrl` allows you to create and delete payloads.
* `payload.use` allows you to run a given payload.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Payloads

## Overview

Payloads are executables or scripts that can be delivered and executed through LimaCharlie's Endpoint Agent.

Those payloads can be any executable or script natively understood by the endpoint. The main use case is to run something with specific functionality not available in the main LimaCharlie functionality. For example: custom executables provided by another vendor to cleanup a machine, forensic utilities or firmware-related utilities.

We encourage you to look at LimaCharlie native functionality first as it has several advantages:

* Usually has better performance.
* Data returned is always well structured JSON.
* Can be tasked automatically and [Detection & Response Rules](/v2/docs/detection-and-response) can be created from their data.
* Data returned is indexed and searchable.

It is possible to set the Payload's file extension on the endpoint by making the Payload name end with that extension. For example, naming a Payload `extract_everything.bat`, the Payload will be sent as a batch file (`.bat`) and executed as such. This is also true for PowerShell files (`.ps1`).

## Lifecycle

Payloads are uploaded to the LimaCharlie platform and given a name. The task `run` can then be used with the `--payload-name MY-PAYLOAD --arguments "-v EulaAccepted"` can be used to run the payload with optional arguments.

The STDOUT and STDERR data will be returned in a related `RECEIPT` event, up to ~10 MB. If your payload generates more data, we recommend to pipe the data to a file on disk and use the `log_get` command to retrieve it.

The payload is retrieved by the endpoint agent over HTTPS to the Ingestion API DNS endpoint. This DNS entry is available from the Sensor Download section of the web app if you need to allow it.

## Upload / Download via REST

Creating and getting Payloads is done asynchronously. The relevant REST APIs will return specific signed URLs instead of the actual Payload. In the case of a retrieving an existing payload, simply doing an HTTP GET using the returned URL will download the payload content. When creating a Payload the returned URL should be used in an HTTP PUT using the URL like:

```
curl -X PUT "THE-SIGNED-URL-HERE" -H "Content-Type: application/octet-stream" --upload-file your-file.exe
```

Note that the signed URLs are only valid for a few minutes.

## Permissions

Payloads are managed with two permissions:

* `payload.ctrl` allows you to create and delete payloads.
* `payload.use` allows you to run a given payload.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Plaso

## About

[Plaso](https://plaso.readthedocs.io/) is a Python-based suite of tools used for creation of analysis timelines from forensic artifacts acquired from an endpoint.

These timelines are invaluable tools for digital forensic investigators and analysts, enabling them to effectively correlate the vast quantities of information encountered in logs and various forensic artifacts encountered in an intrusion investigation.

The primary tools in the Plaso suite used for this process are [log2timeline](https://plaso.readthedocs.io/en/latest/sources/user/Using-log2timeline.html), [psort](https://plaso.readthedocs.io/en/latest/sources/user/Using-psort.html), and [psteal](https://plaso.readthedocs.io/en/latest/sources/user/Using-psteal.html).

* `log2timeline` - bulk forensic artifact parser
* `psort` - builds timelines based on output from `log2timeline`
* `psteal` - Simply a wrapper for `log2timeline` and `psort`

The `ext-plaso` extension within LimaCharlie allows you to run `log2timeline` and `psort` (using the `psteal` wrapper) against artifacts obtained from an endpoint, such as event logs, registry hives, and various other forensic artifacts. When executed, Plaso will parse and extract information from all acquired evidence artifacts that it has support for. Supported parsers are found [here](https://plaso.readthedocs.io/en/latest/sources/user/Parsers-and-plugins.html).

### Plaso Extension Pricing

While it is free to enable the Plaso extension, pricing is applied to both the original downloaded artifact and the processed (Plaso) artifacts -- $0.02/GB for the original downloaded artifact, and $1.0/GB for the generation of the processed artifacts.

## Extension Configuration

> **Long Execution Times**
>
> Note that it can take **several minutes** for the plaso generation to complete for larger triage collections, but once it finishes you will see the results in the `ext-plaso` Sensor timeline, as well as the uploaded artifacts on the Artifacts page.

The `ext-plaso` extension runs `psteal` (`log2timeline` + `psort`) against the acquired evidence using the following commands:

1. ```
   psteal.py --source /path/to/artifact -o dynamic --storage-file $artifact_id.plaso -w $artifact_id.csv
   ```

Upon running `psteal.py`, a `.plaso` file and a `.csv` file are generated. They will be uploaded as LimaCharlie artifacts.

* Resulting `.plaso` file contains the raw output of `log2timeline.py`
* Resulting `.csv` file contains the CSV formatted version of the `.plaso` file contents

2. ```
   pinfo.py $artifact_id.plaso -w $artifact_id_pinfo.json --output_format json
   ```

After `psteal.py` runs, information is gathered from the resulting `.plaso` file using the `pinfo.py` utility and pushed into the `ext-plaso` sensor timeline as a `pinfo` event. This event provides a detailed summary with metrics of the processing that occurred, as well as any relevant errors you should be aware of.

The following events will be pushed to the `ext-plaso` sensor timeline:

* `job_queued`: indicates that `ext-plaso` has received and queued a request to process data
* `job_started`: indicates that `ext-plaso` has started processing the data
* `pinfo`: contains the `pinfo.py` output summarizing the results of the plaso file generation
* `plaso`: contains the `artifact_id` of the plaso file that was uploaded to LimaCharlie
* `csv`: contains the `artifact_id` of the CSV file that was uploaded to LimaCharlie

## Usage & Automation

LimaCharlie can automatically kick off evidence processing with Plaso based off of the artifact ID provided in a rule action, or you can run it manually via the extension.

### Velociraptor Triage Acquisition Processing

If you use the LimaCharlie [Velociraptor](/v2/docs/ext-velociraptor) extension, a good use case of `ext-plaso` would be to trigger Plaso evidence processing upon ingestion of a Velociraptor KAPE files artifact collection.

1. Configure a D&R rule to watch for Velociraptor collection events upon ingestion, and then trigger the Plaso extension:

   **Detect:**

   ```
   op: and
   target: artifact_event
   rules:
       - op: is
         path: routing/log_type
         value: velociraptor
       - op: is
         not: true
         path: routing/event_type
         value: export_complete
   ```

   **Respond:**

   ```
   - action: extension request
     extension action: generate
     extension name: ext-plaso
     extension request:
         artifact_id: '{{ .routing.log_id }}'
   ```

2. Launch a `Windows.KapeFiles.Targets` artifact collection in the LimaCharlie Velociraptor extension. This instructs Velociraptor to gather all endpoint artifacts defined in [this KAPE Target file](https://github.com/EricZimmerman/KapeFiles/blob/master/Targets/Compound/KapeTriage.tkape).

   **Argument options:**

   * `EventLogs=Y` - EventLogs only, quicker processing time for proof of concept
   * `KapeTriage=Y` - full [KapeTriage](https://github.com/EricZimmerman/KapeFiles/blob/master/Targets/Compound/KapeTriage.tkape) files collection

3. Once Velociraptor collects, zips, and uploads the evidence, the previously created D&R rule will send the triage `.zip` to `ext-plaso` for processing. Watch the `ext-plaso` sensor timeline for status and the Artifacts page for the resulting `.plaso` & `.csv` output files. See [Working with the Output](#working-with-the-output).

### MFT Processing

If you use the LimaCharlie [Dumper](/v2/docs/ext-dumper) extension, a good use case of `ext-plaso` would be to trigger Plaso evidence processing upon ingestion of a MFT CSV artifact.

1. Configure a D&R rule to watch for MFT collection events upon ingestion, and then trigger the Plaso extension:

   **Detect:**

   ```
   op: and
   target: artifact_event
   rules:
       - op: is
         path: routing/log_type
         value: mftcsv
       - op: is
         not: true
         path: routing/event_type
         value: export_complete
   ```

   **Respond:**

   ```
   - action: extension request
     extension action: generate
     extension name: ext-plaso
     extension request:
         artifact_id: '{{ .routing.log_id }}'
   ```

2. Launch an MFT dump in the LimaCharlie Dumper extension.

3. Once dumper is complete and uploads the evidence, the previously created D&R rule will send the zipped MFT CSV to `ext-plaso` for processing. Watch the `ext-plaso` sensor timeline for status and the Artifacts page for the resulting `.plaso` & `.csv` output files. See [Working with the Output](#working-with-the-output).

## Working with the Output

Running the extension generates the following useful outputs:

### pinfo on ext-plaso sensor timeline

First and foremost, after the completion of a processing job by `ext-plaso`, it is highly encouraged to analyze the resulting `pinfo` event on the `ext-plaso` sensor timeline. This event provides a detailed summary with metrics of the processing that occurred, as well as any relevant errors you should be aware of.

* Pay close attention to fields such as `warnings_by_parser` or `warnings_by_path_spec` which may reveal parser errors that were encountered.
* Sample output of `pinfo` showing counts of parsed artifacts nested under `storage_counters` -- this provides insight as to which, and how many events will be present in your CSV timeline.

```
"amcache": 986,
"appcompatcache": 4096,
"bagmru": 29,
"chrome_27_history": 29,
"chrome_66_cookies": 246,
"explorer_mountpoints2": 2,
"explorer_programscache": 1,
"filestat": 3495,
"lnk": 160,
"mft": 4790977,
"mrulist_string": 2,
"mrulistex_shell_item_list": 3,
"mrulistex_string": 5,
"mrulistex_string_and_shell_item": 5,
"mrulistex_string_and_shell_item_list": 1,
"msie_webcache": 143,
"msie_zone": 60,
"networks": 4,
"olecf_automatic_destinations": 37,
"olecf_default": 5,
"recycle_bin": 3,
"shell_items": 297,
"total": 5840430,
"user_access_logging": 34,
"userassist": 44,
"utmp": 13,
"windows_boot_execute": 8,
"windows_run": 10,
"windows_sam_users": 16,
"windows_services": 2004,
"windows_shutdown": 8,
"windows_task_cache": 835,
"windows_timezone": 4,
"windows_typed_urls": 3,
"windows_version": 6,
"winevtx": 382674,
"winlogon": 8,
"winreg_default": 654177
```

### Downloadable Artifacts

#### plaso artifact

The downloadable `.plaso` file contains the raw output of `log2timeline.py` and can be [imported into Timesketch](https://timesketch.org/guides/user/upload-data/) as a timeline.

#### csv artifact

The downloadable `.csv` file can be easily viewed in any CSV viewer, but a highly recommended tool for this is [Timeline Explorer](https://ericzimmerman.github.io/) from Eric Zimmerman.

---

# Plaso

## About

[Plaso](https://plaso.readthedocs.io/) is a Python-based suite of tools used for creation of analysis timelines from forensic artifacts acquired from an endpoint.

These timelines are invaluable tools for digital forensic investigators and analysts, enabling them to effectively correlate the vast quantities of information encountered in logs and various forensic artifacts encountered in an intrusion investigation.

The primary tools in the Plaso suite used for this process are [log2timeline](https://plaso.readthedocs.io/en/latest/sources/user/Using-log2timeline.html), [psort](https://plaso.readthedocs.io/en/latest/sources/user/Using-psort.html), and [psteal](https://plaso.readthedocs.io/en/latest/sources/user/Using-psteal.html).

* `log2timeline` - bulk forensic artifact parser
* `psort` - builds timelines based on output from `log2timeline`
* `psteal` - Simply a wrapper for `log2timeline` and `psort`

The `ext-plaso` extension within LimaCharlie allows you to run `log2timeline` and `psort` (using the `psteal` wrapper) against artifacts obtained from an endpoint, such as event logs, registry hives, and various other forensic artifacts. When executed, Plaso will parse and extract information from all acquired evidence artifacts that it has support for. Supported parsers are found [here](https://plaso.readthedocs.io/en/latest/sources/user/Parsers-and-plugins.html).

### Plaso Extension Pricing

While it is free to enable the Plaso extension, pricing is applied to both the original downloaded artifact and the processed (Plaso) artifacts -- $0.02/GB for the original downloaded artifact, and $1.0/GB for the generation of the processed artifacts.

## Extension Configuration

The `ext-plaso` extension runs `psteal` (`log2timeline` + `psort`) against the acquired evidence using the following commands:

1. ```
   psteal.py --source /path/to/artifact -o dynamic --storage-file $artifact_id.plaso -w $artifact_id.csv
   ```

Upon running `psteal.py`, a `.plaso` file and a `.csv` file are generated. They will be uploaded as LimaCharlie artifacts.

* Resulting `.plaso` file contains the raw output of `log2timeline.py`
* Resulting `.csv` file contains the CSV formatted version of the `.plaso` file contents

2. ```
   pinfo.py $artifact_id.plaso -w $artifact_id_pinfo.json --output_format json
   ```

After `psteal.py` runs, information is gathered from the resulting `.plaso` file using the `pinfo.py` utility and pushed into the `ext-plaso` sensor timeline as a `pinfo` event. This event provides a detailed summary with metrics of the processing that occurred, as well as any relevant errors you should be aware of.

The following events will be pushed to the `ext-plaso` sensor timeline:

* `job_queued`: indicates that `ext-plaso` has received and queued a request to process data
* `job_started`: indicates that `ext-plaso` has started processing the data
* `pinfo`: contains the `pinfo.py` output summarizing the results of the plaso file generation
* `plaso`: contains the `artifact_id` of the plaso file that was uploaded to LimaCharlie
* `csv`: contains the `artifact_id` of the CSV file that was uploaded to LimaCharlie

> **Note:** It can take **several minutes** for the plaso generation to complete for larger triage collections, but once it finishes you will see the results in the `ext-plaso` Sensor timeline, as well as the uploaded artifacts on the Artifacts page.

## Usage & Automation

LimaCharlie can automatically kick off evidence processing with Plaso based off of the artifact ID provided in a rule action, or you can run it manually via the extension.

### Velociraptor Triage Acquisition Processing

If you use the LimaCharlie [Velociraptor](/v2/docs/ext-velociraptor) extension, a good use case of `ext-plaso` would be to trigger Plaso evidence processing upon ingestion of a Velociraptor KAPE files artifact collection.

1. Configure a D&R rule to watch for Velociraptor collection events upon ingestion, and then trigger the Plaso extension:

   **Detect:**

   ```
   op: and
   target: artifact_event
   rules:
       - op: is
         path: routing/log_type
         value: velociraptor
       - op: is
         not: true
         path: routing/event_type
         value: export_complete
   ```

   **Respond:**

   ```
   - action: extension request
     extension action: generate
     extension name: ext-plaso
     extension request:
         artifact_id: '{{ .routing.log_id }}'
   ```

2. Launch a `Windows.KapeFiles.Targets` artifact collection in the LimaCharlie Velociraptor extension. This instructs Velociraptor to gather all endpoint artifacts defined in [this KAPE Target file](https://github.com/EricZimmerman/KapeFiles/blob/master/Targets/Compound/KapeTriage.tkape).

   **Argument options:**

   * `EventLogs=Y` - EventLogs only, quicker processing time for proof of concept
   * `KapeTriage=Y` - full [KapeTriage](https://github.com/EricZimmerman/KapeFiles/blob/master/Targets/Compound/KapeTriage.tkape) files collection

3. Once Velociraptor collects, zips, and uploads the evidence, the previously created D&R rule will send the triage `.zip` to `ext-plaso` for processing. Watch the `ext-plaso` sensor timeline for status and the Artifacts page for the resulting `.plaso` & `.csv` output files. See [Working with the Output](#working-with-the-output).

### MFT Processing

If you use the LimaCharlie [Dumper](/v2/docs/ext-dumper) extension, a good use case of `ext-plaso` would be to trigger Plaso evidence processing upon ingestion of a MFT CSV artifact.

1. Configure a D&R rule to watch for MFT collection events upon ingestion, and then trigger the Plaso extension:

   **Detect:**

   ```
   op: and
   target: artifact_event
   rules:
       - op: is
         path: routing/log_type
         value: mftcsv
       - op: is
         not: true
         path: routing/event_type
         value: export_complete
   ```

   **Respond:**

   ```
   - action: extension request
     extension action: generate
     extension name: ext-plaso
     extension request:
         artifact_id: '{{ .routing.log_id }}'
   ```

2. Launch an MFT dump in the LimaCharlie Dumper extension.

3. Once dumper is complete and uploads the evidence, the previously created D&R rule will send the zipped MFT CSV to `ext-plaso` for processing. Watch the `ext-plaso` sensor timeline for status and the Artifacts page for the resulting `.plaso` & `.csv` output files. See [Working with the Output](#working-with-the-output).

## Working with the Output

Running the extension generates the following useful outputs:

### pinfo on ext-plaso sensor timeline

First and foremost, after the completion of a processing job by `ext-plaso`, it is highly encouraged to analyze the resulting `pinfo` event on the `ext-plaso` sensor timeline. This event provides a detailed summary with metrics of the processing that occurred, as well as any relevant errors you should be aware of.

* Pay close attention to fields such as `warnings_by_parser` or `warnings_by_path_spec` which may reveal parser errors that were encountered.
* Sample output of `pinfo` showing counts of parsed artifacts nested under `storage_counters` -- this provides insight as to which, and how many events will be present in your CSV timeline.

```
"amcache": 986,
"appcompatcache": 4096,
"bagmru": 29,
"chrome_27_history": 29,
"chrome_66_cookies": 246,
"explorer_mountpoints2": 2,
"explorer_programscache": 1,
"filestat": 3495,
"lnk": 160,
"mft": 4790977,
"mrulist_string": 2,
"mrulistex_shell_item_list": 3,
"mrulistex_string": 5,
"mrulistex_string_and_shell_item": 5,
"mrulistex_string_and_shell_item_list": 1,
"mrulistex_string_and_shell_item_list": 1,
"msie_webcache": 143,
"msie_zone": 60,
"networks": 4,
"olecf_automatic_destinations": 37,
"olecf_default": 5,
"recycle_bin": 3,
"shell_items": 297,
"total": 5840430,
"user_access_logging": 34,
"userassist": 44,
"utmp": 13,
"windows_boot_execute": 8,
"windows_run": 10,
"windows_sam_users": 16,
"windows_services": 2004,
"windows_shutdown": 8,
"windows_task_cache": 835,
"windows_timezone": 4,
"windows_typed_urls": 3,
"windows_version": 6,
"winevtx": 382674,
"winlogon": 8,
"winreg_default": 654177
```

### Downloadable Artifacts

#### plaso artifact

The downloadable `.plaso` file contains the raw output of `log2timeline.py` and can be [imported into Timesketch](https://timesketch.org/guides/user/upload-data/) as a timeline.

#### csv artifact

The downloadable `.csv` file can be easily viewed in any CSV viewer, but a highly recommended tool for this is [Timeline Explorer](https://ericzimmerman.github.io/) from Eric Zimmerman.

---

# Playbook [LABS]

> LimaCharlie LABS

The Playbook Extension allows you to execute Python playbooks within the context of your Organization in order to automate tasks and customize more complex detections.

The playbooks themselves are managed in the playbook [Hive](/v2/docs/config-hive) Configurations and can be managed across tenants using the Infrastructure as Code extension.

The execution of a playbook can be triggered through the following means:

1. Interactively in the web app by going to the Extensions section for the Playbook extension.
2. By issuing an `extension request` action through a [D&R rule](/v2/docs/detection-and-response-examples).
3. By issuing an extension request on the API directly: <https://api.limacharlie.io/static/swagger/#/Extensions/createExtensionRequest>
4. By issuing an extension request through the Python CLI/SDK or Golang SDK.

This means playbooks can be issued in a fully automated fashion based on events, detections, audit messages or any other [target](/v2/docs/detection-on-alternate-targets) of D&R rules. But it can also be used in an ad-hoc fashion triggered manually.

## Enabling Extension

The Playbook extension can be enabled by subscribing your organization to the ext-playbook add-on.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(317).png)

## Accessing Playbooks

Playbooks are created, modified, and deleted via the Playbooks option located within the Automation menu.

> Note: If you are unable to see the Playbooks option, ensure your user account has the appropriate permissions enabled.
>
> ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(319).png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(321).png)

## Usage

When invoking a playbook, all you need is the playbook name as defined in Hive. Optionally, a playbook can also receive a JSON dictionary object as parameters, this is useful when triggering a playbook from a D&R rule and you want to pass some context, or when passing context interactively.

### D&R rule example

Here is an example D&R rule starting a new invocation of a playbook.

```
- action: extension request
  extension name: ext-playbook
  extension action: run_playbook
  extension request:
    name: '{{ "my-playbook" }}'
    credentials: '{{ "hive://secret/my-api-key" }}'
    data:
      some: event.FILE_PATH
      for_the: '{{ "running of the playbook" }}'
```

### Python example

```
# Import LC SDK
import limacharlie
# Instantiate the SDK with default creds.
lc = limacharlie.Manager()
# Instantiate the Extension manager object.
ext = limacharlie.Extension(lc)

# Issue a request to the "ext-playbook" extension.
response = ext.request("ext-playbook", "run_playbook", {
    "name": "my-playbook",
    "credentials": "hive://secret/my-playbook-api-key",
    "data": {
        "some": "data"
    }
})

# The returned data from the playbook.
print(response)
```

## Playbook structure

A playbook is a normal python script. The only required component is a top level function called `playbook` which takes 2 arguments:

* `sdk`: an instance of the LC Python SDK ( `limacharlie.Manager()` ) pre-authenticated to the relevant Organization based on the credentials provided, if any, `None` otherwise.
* `data`: the optional JSON dictionary provided as context to your playbook.

The function must return a dictionary with the following optional keys:

1. `data`: a dictionary of data to return to the caller
2. `error`: an error message (string) to return to the caller
3. `detection`: a dictionary to use as detection
4. `cat`: a string to use as the category of the detection, if `detection` is specified.

This allows your playbook to return information about its execution, return data, errors or generate a detection. The python `print()` statement is not currently being returned to the caller or otherwise accessible, so you will want to use the `data` in order to return information about the execution of your playbook.

### Example playbook

The following is a sample playbook that sends a webhook to an external product with a secret stored in LimaCharlie, and it returns the data as the response from the playbook.

```
import limacharlie
import json
import urllib.request

def playbook(sdk, data):
  # Get the secret we need from LimaCharlie.
  mySecret = limacharlie.Hive(sdk, "secret").get("my-secret-name").data["secret"]

  # Send the Webhook.
  request = urllib.request.Request("https://example.com/webhook", data=json.dumps(data).encode('utf-8'), headers={
    "Content-Type": "application/json",
    "Authorization": f"Bearer {mySecret}"
  }, method="POST")

  try:
    with urllib.request.urlopen(request) as response:
      response_body = response.read().decode('utf-8')
      # Parse the JSON response
      parsed_response = json.loads(response_body)
  except Exception as e:
    # Some error occured, let the caller/LC know.
    return {
      "error": str(e),
    }

  # Return the data to the caller/LC.
  return {
    "data": parsed_response,
  }
```

### Execution environment

Playbooks contents are cached for short periods of time ( on the order of 10 seconds ) in the cloud.

Playbooks are instantiated on demand and the instance is reused for an undefined amount of time.

Playbook code only executes during the main call to the `playbook` function, background on-going running is not supported.

The execution environment is provisioned on a per-Organization basis, meaning all your playbooks may execute within the same container, but NEVER on a container used by another Organization.

Although you have access to the local environment, this environment is ephemeral and can be wiped at any moment in between executions so you should take care that your playbook is self contained and doesn't assume pre-existing conditions.

A single execution of a playbook is limited to 10 minutes.

The current execution environment is based on the default libraries provided by the `python:slim` Dockerhub official container plus the following packages:

* Python
  + `weasyprint`
  + `flask`
  + `gunicorn`
  + `flask`
  + `limacharlie` (LimaCharlie SDK/CLI)
  + `lcextension` (LimaCharlie Extension SDK)
  + `scikit-learn` (Python Machine Learning kit)
  + `jinja2`
  + `markdown`
  + `pillow`
* NodeJS
* AI
  + Claude Code (`claude`) CLI tool
  + Codex (`codex`) CLI tool
  + Gemini CLI (`gemini`) CLI tool

Custom packages and execution environment tweaks are not available in self-serve mode, but they *may* be available on demand, get in touch with us at support@limacharlie.io.

## Infrastructure as Code

Example:

```
hives:
    playbook:
        my-playbook:
            data:
                python: |-
                    def playbook(sdk, data):
                        if not sdk:
                            return {"error": "LC API key required to list sensors"}
                        return {
                            "data": {
                                "sensors": [s.getInfo() for s in sdk.sensors()]
                            }
                        }
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: ""
```

## Billing

Playbooks are billed per seconds of total execution time.

---

# Purple Teaming

LimaCharlie introduces a dynamic, continuous approach to purple teaming. Experience rapid deployment, centralized visibility, flexible outputs, and automated validation to keep your security posture agile and adaptable in the face of ever-changing threats.

## Purple teaming problems

* **Slow and cumbersome purple teaming exercises:** Traditional purple teaming setups involve deploying complex infrastructure, manually configuring attack simulations, and waiting for results, hindering the frequency and efficiency of security validation.
* **Limited visibility and control:** Siloed security tools often lack centralized visibility into both red and blue team activities, making it difficult to measure the effectiveness of implemented controls and identify gaps in defense strategy.
* **Inaccessible validation results:** Sharing and analyzing purple teaming outputs across different platforms and teams can be tedious and time-consuming, hampering efficient collaboration and feedback loops.
* **Static security posture:** Traditional setups rarely provide continuous validation, leaving organizations vulnerable to evolving threats and undetected weaknesses between infrequent purple teaming exercises.

## LimaCharlie's solution

* **Rapid deployment and execution:** Leverage LimaCharlie's multi-platform, full-parity Sensors and pre-built Atomic Red Team integrations to launch sophisticated attack simulations instantly across your entire infrastructure. No need for time-consuming manual setups or specialized red team expertise.
* **Unified visibility and control:** Gain a single pane of glass view into both red and blue team activities within LimaCharlie. Monitor attack simulations, analyze responses from your security tools, and identify areas for improvement with ease.
* **Flexible output destinations:** Seamlessly send purple teaming outputs to any destination you choose  SIEMs, security dashboards, incident response platforms, or even custom tools. Streamline collaboration, facilitate data analysis, and accelerate the validation process.
* **Continuous feedback loop:** Integrate LimaCharlie into your security workflow for ongoing validation. Conduct automated purple teaming exercises at regular intervals, continuously testing your defenses against the latest threats and adapting your security posture based on feedback.

---

# Purple Teaming

LimaCharlie introduces a dynamic, continuous approach to purple teaming. Experience rapid deployment, centralized visibility, flexible outputs, and automated validation to keep your security posture agile and adaptable in the face of ever-changing threats.

## Purple teaming problems

* **Slow and cumbersome purple teaming exercises:** Traditional purple teaming setups involve deploying complex infrastructure, manually configuring attack simulations, and waiting for results, hindering the frequency and efficiency of security validation.
* **Limited visibility and control:** Siloed security tools often lack centralized visibility into both red and blue team activities, making it difficult to measure the effectiveness of implemented controls and identify gaps in defense strategy.
* **Inaccessible validation results:** Sharing and analyzing purple teaming outputs across different platforms and teams can be tedious and time-consuming, hampering efficient collaboration and feedback loops.
* **Static security posture:** Traditional setups rarely provide continuous validation, leaving organizations vulnerable to evolving threats and undetected weaknesses between infrequent purple teaming exercises.

## LimaCharlie's solution

* **Rapid deployment and execution:** Leverage LimaCharlie's multi-platform, full-parity Sensors and pre-built Atomic Red Team integrations to launch sophisticated attack simulations instantly across your entire infrastructure. No need for time-consuming manual setups or specialized red team expertise.
* **Unified visibility and control:** Gain a single pane of glass view into both red and blue team activities within LimaCharlie. Monitor attack simulations, analyze responses from your security tools, and identify areas for improvement with ease.
* **Flexible output destinations:** Seamlessly send purple teaming outputs to any destination you choose  SIEMs, security dashboards, incident response platforms, or even custom tools. Streamline collaboration, facilitate data analysis, and accelerate the validation process.
* **Continuous feedback loop:** Integrate LimaCharlie into your security workflow for ongoing validation. Conduct automated purple teaming exercises at regular intervals, continuously testing your defenses against the latest threats and adapting your security posture based on feedback.

---

# REnigma

### About REnigma

[REnigma](https://dtrsec.com/) is an advanced malware analysis platform leveraging its unique Record and Replay technology to deliver unparalleled precision and depth. By recording every state change in a virtual machine during live execution, REnigma enables analysts to replay and analyze malware behaviors offline, down to the instruction level. This approach eliminates the risk of evasion and ensures a comprehensive capture of malicious activity. For SOC teams triaging alerts or incident responders conducting deep dives, REnigma offers rapid detonation, precision analysis, and effortless artifact extraction. Its API integrations further enhance workflows, enabling seamless automation and streamlined investigation processes.

### About the Extension

The LimaCharlie Extension for REnigma seamlessly integrates with the REnigma API, enabling automated analysis of suspicious URLs or files collected using the LimaCharlie BinLib or Artifact Extensions. When a file or URL triggers an alert in LimaCharlie, preconfigured Detection & Response (D&R) rules can automatically queue the item for further investigation in REnigma.

Through the integration, these D&R rules send the artifact or URL directly to REnigma, where it is recorded and analyzed in a controlled virtual machine environment. Analysts can then access detailed execution data, artifacts, and network patterns captured by REnigma's Record and Replay technology. This workflow not only streamlines the triage process but also provides deep insights into potential threats without requiring manual intervention at every step.

### Configuration

To use the REnigma extension, you will need your REnigma URL and API key. [Contact the REnigma team for access](https://dtrsec.com/contact.html).

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(284).png)

### Using the Extension

You can submit a file or URL to the REnigma extension for processing in one of 2 ways:

1. Via the LimaCharlie web UI:

   1. Submit the ID of the artifact you wish to process with REnigma, and it will get uploaded and processed via a series of D&R rules. You will see the output in the `ext-renigma` sensor timeline.
      ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(297).png)
   2. Submit the URL you wish to analyze with REnigma, and it will get sent and processed via a series of D&R rules. You will see the output in the `ext-renigma` sensor timeline.
      ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(296).png)

2. Via D&R rules:

   1. Detect:

      ```
      event: ingest
      op: exists
      path: /
      target: artifact_event
      artifact type: ext-binlib-bin
      ```

   2. Respond:

      ```
      - action: "extension request"
        extension name: "ext-renigma"
        extension action: "upload_file"
        extension request:
            file_id: '{{ .routing.log_id }}'
            disable_internet: false
      ```

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

---

# Reference: Authenticated Resource Locator

## Overview

Many features in LimaCharlie require access to external resources, sometimes authenticated, provided by users.

Authenticated Resource Locators (ARLs) describe a way to specify access to a remote resource, supporting many methods, including authentication data, and all that within a single string.

## Format

### With authentication

```
[methodName,methodDest,authType,authData]
```

### Without authentication

```
[methodName,methodDest]
```

* `methodName`: the transport to use, one of `http`, `https`, `gcs` and `github`.
* `methodDest`: the actual destination of the transport. A domain and path for HTTP(S) and a bucket name and path for GCS.
* `authType`: how to authenticate, one of `basic`, `bearer`, `token`, `gaia` or `otx`.
* `authData`: the auth data, like `username:password` for `basic`, or access token values. If the value is a complex structure, like a `gaia` JSON service key, it must be base64-encoded.

## Examples

### HTTP GET with no auth

`[https,my.corpwebsite.com/resourdata]`

### HTTP GET with basic auth

`[https,my.corpwebsite.com/resourdata,basic,myusername:mypassword]`

### HTTP GET with bearer auth

`[https,my.corpwebsite.com/resourdata,bearer,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

### HTTP GET with token auth

`[https,my.corpwebsite.com/resourdata,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

### Retrieve from Google Cloud Storage

`[gcs,my-bucket-name/some-blob-prefix,gaia,base64(GCP_SERVICE_KEY)]`

### Retrieve OTX Pulse via REST API

`[https,otx.alienvault.com/api/v1/pulses/5dc56c60a9edbde72dd5d013,otx,9uhr438uhf4h4u9fj7f6the8h383v8jv4ccc1e263d37f29d034d]`

### Retrieve from public GitHub repo main branch

`[github,myGithubUserOrOrg/repoName/path/to/file]`

**Note:** The path to the repo is NOT the same as the URL. Utilize the UI breadcrumbs for the correct path.
For example, in the following screenshot:

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image-1734104784118.png)

The GitHub user is: *romainmarcoux*
The repo name is: *malicious-domains*
The path is: *sources/alienvault-phishing-scam*
So the ARL would be:  `[github,romainmarcoux/malicious-domains/sources/alienvault-phishing-scam]`

### Retrieve from GitHub repo with Github Personal Access Token

`[github,myGithubUserOrOrg/repoName/optional/subpath/to,token,f1eb898f20a0db07e88878aadfsdfdfsffdsdfadwq8f767a72218f2]`

### Retrieve from public GitHub repo at a specific branch

`[github,refractionPOINT/sigma/some-sub-dir?ref=my-branch]`

---

# Reference: Error Codes

The follow error codes are found within various Report (`*_REP`) events found within the EDR Events, often in response to an endpoint agent command.

| Error Code | Value |
| --- | --- |
| ERROR_SUCCESS | 0, 200 |
| ERROR_INVALID_FUNCTION | 1 |
| ERROR_FILE_NOT_FOUND | 2 |
| ERROR_PATH_NOT_FOUND | 3 |
| ERROR_ACCESS_DENIED | 5 |
| ERROR_INVALID_HANDLE | 6 |
| ERROR_NOT_ENOUGH_MEMORY | 8 |
| ERROR_INVALID_DRIVE | 15 |
| ERROR_CURRENT_DIRECTORY | 16 |
| ERROR_WRITE_PROTECT | 19 |
| ERROR_CRC | 23 |
| ERROR_SEEK | 25 |
| ERROR_WRITE_FAULT | 29 |
| ERROR_READ_FAULT | 30 |
| ERROR_SHARING_VIOLATION | 32 |
| ERROR_LOCK_VIOLATION | 33 |
| ERROR_HANDLE_EOF | 38 |
| ERROR_HANDLE_DISK_FULL | 39 |
| ERROR_NOT_SUPPORTED | 50 |
| ERROR_BAD_NETPATH | 53 |
| ERROR_NETWORK_BUSY | 54 |
| ERROR_NETWORK_ACCESS_DENIED | 65 |
| ERROR_BAD_NET_NAME | 67 |
| ERROR_FILE_EXISTS | 80 |
| ERROR_INVALID_PASSWORD | 86 |
| ERROR_INVALID_PARAMETER | 87 |
| ERROR_BROKEN_PIPE | 109 |
| ERROR_OPEN_FAILED | 110 |
| ERROR_BUFFER_OVERFLOW | 111 |
| ERROR_DISK_FULL | 112 |
| ERROR_INVALID_NAME | 123 |
| ERROR_NEGATIVE_SEEK | 131 |
| ERROR_DIR_NOT_EMPTY | 145 |
| ERROR_BUSY | 170 |
| ERROR_BAD_EXE_FORMAT | 193 |
| ERROR_FILENAME_EXCED_RANGE | 206 |
| ERROR_FILE_TOO_LARGE | 223 |
| ERROR_DIRECTORY | 267 |
| ERROR_INVALID_ADDRESS | 487 |
| ERROR_TIMEOUT | 1460 |

## Payload Specific

When dealing with Payloads or Artifact collection, you may receive HTTP specific error codes:
https://developer.mozilla.org/en-US/docs/Web/HTTP/Status

## Yara Specific

When doing Yara scanning operations, you may receive Yara specific error codes.

These are documented here:
https://github.com/VirusTotal/yara/blob/master/libyara/include/yara/error.h

---

# Reference: ID Schema

## Agent IDs

An AgentID is a 5-tuple that completely describes a Sensor, while a Sensor ID is the smallest single unique identifier that can identify a sensor.

The AgentID's components look like this: `OID.IID.SID.PLATFORM.ARCHITECTURE`.

For all components, a value of `0` indicates a wildcard that matches any value when comparing AgentIDs as masks.

## Architecture

The architecture is an 8 bit integer that identifies the exact architecture the sensor runs on. The important values are:

* `1`: 32 bit (`x86`)
* `2`: 64 bit (`x64`)
* `3`: ARM (`arm`)
* `4`: ARM64 (`arm64`)
* `5`: Alpine 64 (`alpine64`)
* `6`: Chrome (`chromium`)
* `7`: Wireguard (`wireguard`)
* `8`: ARML (`arml`)
* `9`: lc-adapter (`usp_adapter`)

### Operating System Specifics

Looking for more detailed version information on a specific operating system? Check out these vendor guides:

* [Microsoft Windows](https://learn.microsoft.com/en-us/windows/win32/sysinfo/operating-system-version)
* [RHEL](https://access.redhat.com/articles/3078)
* [Ubuntu](https://wiki.ubuntu.com/Releases)

## Device IDs

Given the breadth of platforms supported by LimaCharlie, it is not unusual for one "device" (laptop, server, mobile etc) to be visible from multiple sensors. A basic example of this might be:

* We have a laptop, running macOS as its operating system and running a macOS sensor
* The laptop is also running a Windows Virtual Machine, running a Windows sensor

In this example, we're dealing with one piece of hardware, but two different sensors.

To help provide a holistic view of activity, LimaCharlie introduces the concept of a Device ID. This ID is mostly visible in the sensor's basic info and in the `routing` component of sensor events under the name `did` (Device ID).

This Device ID is automatically generated and assigned by LimaCharlie using correlation of specific low level events common to all the sensors. This means that if two sensors share a `did: 1234-5678...` ID, it means they are either on the same device or at least share the same visibility (they see the same activity from two angles).

## Installer ID

The Installer ID (IID) is a UUID that identifies a unique Installation Key. This allows us to cycle installation keys and repudiate old keys, in the event the key gets leaked.

## Organization ID

The Organization ID (OID) is a UUID which identifies a unique organization.

## Platform

The platform is a 32-bit integer (in its hex format) which identifies the exact platform the sensor runs on. Sensor telemetry will display the `plat` value in decimal format. Although it is structured with a major and minor platform, the important values are:

```
  | Hex ID     | Decimal    | API Name                     | Platform Name                |
  |------------|------------|------------------------------|------------------------------|
  | 0x01000000 | 16777216   | crowdstrike                  | CrowdStrike                  |
  | 0x02000000 | 33554432   | xml                          | XML                          |
  | 0x03000000 | 50331648   | wel                          | Windows Event Logs           |
  | 0x04000000 | 67108864   | msdefender                   | Microsoft Defender           |
  | 0x05000000 | 83886080   | duo                          | Duo                          |
  | 0x06000000 | 100663296  | okta                         | Okta                         |
  | 0x07000000 | 117440512  | sentinel_one                 | SentinelOne                  |
  | 0x08000000 | 134217728  | github                       | GitHub                       |
  | 0x09000000 | 150994944  | slack                        | Slack                        |
  | 0x0A000000 | 167772160  | cef                          | Common Event Format (CEF)    |
  | 0x0B000000 | 184549376  | lc_event                     | LimaCharlie Events           |
  | 0x0C000000 | 201326592  | azure_ad                     | Azure Active Directory       |
  | 0x0D000000 | 218103808  | azure_monitor                | Azure Monitor                |
  | 0x0E000000 | 234881024  | canary_token                 | Canary Token                 |
  | 0x0F000000 | 251658240  | guard_duty                   | Guard Duty                   |
  | 0x11000000 | 285212672  | itglue                       | IT Glue                      |
  | 0x12000000 | 301989888  | k8s_pods                     | Kubernetes Pods              |
  | 0x13000000 | 318767104  | zeek                         | Zeek                         |
  | 0x14000000 | 335544320  | mac_unified_logging          | Macos Unified Logging        |
  | 0x15000000 | 352321536  | azure_event_hub_namespace    | Azure Event Hub Namespace    |
  | 0x16000000 | 369098752  | azure_key_vault              | Azure Key Vault              |
  | 0x17000000 | 385875968  | azure_kubernetes_service     | Azure Kubernetes Service     |
  | 0x18000000 | 402653184  | azure_network_security_group | Azure Network Security Group |
  | 0x19000000 | 419430400  | azure_sql_audit              | Azure SQL Audit              |
  | 0x1A000000 | 436207616  | email                        | Email                        |
  | 0x21000000 | 553648128  | hubspot                      | HubSpot                      |
  | 0x22000000 | 570425344  | zendesk                      | Zendesk                      |
  | 0x23000000 | 587202560  | pandadoc                     | PandaDoc                     |
  | 0x24000000 | 603979776  | falconcloud                  | FalconCloud                  |
  | 0x25000000 | 620756992  | mimecast                     | Mimecast                     |
  | 0x26000000 | 637534208  | sublime                      | Sublime                      |
  | 0x27000000 | 654311424  | box                          | Box                          |
  | 0x28000000 | 671088640  | cylance                      | Cylance                      |
  | 0x29000000 | 687865856  | proofpoint                   | Proofpoint                   |
  | 0x2A000000 | 704643072  | entraid                      | EntraID                      |
  | 0x2B000000 | 721420288  | wiz                          | Wiz                          |
  | 0x10000000 | 268435456  | windows                      | Windows                      |
  | 0x20000000 | 536870912  | linux                        | Linux                        |
  | 0x30000000 | 805306368  | macos                        | MacOS                        |
  | 0x40000000 | 1073741824 | ios                          | iOS                          |
  | 0x50000000 | 1342177280 | android                      | Android                      |
  | 0x60000000 | 1610612736 | chrome                       | ChromeOS                     |
  | 0x70000000 | 1879048192 | vpn                          | VPN                          |
  | 0x80000000 | 2147483648 | text                         | Text (external telemetry)    |
  | 0x90000000 | 2415919104 | json                         | JSON (external telemetry)    |
  | 0xA0000000 | 2684354560 | gcp                          | GCP (external telemetry)     |
  | 0xB0000000 | 2952790016 | aws                          | AWS (external telemetry)     |
  | 0xC0000000 | 3221225472 | carbon_black                 | VMWare Carbon Black          |
  | 0xD0000000 | 3489660928 | 1password                    | 1Password                    |
  | 0xE0000000 | 3758096384 | office365                    | Microsoft/Office 365         |
  | 0xF0000000 | 4026531840 | sophos                       | Sophos                       |
```

> **Tip:** If you're writing a rule to target a specific platform, consider using the `is_platform` operator instead of the decimal value for easier readability.

## Sensor ID

The Sensor ID (SID) is a UUID that identifies a unique sensor.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, a Sensor ID is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately linked to specific devices or endpoints.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

In LimaCharlie, an Organization ID is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

In LimaCharlie, a Sensor ID (SID) is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately link to specific devices or endpoints.

---

# Reference: ID Schema

## Agent IDs

An AgentID is a 5-tuple that completely describes a Sensor, while a Sensor ID is the smallest single unique identifier that can identify a sensor.

The AgentID's components look like this: `OID.IID.SID.PLATFORM.ARCHITECTURE`.

For all components, a value of `0` indicates a wildcard that matches any value when comparing AgentIDs as masks.

## Architecture

The architecture is an 8 bit integer that identifies the exact architecture the sensor runs on. The important values are:

* `1`: 32 bit (`x86`)
* `2`: 64 bit (`x64`)
* `3`: ARM (`arm`)
* `4`: ARM64 (`arm64`)
* `5`: Alpine 64 (`alpine64`)
* `6`: Chrome (`chromium`)
* `7`: Wireguard (`wireguard`)
* `8`: ARML (`arml`)
* `9`: lc-adapter (`usp_adapter`)

### Operating System Specifics

Looking for more detailed version information on a specific operating system? Check out these vendor guides:

* [Microsoft Windows](https://learn.microsoft.com/en-us/windows/win32/sysinfo/operating-system-version)
* [RHEL](https://access.redhat.com/articles/3078)
* [Ubuntu](https://wiki.ubuntu.com/Releases)

## Device IDs

Given the breadth of platforms supported by LimaCharlie, it is not unusual for one "device" (laptop, server, mobile etc) to be visible from multiple sensors. A basic example of this might be:

* We have a laptop, running macOS as its operating system and running a macOS sensor
* The laptop is also running a Windows Virtual Machine, running a Windows sensor

In this example, we're dealing with one piece of hardware, but two different sensors.

To help provide a holistic view of activity, LimaCharlie introduces the concept of a Device ID. This ID is mostly visible in the sensor's basic info and in the `routing` component of sensor events under the name `did` (Device ID).

This Device ID is automatically generated and assigned by LimaCharlie using correlation of specific low level events common to all the sensors. This means that if two sensors share a `did: 1234-5678...` ID, it means they are either on the same device or at least share the same visibility (they see the same activity from two angles).

## Installer ID

The Installer ID (IID) is a UUID that identifies a unique Installation Key. This allows us to cycle installation keys and repudiate old keys, in the event the key gets leaked.

## Organization ID

The Organization ID (OID) is a UUID which identifies a unique organization.

## Platform

The platform is a 32-bit integer (in its hex format) which identifies the exact platform the sensor runs on. Sensor telemetry will display the `plat` value in decimal format. Although it is structured with a major and minor platform, the important values are:

```
  | Hex ID     | Decimal    | API Name                     | Platform Name                |
  |------------|------------|------------------------------|------------------------------|
  | 0x01000000 | 16777216   | crowdstrike                  | CrowdStrike                  |
  | 0x02000000 | 33554432   | xml                          | XML                          |
  | 0x03000000 | 50331648   | wel                          | Windows Event Logs           |
  | 0x04000000 | 67108864   | msdefender                   | Microsoft Defender           |
  | 0x05000000 | 83886080   | duo                          | Duo                          |
  | 0x06000000 | 100663296  | okta                         | Okta                         |
  | 0x07000000 | 117440512  | sentinel_one                 | SentinelOne                  |
  | 0x08000000 | 134217728  | github                       | GitHub                       |
  | 0x09000000 | 150994944  | slack                        | Slack                        |
  | 0x0A000000 | 167772160  | cef                          | Common Event Format (CEF)    |
  | 0x0B000000 | 184549376  | lc_event                     | LimaCharlie Events           |
  | 0x0C000000 | 201326592  | azure_ad                     | Azure Active Directory       |
  | 0x0D000000 | 218103808  | azure_monitor                | Azure Monitor                |
  | 0x0E000000 | 234881024  | canary_token                 | Canary Token                 |
  | 0x0F000000 | 251658240  | guard_duty                   | Guard Duty                   |
  | 0x11000000 | 285212672  | itglue                       | IT Glue                      |
  | 0x12000000 | 301989888  | k8s_pods                     | Kubernetes Pods              |
  | 0x13000000 | 318767104  | zeek                         | Zeek                         |
  | 0x14000000 | 335544320  | mac_unified_logging          | Macos Unified Logging        |
  | 0x15000000 | 352321536  | azure_event_hub_namespace    | Azure Event Hub Namespace    |
  | 0x16000000 | 369098752  | azure_key_vault              | Azure Key Vault              |
  | 0x17000000 | 385875968  | azure_kubernetes_service     | Azure Kubernetes Service     |
  | 0x18000000 | 402653184  | azure_network_security_group | Azure Network Security Group |
  | 0x19000000 | 419430400  | azure_sql_audit              | Azure SQL Audit              |
  | 0x1A000000 | 436207616  | email                        | Email                        |
  | 0x21000000 | 553648128  | hubspot                      | HubSpot                      |
  | 0x22000000 | 570425344  | zendesk                      | Zendesk                      |
  | 0x23000000 | 587202560  | pandadoc                     | PandaDoc                     |
  | 0x24000000 | 603979776  | falconcloud                  | FalconCloud                  |
  | 0x25000000 | 620756992  | mimecast                     | Mimecast                     |
  | 0x26000000 | 637534208  | sublime                      | Sublime                      |
  | 0x27000000 | 654311424  | box                          | Box                          |
  | 0x28000000 | 671088640  | cylance                      | Cylance                      |
  | 0x29000000 | 687865856  | proofpoint                   | Proofpoint                   |
  | 0x2A000000 | 704643072  | entraid                      | EntraID                      |
  | 0x2B000000 | 721420288  | wiz                          | Wiz                          |
  | 0x10000000 | 268435456  | windows                      | Windows                      |
  | 0x20000000 | 536870912  | linux                        | Linux                        |
  | 0x30000000 | 805306368  | macos                        | MacOS                        |
  | 0x40000000 | 1073741824 | ios                          | iOS                          |
  | 0x50000000 | 1342177280 | android                      | Android                      |
  | 0x60000000 | 1610612736 | chrome                       | ChromeOS                     |
  | 0x70000000 | 1879048192 | vpn                          | VPN                          |
  | 0x80000000 | 2147483648 | text                         | Text (external telemetry)    |
  | 0x90000000 | 2415919104 | json                         | JSON (external telemetry)    |
  | 0xA0000000 | 2684354560 | gcp                          | GCP (external telemetry)     |
  | 0xB0000000 | 2952790016 | aws                          | AWS (external telemetry)     |
  | 0xC0000000 | 3221225472 | carbon_black                 | VMWare Carbon Black          |
  | 0xD0000000 | 3489660928 | 1password                    | 1Password                    |
  | 0xE0000000 | 3758096384 | office365                    | Microsoft/Office 365         |
  | 0xF0000000 | 4026531840 | sophos                       | Sophos                       |
```

> **Tip:** If you're writing a rule to target a specific platform, consider using the `is_platform` operator instead of the decimal value for easier readability.

## Sensor ID

The Sensor ID (SID) is a UUID that identifies a unique sensor.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, a Sensor ID is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately linked to specific devices or endpoints.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

In LimaCharlie, an Organization ID is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

In LimaCharlie, a Sensor ID (SID) is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately linked to specific devices or endpoints.

---

# Reference: Permissions

## Overview

LimaCharlie uses a granular permission system that controls access to all platform functionality. Permissions are applied through User accounts, API Keys, or Groups and follow a hierarchical naming convention: `category`.`action`

## Permission Structure

### Naming Convention

* **Category**: Functional area (e.g. sensor, org, dr)
* **Action**: Operation type (e.g. get, list, set, del, ctrl)

## Core Permissions

### Organization Management

| Permission | Description |
| --- | --- |
| org.get | View organization information |
| org.del | Delete organization |
| org.set_quota | Manage organization quotas |
| org.conf.get | View organization configuration |
| org.conf.set | Modify organization configuration |

### User & Access Control

| Permission | Description |
| --- | --- |
| apikey.ctrl | Create, delete, and modify API keys |
| user.ctrl | Manage user accounts and permissions |
| billing.ctrl | Access and modify billing information |

### Sensor Management

| Permission | Description |
| --- | --- |
| sensor.list | List all sensors in organization |
| sensor.get | View detailed sensor information |
| sensor.task | Send commands and tasks to sensors |
| sensor.del | Delete sensors |
| sensor.tag | Manage sensor tags and labels |

### Installation Keys

| Permission | Description |
| --- | --- |
| ikey.list | List installation keys |
| ikey.set | Create new installation keys |
| ikey.del | Delete installation keys |

### Detection & Response (D&R)

#### General D&R Rules

| Permission | Description |
| --- | --- |
| dr.list | List general detection rules |
| dr.set | Create and modify general detection rules |
| dr.del | Delete general detection rules |

#### Managed D&R Rules

| Permission | Description |
| --- | --- |
| dr.list.managed | List managed detection rules |
| dr.set.managed | Create and modify managed detection rules |
| dr.del.managed | Delete managed detection rules |

#### Service D&R Rules

| Permission | Description |
| --- | --- |
| dr.list.service | List service detection rules |
| dr.set.service | Create and modify service detection rules |
| dr.del.service | Delete service detection rules |

#### False Positives

| Permission | Description |
| --- | --- |
| fp.ctrl | Manage false positive suppressions |

## Configuration Management (Hive)

### Secrets

| Permission | Description |
| --- | --- |
| secret.get | Access secret values |
| secret.set | Create and modify secrets |
| secret.del | Delete secrets |
| secret.get.mtd | View secret metadata only |
| secret.set.mtd | Modify secret metadata only |

### Lookups

| Permission | Description |
| --- | --- |
| lookup.get | Access lookup tables |
| lookup.set | Create and modify lookup tables |
| lookup.del | Delete lookup tables |
| lookup.get.mtd | View lookup metadata only |
| lookup.set.mtd | Modify lookup metadata only |

### Models

| Permission | Description |
| --- | --- |
| model.get | Access behavioral models |
| model.set | Create and modify behavioral models |
| model.del | Delete behavioral models |
| model.get.mtd | View model metadata only |
| model.set.mtd | Modify model metadata only |

### Queries

| Permission | Description |
| --- | --- |
| query.get | Access saved queries |
| query.set | Create and modify saved queries |
| query.del | Delete saved queries |
| query.get.mtd | View query metadata only |
| query.set.mtd | Modify query metadata only |

### YARA Rules

| Permission | Description |
| --- | --- |
| yara.get | Access YARA rules |
| yara.set | Create and modify YARA rules |
| yara.del | Delete YARA rules |
| yara.get.mtd | View YARA rule metadata only |
| yara.set.mtd | Modify YARA rule metadata only |

### AI Agents

| Permission | Description |
| --- | --- |
| ai_agent.get | Access AI agent configurations |
| ai_agent.set | Create and modify AI agents |
| ai_agent.del | Delete AI agents |
| ai_agent.get.mtd | View AI agent metadata only |
| ai_agent.set.mtd | Modify AI agent metadata only |

### Cloud Sensors

| Permission | Description |
| --- | --- |
| cloudsensor.get | Access cloud sensor configurations |
| cloudsensor.set | Create and modify cloud sensor configurations |
| cloudsensor.del | Delete cloud sensor configurations |
| cloudsensor.get.mtd | View cloud sensor metadata only |
| cloudsensor.set.mtd | Modify cloud sensor metadata only |

### Playbooks

| Permission | Description |
| --- | --- |
| playbook.get | Access playbooks |
| playbook.set | Create and modify playbooks |
| playbook.del | Delete playbooks |
| playbook.get.mtd | View playbook metadata only |
| playbook.set.mtd | Modify playbook metadata only |

### External Adapters

| Permission | Description |
| --- | --- |
| externaladapter.get | Access external adapter configurations |
| externaladapter.set | Create and modify external adapters |
| externaladapter.del | Delete external adapter configurations |
| externaladapter.get.mtd | View external adapter metadata only |
| externaladapter.set.mtd | Modify external adapter metadata only |

## Extensions & Services

### Extensions

| Permission | Description |
| --- | --- |
| ext.request | Request extension actions |
| ext.conf.get | View extension configurations |
| ext.conf.set | Modify extension configurations |
| ext.conf.del | Delete extension configurations |
| ext.conf.get.mtd | View extension metadata only |
| ext.conf.set.mtd | Modify extension metadata only |
| ext.sub | Subscribe to extension services |
| ext.sub.mtd | Manage extension subscription metadata |

### Replicant Services

| Permission | Description |
| --- | --- |
| replicant.get | View replicant service status |
| replicant.ctrl | Control replicant services |

## Data Access & Analytics

### Insight & Detections

| Permission | Description |
| --- | --- |
| insight.list | List available insights |
| insight.ctrl | Control insight generation |
| insight.del | Delete insights |
| insight.evt.get | Access detailed event data |
| insight.evt.get.simple | Access simplified event data |
| insight.det.get | Access detection details |
| insight.stat | Access insight statistics |

### Audit & Logging

| Permission | Description |
| --- | --- |
| audit.get | Access audit logs and error messages |
| audit.set | Create audit logs entries |

## Operations Management

### Jobs

| Permission | Description |
| --- | --- |
| job.get | View job status and results |
| job.ctrl | Create and schedule jobs |

### Outputs

| Permission | Description |
| --- | --- |
| output.list | List output configurations |
| output.set | Create and modify output configurations |
| output.del | Delete output configurations |

### Payloads

| Permission | Description |
| --- | --- |
| payload.ctrl | Manage sensor payloads |

### Module Management

| Permission | Description |
| --- | --- |
| module.update | Update sensor modules |

### Ingestion

| Permission | Description |
| --- | --- |
| ingestkey.ctrl | Manage data ingestion keys |

## Permission Application

Permissions can be applied through:

1. **User Accounts**: Direct assignment to individual users
2. **API Keys**: Embedded in API key configurations for programmatic access
3. **Groups**: Assigned to groups, then inherited by group members

## Best Practices

1. **Principle of Least Privilege**: Grant only the minimum permissions required
2. **Use Groups**: Manage permissions through groups rather than individual assignments
3. **Regular Auditing**: Periodically review and audit permission assignments
4. **Separate Environments**: Use different permission sets for development, staging, and production
5. **API Key Management**: Rotate API keys regularly and scope them appropriately

---

# Reliable Tasking

The Reliable Tasking Extension enables you to task a Sensor(s) that are currently offline. The extension will automatically send the task(s) to Sensor(s) once it comes online.

## Enabling the Reliable Tasking Extension

To enable the Reliable Tasking extension, navigate to the [Reliable Tasking extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-reliable-tasking) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

After clicking **Subscribe**, the Reliable Tasking extension should be available almost immediately.

## Using the Reliable Tasking Extension

Once enabled, you will see a **Reliable Tasking** option under **Automation** within the LimaCharlie web UI. You can also interact with the extension via REST API.

Within the Reliable Tasking module, you can:

* Task Sensor(s)
* Untask Sensor(s)
* List active task(s)

## Actions via REST API

The following REST API actions can be sent to interact with the Reliable Tasking extension:

#### **Create a Task**

```
curl --location 'https://api.limacharlie.io/v1/extension/request/ext-reliable-tasking' \
--header 'Authorization: Bearer $JWT' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data 'oid=$YOUR_OID&action=task&data={"context":"version","selector":"plat==windows","task":"run --shell-command whoami","ttl":3600}'
```

The `task` is similar to a command-line `task`. Optionally, you can also specify which endpoints to task by specifying:

* `sid` : A specific Sensor ID
* `tag` : All Sensor(s) with a Tag
* `plat`: All Sensor(s) of the specified platform

You can use the `ttl` to specify how long the extension should try to keep sending the task. The `ttl` value is a number of seconds and defaults to 1 week.

#### **List Tasks**

```
curl --location 'https://api.limacharlie.io/v1/extension/request/ext-reliable-tasking' \
--header 'Authorization: Bearer $JWT' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data 'oid=$YOUR_OID&action=list&data={}'
```

When listing tasks, you can specify which endpoints to get queued tasks from by using one of:

* `sid` : A specific sensor ID
* `tag` : All Sensor(s) with a tag
* `plat`: All Sensor(s) of the specified platform

## Capturing Task Responses

If you're using reliable tasks to issue commands across your sensors, you're probably going to want to view or act on the responses from these commands as well.

If you add a value to the `context` parameter in the extension request, this value will be reflected in the `investigation_id` of the corresponding `RECEIPT` or `_REP` event, allowing you to craft a D&R rule based on the response.

The above example cURL command has a `context` of `version` so the below D&R rule looks for that value.

#### Example detect block:

```
op: contains
event: RECEIPT
path: routing/investigation_id
value: version
```

#### Example respond block:

```
- action: output
  name: tasks-output         # Send responses to the specified output
- action: report
  name: "Reliable task ran"  # Detect on the task being run
```

## Migrating Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy reliable tasking service, preview the change and execute the conversion required in the rule "response".

Command line to preview Reliable Tasking rule conversion:

```
limacharlie extension convert_rules --name ext-reliable-tasking
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute reliable tasking rule conversion:

```
limacharlie extension convert_rules --name ext-reliable-tasking --no-dry-run
```

---

# Reliable Tasking

The Reliable Tasking Extension enables you to task a Sensor(s) that are currently offline. The extension will automatically send the task(s) to Sensor(s) once it comes online.

## Enabling the Reliable Tasking Extension

To enable the Reliable Tasking extension, navigate to the [Reliable Tasking extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-reliable-tasking) in the marketplace. Select the Organization you wish to enable the extension for, and select **Subscribe**.

After clicking **Subscribe**, the Reliable Tasking extension should be available almost immediately.

## Using the Reliable Tasking Extension

Once enabled, you will see a **Reliable Tasking** option under **Automation** within the LimaCharlie web UI. You can also interact with the extension via REST API.

Within the Reliable Tasking module, you can:

* Task Sensor(s)
* Untask Sensor(s)
* List active task(s)

## Actions via REST API

The following REST API actions can be sent to interact with the Reliable Tasking extension:

#### **Create a Task**

```
curl --location 'https://api.limacharlie.io/v1/extension/request/ext-reliable-tasking' \
--header 'Authorization: Bearer $JWT' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data 'oid=$YOUR_OID&action=task&data={"context":"version","selector":"plat==windows","task":"run --shell-command whoami","ttl":3600}'
```

The `task` is similar to a command-line `task`. Optionally, you can also specify which endpoints to task by specifying:

* `sid` : A specific Sensor ID
* `tag` : All Sensor(s) with a Tag
* `plat`: All Sensor(s) of the specified platform

You can use the `ttl` to specify how long the extension should try to keep sending the task. The `ttl` value is a number of seconds and defaults to 1 week.

#### **List Tasks**

```
curl --location 'https://api.limacharlie.io/v1/extension/request/ext-reliable-tasking' \
--header 'Authorization: Bearer $JWT' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--data 'oid=$YOUR_OID&action=list&data={}'
```

When listing tasks, you can specify which endpoints to get queued tasks from by using one of:

* `sid` : A specific sensor ID
* `tag` : All Sensor(s) with a tag
* `plat`: All Sensor(s) of the specified platform

## Capturing Task Responses

If you're using reliable tasks to issue commands across your sensors, you're probably going to want to view or act on the responses from these commands as well.

If you add a value to the `context` parameter in the extension request, this value will be reflected in the `investigation_id` of the corresponding `RECEIPT` or `_REP` event, allowing you to craft a D&R rule based on the response.

The above example cURL command has a `context` of `version` so the below D&R rule looks for that value.

#### Example detect block:

```
op: contains
event: RECEIPT
path: routing/investigation_id
value: version
```

#### Example respond block:

```
- action: output
  name: tasks-output         # Send responses to the specified output
- action: report
  name: "Reliable task ran"  # Detect on the task being run
```

## Migrating Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy reliable tasking service, preview the change and execute the conversion required in the rule "response".

Command line to preview Reliable Tasking rule conversion:

```
limacharlie extension convert_rules --name ext-reliable-tasking
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute reliable tasking rule conversion:

```
limacharlie extension convert_rules --name ext-reliable-tasking --no-dry-run
```

---

# Reporting

## Building Reports with BigQuery + Looker Studio

LimaCharlie does not include reporting by default, however our granular and customizable Output options allow you to push data to any source and use third-party tools for reporting. In this tutorial, we'll push a subset of LimaCharlie EDR telemetry to BigQuery and build reports using Looker Studio.

---

# Reporting

## Building Reports with BigQuery + Looker Studio

LimaCharlie does not include reporting by default, however our granular and customizable Output options allow you to push data to any source and use third-party tools for reporting. In this tutorial, we'll push a subset of LimaCharlie EDR telemetry to BigQuery and build reports using Looker Studio.

---

# SOAR / Automation

The SecOps Cloud Platform makes building, modifying, or streamlining your security orchestration, automation, and response (SOAR) operations simple. Lower your costs and increase your response time by aggregating SOAR tooling, integrating resources, and normalizing telemetry with the SecOps Cloud Platform.

## SOAR/Automation problems

Security orchestration, automation, and response (SOAR) solutions play a key role in detecting and responding to cyber threats. However, adopting a standalone SOAR solution may also create new challenges including:

* **Alert fatigue:** Security analysts receive and evaluate countless alerts before uncovering and responding to legitimate issues.
* **High data costs:** Sending telemetry data to a SIEM can be an expensive, resource-intensive process that only increases as businesses grow, resulting in new endpoints and data sources.
* **Unnecessary detection friction:** Security tools detect suspicious activity and send events to the SIEM, generating an alert for the analyst to investigate. After investigation, various manual procedures are invoked to remediate the alert.
* **Inefficient manual processes:** Critical time is lost as analysts coordinate transferring crucial information into various tools and performing response actions.

## LimaCharlie's solutions

The SecOps Cloud Platform consolidates and integrates SOAR tooling in a single place. It offers a more efficient, customizable way to implement SOAR by integrating the security stack, normalizing data, and expanding automation capabilities.

* **Data normalization, collection, false positive rules, and filtering:** The LimaCharlie SecOps Cloud Platform collects and normalizes telemetry making it easy to filter out noise, share information between resources, and detect real problems.
* **Reduced data costs:** All events, telemetry, and detections within LimaCharlie are stored online and searchable for one year. This allows users to keep everything instead of aggregating and having to choose which data is important.
* **Bi-directionality:** LimaCharlie supports bi-directionality which allows automated responses to be sent directly to the source of a detection. For example, if the SecOps Cloud Platform receives a suspicious login alert from O365 it can send a direct automated response to suspend the account. This eliminates a persistence method for attackers.
* **API-first foundation:** LimaCharlie can perform critical response actions for any asset in your security stack via API. Automated responses can trigger remediation actions and send telemetry to security tooling without (comparatively slow) human intervention. Python playbooks allow you to automatically perform standard, repetitive tasks, reducing mean time to resolve, and allowing analysts to focus on higher priority alerts.

---

# SOAR / Automation

The SecOps Cloud Platform makes building, modifying, or streamlining your security orchestration, automation, and response (SOAR) operations simple. Lower your costs and increase your response time by aggregating SOAR tooling, integrating resources, and normalizing telemetry with the SecOps Cloud Platform.

## SOAR/Automation problems

Security orchestration, automation, and response (SOAR) solutions play a key role in detecting and responding to cyber threats. However, adopting a standalone SOAR solution may also create new challenges including:

* **Alert fatigue:** Security analysts receive and evaluate countless alerts before uncovering and responding to legitimate issues.
* **High data costs:** Sending telemetry data to a SIEM can be an expensive, resource-intensive process that only increases as businesses grow, resulting in new endpoints and data sources.
* **Unnecessary detection friction:** Security tools detect suspicious activity and send events to the SIEM, generating an alert for the analyst to investigate. After investigation, various manual procedures are invoked to remediate the alert.
* **Inefficient manual processes:** Critical time is lost as analysts coordinate transferring crucial information into various tools and performing response actions.

## LimaCharlie's solutions

The SecOps Cloud Platform consolidates and integrates SOAR tooling in a single place. It offers a more efficient, customizable way to implement SOAR by integrating the security stack, normalizing data, and expanding automation capabilities.

* **Data normalization, collection, false positive rules, and filtering:** The LimaCharlie SecOps Cloud Platform collects and normalizes telemetry making it easy to filter out noise, share information between resources, and detect real problems.
* **Reduced data costs:** All events, telemetry, and detections within LimaCharlie are stored online and searchable for one year. This allows users to keep everything instead of aggregating and having to choose which data is important.
* **Bi-directionality:** LimaCharlie supports bi-directionality which allows automated responses to be sent directly to the source of a detection. For example, if the SecOps Cloud Platform receives a suspicious login alert from O365 it can send a direct automated response to suspend the account. This eliminates a persistence method for attackers.
* **API-first foundation:** LimaCharlie can perform critical response actions for any asset in your security stack via API. Automated responses can trigger remediation actions and send telemetry to security tooling without (comparatively slow) human intervention. Python playbooks allow you to automatically perform standard, repetitive tasks, reducing mean time to resolve, and allowing analysts to focus on higher priority alerts.

---

# Schema Data Types

## All Data Types

The data types in your schema can be further subdivided into three categories. Primitives, Code Blocks, and Objects (including tables). These data types allow for a cleaner UI and a more intuitive schema.

For a direct code reference, check out the type definition [here](https://github.com/refractionPOINT/lc-extension/blob/master/common/config_schema.go).

### Before you Start

When getting started, we recommend utilizing the simplest data type applicable for each field in your schema as to enable quick and reliable testing of your service.

## Primitives

The following is the list of primitive values. Note that the following fields are also affected by filters:

* number, time and date types are affected by `min` and `max`
* events and string types are affected by `whitelist` and `blacklist`
* only string types are affected by `valid_re` and `invalid_re` (regex)
* SID types (and maybe platforms) are affected by `platforms` filters

> Oops, some fields may be missing support for filters
>
> Please reach out if any of the above use-cases don't work as you might expect.

| name | description |
| --- | --- |
| string |  |
| integer |  |
| bool |  |
| enum | Requires the field `enum_values` |
| complex\_enum | a complex enum allows for a more detailed enum selection, including categories and description. Requires the field `complex_enum_values` |
| sid | your Organization's sensor ids |
| oid | your Organization's ID |
| platform |  |
| architecture |  |
| sensor\_selector |  |
| tag |  |
| duration |  |
| time |  |
| url |  |
| domain |  |
| yara\_rule\_name | Will show your Organization's list of yara rules available, if the user has permission |
| event\_name |  |
| secret | Will show your Organization's list of secrets as per the secrets manager |

## Code Blocks

There are currently 3 code types available:

1. JSON
2. YAML
3. Yara\_rule

> Yara Rule UI Support is limited
>
> Code blocks do not support the field `is_list`. If your extensions require a set of code blocks, we recommend wrapping it into key-value pair using the 'record' data type (see 'objects' section below).

## Objects (and tables)

While objects generally reflect a nested layer of abstraction, it's utility grows when using the field `is_list` to utilize the tables UI, or when defining a set of key-value pairs in the 'record' data type.

Note: there is a functional difference between an 'object' and 'record' data type.

**Single Objects**

Plain objects allow for nested fields, and are visually indifferent from if the nested fields were flattened to begin with. They also allow for extra context to be wrapped in the parent object's description.

```
table: {
  is_list: false,
  data_type: "object",
  object: {
    fields: { ... }, // key-value pairs
    requirements: null
  }
}
```

**Lists of Objects**

Lists of objects display as tables and allow for a more complex and scalable data structure. Simply enable `is_list` on a base object.

```
table: {
  is_list: true,
  data_type: "object",
  object: {
    fields: { ... }, // key-value pairs
    requirements: null
  }
}
```

**Record Type**

Records are inherently lists of a key-value pair, where the value is the defined object, and the key may vary. Record types require a key to be defined in the nested object details, and also supports additional fields for the nested element's name and description.

```
table: {
  is_list: true,
  data_type: "object",
  object: {
    key: {
      name: "key",
      data_type: "string"
    },
    element_name: "single row", // optional
    element_desc: "a single row that represents a key-value pair on a record type", // optional

    fields: { ... }, // key-value pairs
    requirements: null
  }
}
```

---

# SecOps Development

The SecOps Cloud Platform accelerates building, customizing, integrating, and scaling security operations. By handling the infrastructure-intensive aspects of security engineering, LimaCharlie takes on the heavy lifting making it easy for developers to focus on their core projects.

## SecOps development problems

Innovating new security products and modifying existing ones is crucial for protecting organizations from evolving cyberthreats. Unfortunately, the process of building better cybersecurity solutions is often hampered by:

* **Time consuming infrastructure management:** Developers spend too much time building, maintaining, and updating infrastructure instead of working on projects.
* **Integration headaches:** Valuable time is lost to solving integration issues between the tools, services, and platforms critical to a project's success.
* **Scalability issues:** While your solution works well at a small scale there are serious concerns over whether it can scale effectively as your business grows.
* **Lack of control:** Your project relies on third-party or open source tooling that lacks the flexibility you need to succeed.

## LimaCharlie's solution

The SecOps Cloud Platform delivers core security capabilities in the form of "primitives" that can be modified to accommodate specific needs. It offers infrastructure-as-code and other resources that can improve a developer's mean-time-to-market, including:

* **Infrastructure-as-a-service:** LimaCharlie delivers security-oriented cloud-primitives that provide scalable infrastructure and critical resources for your project development. Think AWS, but for security.
* **API-first foundation:** The SecOps Cloud Platform (SCP) is built upon an API-first foundation. Information normalized and shared via API, freeing you from vendor lock-in and giving you maximum flexibility.
* **Seamless scalability:** The SCP is a cloud platform, and scales effortlessly with the growth of your company.
* **Maximum flexibility:** The SCP offers full transparency and granular management of the resources you choose to incorporate. Where third-party tools or open source solutions offer roadblocks, the SCP provides the capability to customize, innovate, and create needed capabilities.

---

# Secure Annex

[Secure Annex](https://secureannex.com/) is a browser extension security platform that provides a comprehensive analysis of the Chrome extensions installed across your organization's endpoints.

The Secure Annex LimaCharlie Extension allows you to query the Secure Annex API with the IDs of Chrome extensions installed on endpoints within your organization in order to get detailed information about the extensions. You can then perform additional analysis or craft rules based on the results.

API endpoints available for querying are:

* /manifest
* /extensions
* /vulnerabilities
* /signatures
* /urls
* /analysis

This is currently only supported on Windows, macOS, and Chrome sensors.

## Setup

1. Sign up and get an API key at <https://app.secureannex.com/settings/api>
2. Subscribe to the Secure Annex extension in LimaCharlie - <https://app.limacharlie.io/add-ons/extension-detail/ext-secureannex>
3. Add the API key to the Secure Annex extension configuration within LimaCharlie

## Usage

### Manually in the GUI

You can trigger an extension request manually within the web app by clicking the `Get extensions from endpoint` button. This will allow you to choose a sensor, or sensors via a Sensor Selector, to get extensions from. More examples of sensor selectors can be found [here](/v2/docs/reference-sensor-selector-expressions).

The extensions are gathered from endpoints via the reliable tasking extension, which appends `secureannex_extensions` to the investigation ID of the `RECEIPT` or `OS_PACKAGES_REP` event in order to trigger an extension request to query Secure Annex. The results will be in the timeline of the `ext-secureannex` sensor.

### Automatically via D&R Rules

Upon subscribing to the Secure Annex extension, several D&R rules are added to your organization in a **disabled** **state** to help you get more use out of the extension and automate your detections. They are as follows:

* `ext-secureannex-detect-vulnerabilities`
  + This will look at the vulnerabilities and associated severities in the `vulnerability` results returned, and create detections on high and critical vulnerabilities found
* `ext-secureannex-detect-risk-rating`
  + This will look at the risks and associated severities in the `manifest` results returned, and create detections on high and critical severities found
* `ext-secureannex-get-extensions-windows`
  + This schedules a base64 encoded PowerShell script to run every 24 hours to query Windows sensors for installed Chrome extensions, and bring back a list of the extension IDs and versions
  + The results will have a `secureannex_extensions` investigation ID associated that will allow LimaCharlie to automatically create Secure Annex extension requests with the IDs and versions included to perform a full analysis and bring back the results into the `ext-secureannex` sensor
* `ext-secureannex-get-extensions-mac`
  + This schedules a base64 encoded bash script to run every 24 hours to query macOS sensors for installed Chrome extensions, and bring back a list of the extension IDs and versions
  + The results will have a `secureannex_extensions` investigation ID associated that will allow LimaCharlie to automatically create Secure Annex extension requests with the IDs and versions included to perform a full analysis and bring back the results into the `ext-secureannex` sensor
* `ext-secureannex-get-extensions-chrome`
  + This schedules the `OS_PACKAGES` command to run every 24 hours to query Chrome sensors for installed Chrome extensions, and bring back a list of the extension IDs and versions
  + The results will have an investigation ID associated that will allow LimaCharlie to automatically create Secure Annex extension requests with the IDs and versions included to perform a full analysis and bring back the results into the `ext-secureannex` sensor

If you wish to use these, you need to enable them first. You can also copy the contents of these rules and create your own so they are no longer managed by the Secure Annex extension if you wish to modify them.

### Results

Results will show up in the live feed and timeline of the `ext-secureannex` Sensor.

---

# Security Monitoring for DevOps

Ditch the data silos and sluggish response  LimaCharlie illuminates your DevOps pipeline, automates threat detection and response, and fosters real-time collaboration, empowering you to build secure and resilient software at record speed.

## Problems monitoring DevOps security

* **Blind Spots in the Pipeline:** Siloed DevOps data from code repositories, third-party tools, and SaaS platforms leaves security teams in the dark, unable to detect threats hidden within routine operations.
* **Reactive Response, Slowed Agility:** Manual security checks and sluggish incident response hinder DevOps agility, creating vulnerable gaps between development and deployment.
* **DevSecOps Disconnect:** Disparate tools and fragmented communication between security and operations teams lead to inefficient incident response and missed opportunities for proactive mitigation.

## LimaCharlie's solution

* **Unified Visibility Across the Flow:** LimaCharlie's architecture effortlessly ingests telemetry from all your DevOps data sources, providing a unified view of your entire pipeline from code commits to production deployments.
* **Automated Threat Detection and Response:** Leverage LimaCharlie's pre-built and custom detection rules to automatically identify suspicious activities within your DevOps data. Trigger instant alerts and pre-defined response actions, including stopping deployments, rolling back changes, or notifying teams.
* **Centralized Collaboration for Faster Response:** Break down silos with LimaCharlie's collaborative platform. Security and operations teams can visualize incidents across the pipeline, analyze threats jointly, and orchestrate coordinated responses in real-time.

---

# Security Monitoring for DevOps

Ditch the data silos and sluggish response  LimaCharlie illuminates your DevOps pipeline, automates threat detection and response, and fosters real-time collaboration, empowering you to build secure and resilient software at record speed.

## Problems monitoring DevOps security

* **Blind Spots in the Pipeline:** Siloed DevOps data from code repositories, third-party tools, and SaaS platforms leaves security teams in the dark, unable to detect threats hidden within routine operations.
* **Reactive Response, Slowed Agility:** Manual security checks and sluggish incident response hinder DevOps agility, creating vulnerable gaps between development and deployment.
* **DevSecOps Disconnect:** Disparate tools and fragmented communication between security and operations teams lead to inefficient incident response and missed opportunities for proactive mitigation.

## LimaCharlie's solution

* **Unified Visibility Across the Flow:** LimaCharlie's architecture effortlessly ingests telemetry from all your DevOps data sources, providing a unified view of your entire pipeline from code commits to production deployments.
* **Automated Threat Detection and Response:** Leverage LimaCharlie's pre-built and custom detection rules to automatically identify suspicious activities within your DevOps data. Trigger instant alerts and pre-defined response actions, including stopping deployments, rolling back changes, or notifying teams.
* **Centralized Collaboration for Faster Response:** Break down silos with LimaCharlie's collaborative platform. Security and operations teams can visualize incidents across the pipeline, analyze threats jointly, and orchestrate coordinated responses in real-time.

---

# Security Service Providers (MSSP, MSP, MDR)

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For managed security services providers (MSSPs), managed detection and response (MDR) firms, and all those involved in digital forensics and incident response (DFIR), the SecOps Cloud Platform is a powerful way to improve security operations and compete more effectively. With the SCP, service providers can deliver security services at scale, control costs, consolidate and customize security tooling, take on new businesses with confidence, and much more.

The platform's public cloud-like delivery model also helps service providers integrate the SCP into their operations gradually and safely. Flexible pay-as-you-go pricing means you only pay for the capabilities you need, and only for as long as you use themwithout long-term contracts, complex licensing, capacity planning, price modeling, or termination fees.

## Implementation strategies for quick wins

The SecOps Cloud Platform contains numerous capabilities and is designed to be highly flexible and customizable. Nevertheless, there are some common implementation strategies that MSSP users have found to be good starting points with the platform. Here are three easy ways that the SCP can help service providers improve security operations and expand their businesses immediately:

### Gain greater visibility into client environments

The SCP can help service providers gain greater visibility into client environmentsand bring telemetry data under a single plane for a more unified view. This is one of the first realizations of value for service providers using the SCP platform. Here's an outline of what this looks like:

**Decide what telemetry data you need to support security operations.** Your options here are extensive. In the SCP, there are two main sources of telemetry:

First, there are the platform's endpoint detection and response (EDR)-type sensors, which can be deployed directly on Windows, Mac, and Linux endpoints with full feature parity across these OSes to capture system events and other telemetry data. There are also browser-based sensors for Chrome and Edge. Sensors stream telemetry data and artifacts into the SCP in real time (and can also be used to take response actions on endpoints). Importing event data from third-party EDR tools such as VMWare Carbon Black, CrowdStrike, and Microsoft Defender is also possible.

The second source of telemetry data can be classed as log-type data. This data can be brought into the SCP using a system of adapters or via webhook. The options are too numerous to list here in full, but supported log data sources include O365, 1Password, AWS CloudTrail, Google Cloud Platform (GCP), Slack Audit logs, and more. For a more comprehensive list, refer to the SCP documentation.

**Configure client organizations to provide the required visibility.** The SCP web interface makes this as simple as making a few clicks to set up the required installation keys. More advanced configuration management options using a REST API or a command-line interface (CLI) are also available. After setup, your client organizations' configurationsincluding what telemetry you want to bring into the SecOps Cloud Platformwill be stored as simple YAML files. Note here that it's possible to use the SCP's multitenancy and organization management features to make configuration changes to multiple organizations at the same time. For a more detailed example of what this might look like, see this demo MSSP setup.

**Bring your data under a single plane.** All telemetry data brought into the SCP is normalized to a common JSON format and explorable through a single interface. In itself, this represents a huge step forward for many service providers because they will no longer have to deal with a fragmented jumble of UIs or competing data formats in order to view and act on their telemetry data.

**Operationalize your telemetry data.** Seeing into your clients' environments is an essential first stepbut this is only the beginning of what is possible with the SecOps Cloud Platform. The SCP's advanced detection and response engine can act on every piece of telemetry brought into the platform, making it possible to apply sophisticated detection and response logic to telemetry data. Applying D&R logic can be as tailored or as simple as you choose, from using custom detections that you write yourself to leveraging curated rulesets like Sigma, Soteria, or SOC Prime rulesor a combination of both approaches.

It's impossible to protect what you can't see. The SCP makes it possible to gain full visibility into a client environment, visualize that telemetry in a single interface and data format, and take action on telemetry data via a powerful detection, automation, and response engine.

### Implement scalable SecOps and simplified client management

The SecOps Cloud Platform is multitenant by design, offers fine-grained role-based access control (RBAC), and supports an Infrastructure-as-Code (IaC) approach to configuration management. These core aspects of the SCP enable service providers to practice modern cybersecurity operations at scale.

**Separate client environments intelligently.** The multitenancy of the SCP allows service providers to create a logical boundary between their client organizations' data while still being able to view and manage everything from a single platform. Multitenancy makes it easier to avoid commingling client dataand comply with regional regulatory requirements such as data residency rules.

**Manage access and permissions more effectively.** RBAC allows you to grant users the access to organizations and the permissions that they need. You can give individual users permissions on a per-organization basis if you choose. But for more efficient access management, you can use Organization Groups, which are groupings of client organizations, permissions, and users.

Organization Groups give the same permissions and organizational access to any user added to the group. Typically, Organization Groups are set up by job function. For example, you might create an Organization Group for security engineers that allows members to edit telemetry ingestion configurations for all of your client organizations, and a separate Organization Group for non-technical roles that provides read-only access or the ability to view general organizational information.

**Build SecOps workflows that scale.** The SecOps Cloud Platform enables service providers to take an infrastructure-as-code approach to security operations. All of your client organizations' security configurationsfrom D&R rules to data forwarding and output settingscan be stored and managed as simple YAML files.

Create new organizations quickly by cloning an existing organization's configurations or using a configuration template. Maintain a global set of configuration settings for all client organizations and then add per-client config files as needed. If you need to make changes to multiple client organizations, this is as simple as editing a global configuration file via CLI or web UI and pushing out the change to all of your organizations at scale.

The SecOps Cloud Platform helps service providers adopt a truly modern and scalable approach to cybersecurity operations. For a more detailed look at how these SCP concepts work in practice, watch Setting Up an MSSP with LimaCharlie.

### Improve incident response times and offer unbeatable service-level agreements

The SecOps Cloud Platform can be tremendously valuable for service providers doing incident response (IR) work. Here are some of the most significant capabilities for IR teams:

**Begin IR engagements without delay.** The on-demand nature of the SecOps Cloud Platform means you will never need to talk to a vendor sales representative or renegotiate a contract before starting an IR engagement. With the SCP, you log into your account, use a credit card or increase your existing sensor quota, and begin.

In addition, it's possible to preconfigure tenants ahead of an IR engagement. Set up your desired SCP IR configuration using custom D&R rulesets, curated rulesets, memory dump capabilities, YARA scanning, and more. Then, export the configuration files for your IR tenant and reuse them whenever you have a new IR engagement to hit the ground running.

**Take the fight to the adversary.** During IR engagements with an active attacker in the environment, the SecOps Cloud Platform gives you a robust response capability on your client's endpoints.

Mass-deploy SCP sensors using an enterprise deployment tool. Then, use those sensors to gather real-time event data, run shell commands and executables on endpoints, deploy security tools and remediation packages at scale, or isolate compromised machines from the networkall with minimal impact on the client's operations and mission-critical IT infrastructure.

**Use security intelligence as soon as you have it.** The SCP's IaC approach means you don't have to rely on a vendor to update a tool or publish an indicator of compromise (IoC) in an emergency. For example, imagine a scenario in which you're dealing with a 0-day compromise. If you have early access to an IoC via an information-sharing network or a colleague, you can literally copy-paste the relevant IoC data from a Slack message into a new SCP D&R rule, update the relevant config file, and push out the change to your client's environmentwhile the all of the vendor-dependent service providers are still waiting on someone else to act.

**Build a true rapid-response capability.** LimaCharlie sensors can be pre-deployed to client environments in "sleeper" mode: i.e., with the telemetry collection settings tuned down to a bare minimum to keep costs to just pennies per month. If an incident occurs, the sensors are already there, ready and waiting on the endpoints, and can turned on for an immediate response. This use case has allowed SCP service provider partners to offer service-level agreements of as little as 20 minutesa considerable advantage when it comes to pitching (and closing) new MDR or MSSP clients.

IR work is high-stakes and high-pressureand, unfortunately, is far too often complicated by the cumbersome sales processes and technical limitations of legacy cybersecurity vendors. The SCP allows incident responders to take action quickly and independently during an incident. It also lets cybersecurity service providers improve their overall response capabilities, enabling attractive service-level agreements that can help win over prospective clients.

---

# Security Service Providers (MSSP, MSP, MDR)

The LimaCharlie SecOps Cloud Platform (SCP) is a unified platform for modern cybersecurity operations.

The SCP delivers core cybersecurity capabilities and infrastructure via a public cloud model: on-demand, pay-per-use, and API-first. For the cybersecurity industry, this is a paradigm shift comparable to how the IT public cloud revolutionized IT.

For managed security services providers (MSSPs), managed detection and response (MDR) firms, and all those involved in digital forensics and incident response (DFIR), the SecOps Cloud Platform is a powerful way to improve security operations and compete more effectively. With the SCP, service providers can deliver security services at scale, control costs, consolidate and customize security tooling, take on new businesses with confidence, and much more.

The platform's public cloud-like delivery model also helps service providers integrate the SCP into their operations gradually and safely. Flexible pay-as-you-go pricing means you only pay for the capabilities you need, and only for as long as you use themwithout long-term contracts, complex licensing, capacity planning, price modeling, or termination fees.

## Implementation strategies for quick wins

The SecOps Cloud Platform contains numerous capabilities and is designed to be highly flexible and customizable. Nevertheless, there are some common implementation strategies that MSSP users have found to be good starting points with the platform. Here are three easy ways that the SCP can help service providers improve security operations and expand their businesses immediately:

### Gain greater visibility into client environments

The SCP can help service providers gain greater visibility into client environmentsand bring telemetry data under a single plane for a more unified view. This is one of the first realizations of value for service providers using the SCP platform. Here's an outline of what this looks like:

**Decide what telemetry data you need to support security operations.** Your options here are extensive. In the SCP, there are two main sources of telemetry:

First, there are the platform's endpoint detection and response (EDR)-type sensors, which can be deployed directly on Windows, Mac, and Linux endpoints with full feature parity across these OSes to capture system events and other telemetry data. There are also browser-based sensors for Chrome and Edge. Sensors stream telemetry data and artifacts into the SCP in real time (and can also be used to take response actions on endpoints). Importing event data from third-party EDR tools such as VMWare Carbon Black, CrowdStrike, and Microsoft Defender is also possible.

The second source of telemetry data can be classed as log-type data. This data can be brought into the SCP using a system of adapters or via webhook. The options are too numerous to list here in full, but supported log data sources include O365, 1Password, AWS CloudTrail, Google Cloud Platform (GCP), Slack Audit logs, and more. For a more comprehensive list, refer to the SCP documentation.

**Configure client organizations to provide the required visibility.** The SCP web interface makes this as simple as making a few clicks to set up the required installation keys. More advanced configuration management options using a REST API or a command-line interface (CLI) are also available. After setup, your client organizations' configurationsincluding what telemetry you want to bring into the SecOps Cloud Platformwill be stored as simple YAML files. Note here that it's possible to use the SCP's multitenancy and organization management features to make configuration changes to multiple organizations at the same time. For a more detailed example of what this might look like, see this demo MSSP setup.

**Bring your data under a single plane.** All telemetry data brought into the SCP is normalized to a common JSON format and explorable through a single interface. In itself, this represents a huge step forward for many service providers because they will no longer have to deal with a fragmented jumble of UIs or competing data formats in order to view and act on their telemetry data.

**Operationalize your telemetry data.** Seeing into your clients' environments is an essential first stepbut this is only the beginning of what is possible with the SecOps Cloud Platform. The SCP's advanced detection and response engine can act on every piece of telemetry brought into the platform, making it possible to apply sophisticated detection and response logic to telemetry data. Applying D&R logic can be as tailored or as simple as you choose, from using custom detections that you write yourself to leveraging curated rulesets like Sigma, Soteria, or SOC Prime rulesor a combination of both approaches.

It's impossible to protect what you can't see. The SCP makes it possible to gain full visibility into a client environment, visualize that telemetry in a single interface and data format, and take action on telemetry data via a powerful detection, automation, and response engine.

### Implement scalable SecOps and simplified client management

The SecOps Cloud Platform is multitenant by design, offers fine-grained role-based access control (RBAC), and supports an Infrastructure-as-Code (IaC) approach to configuration management. These core aspects of the SCP enable service providers to practice modern cybersecurity operations at scale.

**Separate client environments intelligently.** The multitenancy of the SCP allows service providers to create a logical boundary between their client organizations' data while still being able to view and manage everything from a single platform. Multitenancy makes it easier to avoid commingling client dataand comply with regional regulatory requirements such as data residency rules.

**Manage access and permissions more effectively.** RBAC allows you to grant users the access to organizations and the permissions that they need. You can give individual users permissions on a per-organization basis if you choose. But for more efficient access management, you can use Organization Groups, which are groupings of client organizations, permissions, and users.

Organization Groups give the same permissions and organizational access to any user added to the group. Typically, Organization Groups are set up by job function. For example, you might create an Organization Group for security engineers that allows members to edit telemetry ingestion configurations for all of your client organizations, and a separate Organization Group for non-technical roles that provides read-only access or the ability to view general organizational information.

**Build SecOps workflows that scale.** The SecOps Cloud Platform enables service providers to take an infrastructure-as-code approach to security operations. All of your client organizations' security configurationsfrom D&R rules to data forwarding and output settingscan be stored and managed as simple YAML files.

Create new organizations quickly by cloning an existing organization's configurations or using a configuration template. Maintain a global set of configuration settings for all client organizations and then add per-client config files as needed. If you need to make changes to multiple client organizations, this is as simple as editing a global configuration file via CLI or web UI and pushing out the change to all of your organizations at scale.

The SecOps Cloud Platform helps service providers adopt a truly modern and scalable approach to cybersecurity operations. For a more detailed look at how these SCP concepts work in practice, watch Setting Up an MSSP with LimaCharlie.

### Improve incident response times and offer unbeatable service-level agreements

The SecOps Cloud Platform can be tremendously valuable for service providers doing incident response (IR) work. Here are some of the most significant capabilities for IR teams:

**Begin IR engagements without delay.** The on-demand nature of the SecOps Cloud Platform means you will never need to talk to a vendor sales representative or renegotiate a contract before starting an IR engagement. With the SCP, you log into your account, use a credit card or increase your existing sensor quota, and begin.

In addition, it's possible to preconfigure tenants ahead of an IR engagement. Set up your desired SCP IR configuration using custom D&R rulesets, curated rulesets, memory dump capabilities, YARA scanning, and more. Then, export the configuration files for your IR tenant and reuse them whenever you have a new IR engagement to hit the ground running.

**Take the fight to the adversary.** During IR engagements with an active attacker in the environment, the SecOps Cloud Platform gives you a robust response capability on your client's endpoints.

Mass-deploy SCP sensors using an enterprise deployment tool. Then, use those sensors to gather real-time event data, run shell commands and executables on endpoints, deploy security tools and remediation packages at scale, or isolate compromised machines from the networkall with minimal impact on the client's operations and mission-critical IT infrastructure.

**Use security intelligence as soon as you have it.** The SCP's IaC approach means you don't have to rely on a vendor to update a tool or publish an indicator of compromise (IoC) in an emergency. For example, imagine a scenario in which you're dealing with a 0-day compromise. If you have early access to an IoC via an information-sharing network or a colleague, you can literally copy-paste the relevant IoC data from a Slack message into a new SCP D&R rule, update the relevant config file, and push out the change to your client's environmentwhile the all of the vendor-dependent service providers are still waiting on someone else to act.

**Build a true rapid-response capability.** LimaCharlie sensors can be pre-deployed to client environments in "sleeper" mode: i.e., with the telemetry collection settings tuned down to a bare minimum to keep costs to just pennies per month. If an incident occurs, the sensors are already there, ready and waiting on the endpoints, and can turned on for an immediate response. This use case has allowed SCP service provider partners to offer service-level agreements of as little as 20 minutesa considerable advantage when it comes to pitching (and closing) new MDR or MSSP clients.

IR work is high-stakes and high-pressureand, unfortunately, is far too often complicated by the cumbersome sales processes and technical limitations of legacy cybersecurity vendors. The SCP allows incident responders to take action quickly and independently during an incident. It also lets cybersecurity service providers improve their overall response capabilities, enabling attractive service-level agreements that can help win over prospective clients.

---

# Sigma Converter

LimaCharlie is happy to contribute to the [Sigma Project](https://github.com/SigmaHQ/sigma) by maintaining the LimaCharlie Backend for Sigma, enabling most Sigma rules to be converted to the [Detection & Response rule](/v2/docs/detection-and-response) format.

A LimaCharlie [Service](/v2/docs/sigma-rules) is available to apply [many of those converted rules](https://github.com/refractionPOINT/sigma-limacharlie/tree/rules) with a single click to an Organization.

For cases where you either have your own Sigma rules, or you would like to convert/apply specific rules yourself, the Sigma Converter service described below can help streamline the process.

## Converter Service

The Converter service converts one or many Sigma rules into the LimaCharlie rule format. It can accomplish this via the following HTTPS endpoints available at https://sigma.limacharlie.io/:

### Single Rule

Endpoint: `https://sigma.limacharlie.io/convert/rule`
Verb: `POST`
Form Parameters:

* `rule`: the content of a literal Sigma rule to be converted.
* `target`: optional [target](/v2/docs/detection-on-alternate-targets) within LimaCharlie, one of `edr` (default) or `artifact`.

Output Example:

```
{
    "rule": "detect:\n  events:\n  - NEW_PROCESS\n  - EXISTING_PROCESS\n  op: and\n  rules:\n  - op: is windows\n  - op: or\n    rules:\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainlist\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: trustdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dcmodes\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: adinfo\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' dclist '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computer_pwdnotreqd\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: objectcategory=\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: -subnets -f\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: name=\"Domain Admins\"\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: '-sc u:'\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainncs\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dompol\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' oudmp '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: subnetdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: gpodmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: fspdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: users_noexpire\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computers_active\nrespond:\n- action: report\n  metadata:\n    author: Janantha Marasinghe (https://github.com/blueteam0ps)\n    description: AdFind continues to be seen across majority of breaches. It is used\n      to domain trust discovery to plan out subsequent steps in the attack chain.\n    falsepositives:\n    - Admin activity\n    level: high\n    references:\n    - https://thedfirreport.com/2020/05/08/adfind-recon/\n    - https://thedfirreport.com/2021/01/11/trickbot-still-alive-and-well/\n    - https://www.microsoft.com/security/blog/2021/01/20/deep-dive-into-the-solorigate-second-stage-activation-from-sunburst-to-teardrop-and-raindrop/\n    tags:\n    - attack.discovery\n    - attack.t1482\n    - attack.t1018\n  name: AdFind Usage Detection\n\n"
}
```

CURL Example:

```
curl -X POST  https://sigma.limacharlie.io/convert/rule -H 'content-type: application/x-www-form-urlencoded' --data-urlencode "rule@my-rule-file.yaml"
```

### Multiple Rules

Endpoint: `https://sigma.limacharlie.io/convert/repo`
Verb: `POST`
Form Parameters:

* `repo`: the source where to access the rules to convert, one of:
  + An HTTPS link to a direct resource like: `https://corp.com/my-rules.yaml`
  + A GitHub link to a file or repo like:
    - `https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml`
    - `https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation`
  + An [Authenticated Resource Locator](/v2/docs/reference-authentication-resource-locator)
* `target`: optional [target](/v2/docs/detection-on-alternate-targets) within LimaCharlie, one of `edr` (default) or `artifact`.

Output Example:

```
{
    "rules":[
        {
            "file":"https://raw.githubusercontent.com/SigmaHQ/sigma/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml","rule":"detect:\n  events:\n  - NEW_PROCESS\n  - EXISTING_PROCESS\n  op: and\n  rules:\n  - op: is windows\n  - op: or\n    rules:\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainlist\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: trustdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dcmodes\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: adinfo\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' dclist '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computer_pwdnotreqd\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: objectcategory=\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: -subnets -f\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: name=\"Domain Admins\"\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: '-sc u:'\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainncs\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dompol\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' oudmp '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: subnetdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: gpodmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: fspdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: users_noexpire\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computers_active\nrespond:\n- action: report\n  metadata:\n    author: Janantha Marasinghe (https://github.com/blueteam0ps)\n    description: AdFind continues to be seen across majority of breaches. It is used\n      to domain trust discovery to plan out subsequent steps in the attack chain.\n    falsepositives:\n    - Admin activity\n    level: high\n    references:\n    - https://thedfirreport.com/2020/05/08/adfind-recon/\n    - https://thedfirreport.com/2021/01/11/trickbot-still-alive-and-well/\n    - https://www.microsoft.com/security/blog/2021/01/20/deep-dive-into-the-solorigate-second-stage-activation-from-sunburst-to-teardrop-and-raindrop/\n    tags:\n    - attack.discovery\n    - attack.t1482\n    - attack.t1018\n  name: AdFind Usage Detection\n\n"
        }
    ]
}
```

CURL Example:

```
curl -X POST  https://sigma.limacharlie.io/convert/repo -d "repo=https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml"
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Sigma Converter

LimaCharlie is happy to contribute to the [Sigma Project](https://github.com/SigmaHQ/sigma) by maintaining the LimaCharlie Backend for Sigma, enabling most Sigma rules to be converted to the [Detection & Response rule](/v2/docs/detection-and-response) format.

A LimaCharlie [Service](/v2/docs/sigma-rules) is available to apply [many of those converted rules](https://github.com/refractionPOINT/sigma-limacharlie/tree/rules) with a single click to an Organization.

For cases where you either have your own Sigma rules, or you would like to convert/apply specific rules yourself, the Sigma Converter service described below can help streamline the process.

## Converter Service

The Converter service converts one or many Sigma rules into the LimaCharlie rule format. It can accomplish this via the following HTTPS endpoints available at https://sigma.limacharlie.io/:

### Single Rule

Endpoint: `https://sigma.limacharlie.io/convert/rule`
Verb: `POST`
Form Parameters:

* `rule`: the content of a literal Sigma rule to be converted.
* `target`: optional [target](/v2/docs/detection-on-alternate-targets) within LimaCharlie, one of `edr` (default) or `artifact`.

Output Example:

```
{
    "rule": "detect:\n  events:\n  - NEW_PROCESS\n  - EXISTING_PROCESS\n  op: and\n  rules:\n  - op: is windows\n  - op: or\n    rules:\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainlist\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: trustdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dcmodes\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: adinfo\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' dclist '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computer_pwdnotreqd\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: objectcategory=\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: -subnets -f\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: name=\"Domain Admins\"\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: '-sc u:'\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainncs\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dompol\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' oudmp '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: subnetdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: gpodmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: fspdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: users_noexpire\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computers_active\nrespond:\n- action: report\n  metadata:\n    author: Janantha Marasinghe (https://github.com/blueteam0ps)\n    description: AdFind continues to be seen across majority of breaches. It is used\n      to domain trust discovery to plan out subsequent steps in the attack chain.\n    falsepositives:\n    - Admin activity\n    level: high\n    references:\n    - https://thedfirreport.com/2020/05/08/adfind-recon/\n    - https://thedfirreport.com/2021/01/11/trickbot-still-alive-and-well/\n    - https://www.microsoft.com/security/blog/2021/01/20/deep-dive-into-the-solorigate-second-stage-activation-from-sunburst-to-teardrop-and-raindrop/\n    tags:\n    - attack.discovery\n    - attack.t1482\n    - attack.t1018\n  name: AdFind Usage Detection\n\n"
}
```

CURL Example:

```
curl -X POST  https://sigma.limacharlie.io/convert/rule -H 'content-type: application/x-www-form-urlencoded' --data-urlencode "rule@my-rule-file.yaml"
```

### Multiple Rules

Endpoint: `https://sigma.limacharlie.io/convert/repo`
Verb: `POST`
Form Parameters:

* `repo`: the source where to access the rules to convert, one of:
  * An HTTPS link to a direct resource like: `https://corp.com/my-rules.yaml`
  * A GitHub link to a file or repo like:
    * `https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml`
    * `https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation`
  * An [Authenticated Resource Locator](/v2/docs/reference-authentication-resource-locator)
* `target`: optional [target](/v2/docs/detection-on-alternate-targets) within LimaCharlie, one of `edr` (default) or `artifact`.

Output Example:

```
{
    "rules":[
        {
            "file":"https://raw.githubusercontent.com/SigmaHQ/sigma/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml","rule":"detect:\n  events:\n  - NEW_PROCESS\n  - EXISTING_PROCESS\n  op: and\n  rules:\n  - op: is windows\n  - op: or\n    rules:\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainlist\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: trustdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dcmodes\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: adinfo\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' dclist '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computer_pwdnotreqd\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: objectcategory=\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: -subnets -f\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: name=\"Domain Admins\"\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: '-sc u:'\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: domainncs\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: dompol\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: ' oudmp '\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: subnetdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: gpodmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: fspdmp\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: users_noexpire\n    - case sensitive: false\n      op: contains\n      path: event/COMMAND_LINE\n      value: computers_active\nrespond:\n- action: report\n  metadata:\n    author: Janantha Marasinghe (https://github.com/blueteam0ps)\n    description: AdFind continues to be seen across majority of breaches. It is used\n      to domain trust discovery to plan out subsequent steps in the attack chain.\n    falsepositives:\n    - Admin activity\n    level: high\n    references:\n    - https://thedfirreport.com/2020/05/08/adfind-recon/\n    - https://thedfirreport.com/2021/01/11/trickbot-still-alive-and-well/\n    - https://www.microsoft.com/security/blog/2021/01/20/deep-dive-into-the-solorigate-second-stage-activation-from-sunburst-to-teardrop-and-raindrop/\n    tags:\n    - attack.discovery\n    - attack.t1482\n    - attack.t1018\n  name: AdFind Usage Detection\n\n"
        }
    ]
}
```

CURL Example:

```
curl -X POST  https://sigma.limacharlie.io/convert/repo -d "repo=https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_ad_find_discovery.yml"
```

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Single Sign-On

Single sign-on (SSO) is available at no extra cost for customers that leverage LimaCharlie's custom branded offering. If this applies to your Organization, and if you are interested in using the SSO, please submit a [Custom Branding / SSO Request](https://limacharlie.io/custom-branding).

If your organization does not currently have a custom branded site with LimaCharlie, you can learn about the requirements, costs & get started here.

## Strict SSO Enforcement

LimaCharlie offers the ability to implement strict SSO enforcement. This means that SSO can be configured as the only authentication option.

With this capability, you may say that any user with your email domain @example.com must authenticate via Google. This way you can disable the login + password, GitHub, and Microsoft login options for users with your email domain (@example.com) - regardless if they are logging in via your custom branded site, or via app.limacharlie.io

## How It Works

LimaCharlie's single sign-on functionality lets companies add their own SSO option that goes through their authentication server instead of through Google or something else. Identity Platform acts as the coordinator here. After configuring new Providers in Identity Platform, the app only needs to specify a provider ID, and then Identity Platform will handle talking to the company's auth server.

## User Experience

The high-level user experience is as follows:

* For organizations that choose to use SSO, the SSO will be enforced. Users going to custom branded versions of the LimaCharlie site will be presented with only the option to login through SSO, if their domain has the SSO configuration.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/sso-1.png)

* The same user going to the non-branded site would still be presented with all other authentication options. However, a user would only be able to use the authentification option approved for their domain.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/sso-2.png)

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Sleeper Deployment

LimaCharlie's usage-based billing enables incident responders to offer pre-deployments to their customers at almost zero cost. That is, they can deploy across an Organization's entire fleet and lay dormant in 'sleeper mode' at a cost of just $0.10 per 30 days. With agents deployed ahead of an incident, responders can offer competitive SLAs.

> Have more questions?
>
> For more details on sleeper mode deployments, feel free to contact us at answers@limacharlie.io or book a quick call with the engineering team to discuss your use case.

Sleeper and Usage billing use the following metrics:

| Connected Time | Events Processed | Events Retained |
| --- | --- | --- |
| $0.10 per 30 days | $0.67 per 100,000 events | $0.17 per 100,000 events |

Using sleeper and usage deployments is done via Sensor tagging. Applying the `lc:sleeper` Tag to a Sensor will stop LimaCharlie telemetry collection activity on the host. Within 10 minutes of the tag being applied, the sensor will enter sleeper mode and will be billed only for its "Connected Time" as outlined above. If the tag is removed, normal operations resume within 10 minutes.

Applying the `lc:usage` tag will make the sensor operate normally as usual, but its connection will not count against the normal Sensor Quota. Instead it will be billed per time spend connected and number of events process/retained as outlined above.

Using the "usage" and "sleeper" mode requires the organization in question to have billing enabled (a quota of at least 3 to be outside of the free tier).

This means a sample scenario around pre-deploying in an enterprise could look something like this:

1. Create a new Organization in LimaCharlie.
2. Set the Quota to 3 to enable billing.
3. Create a new Installation Key, and set the `lc:sleeper` tag on the key.
4. Enroll any number of EDR sensors. Charges will apply as specified above. For example, if you deploy 100 Sensors in sleeper mode, total monthly costs will be $10.
5. Whenever you need to "wake up" and use some of the EDRs, you have 2 options:

   1. Set the `lc:usage` tag on the Sensor(s) you need. Within 10 minutes, telemetry collection will resume and billed on direct usage.
   2. Set the quota to the number of Sensor(s) you need, remove the `lc:sleeper` tag from the specific Sensors, and within 10 minutes they will be online, billed according to the quota.
6. When you're done, just re-add the `lc:sleeper` tag.

Switching to sleeper mode does not change the binary on disk, however, the code running in memory does change. Whether putting an org into sleeper mode or changing versions, the binary on disk remains as-is.

The changes to sleeper mode go into effect without the need for a reboot. In sleeper mode, activities such as read other process' memory (e.g. YARA) will stop.

## Key Concepts

### Organizations

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

### Tags

Tags in LimaCharlie are strings linked to sensors for classifying endpoints, automating detection and response, and triggering workflows. Tags appear in every event under the `routing` component and help simplify rule writing. Tags can be added manually, via API, or through detection & response rules. System tags like `lc:latest`, `lc:stable`, and `lc:debug` offer special functionality. Tags can be checked, added, or removed through the API or web app, streamlining device management.

### Sensors

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

### Installation Keys

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Sleeper Deployment

LimaCharlie's usage-based billing enables incident responders to offer pre-deployments to their customers at almost zero cost. That is, they can deploy across an Organization's entire fleet and lay dormant in 'sleeper mode' at a cost of just $0.10 per 30 days. With agents deployed ahead of an incident, responders can offer competitive SLAs.

> Have more questions?
>
> For more details on sleeper mode deployments, feel free to contact us at answers@limacharlie.io or book a quick call with the engineering team to discuss your use case.

Sleeper and Usage billing use the following metrics:

| Connected Time | Events Processed | Events Retained |
| --- | --- | --- |
| $0.10 per 30 days | $0.67 per 100,000 events | $0.17 per 100,000 events |

Using sleeper and usage deployments is done via Sensor tagging. Applying the `lc:sleeper` Tag to a Sensor will stop LimaCharlie telemetry collection activity on the host. Within 10 minutes of the tag being applied, the sensor will enter sleeper mode and will be billed only for its "Connected Time" as outlined above. If the tag is removed, normal operations resume within 10 minutes.

Applying the `lc:usage` tag will make the sensor operate normally as usual, but its connection will not count against the normal Sensor Quota. Instead it will be billed per time spend connected and number of events process/retained as outlined above.

Using the "usage" and "sleeper" mode requires the organization in question to have billing enabled (a quota of at least 3 to be outside of the free tier).

This means a sample scenario around pre-deploying in an enterprise could look something like this:

1. Create a new Organization in LimaCharlie.
2. Set the Quota to 3 to enable billing.
3. Create a new Installation Key, and set the `lc:sleeper` tag on the key.
4. Enroll any number of EDR sensors. Charges will apply as specified above. For example, if you deploy 100 Sensors in sleeper mode, total monthly costs will be $10.
5. Whenever you need to "wake up" and use some of the EDRs, you have 2 options:

   1. Set the `lc:usage` tag on the Sensor(s) you need. Within 10 minutes, telemetry collection will resume and billed on direct usage.
   2. Set the quota to the number of Sensor(s) you need, remove the `lc:sleeper` tag from the specific Sensors, and within 10 minutes they will be online, billed according to the quota.
6. When you're done, just re-add the `lc:sleeper` tag.

Switching to sleeper mode does not change the binary on disk, however, the code running in memory does change. Whether putting an org into sleeper mode or changing versions, the binary on disk remains as-is.

The changes to sleeper mode go into effect without the need for a reboot. In sleeper mode, activities such as read other process' memory (e.g. YARA) will stop.

## Key Concepts

**Organization**: An Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Tags**: Tags in LimaCharlie are strings linked to sensors for classifying endpoints, automating detection and response, and triggering workflows. Tags appear in every event under the `routing` component and help simplify rule writing. Tags can be added manually, via API, or through detection & response rules. System tags like `lc:latest`, `lc:stable`, and `lc:debug` offer special functionality. Tags can be checked, added, or removed through the API or web app, streamlining device management.

**Sensors**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Installation Keys**: Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Sleeper Mode

LimaCharlie's sleeper mode is not just about cost-efficiency; it's about transforming your entire network into a dynamic, responsive security infrastructure. By pre-deploying sensors and strategically activating them during incidents, you gain the element of surprise, optimize resource allocation, and ultimately, mitigate the impact of cyberattacks with unmatched agility.

## Traditional sensor deployment problems

Traditional IR relies on reactive deployment of sensors, leaving critical blind spots during early stages of an incident. Delays in gaining visibility slow down response times and increase damage potential.

* **Limited visibility:** Lack of visibility during the early stages of an incident due to the absence of pre-deployed sensors.
* **Manual processes:** Delayed response times caused by the need to manually deploy sensors after an incident has been detected.
* **Delayed response:** Increased potential for damage and lateral movement of threats while waiting for sensor deployment and data collection.

## LimaCharlie's solution

Sleeper mode transforms your entire network into a pre-wired security grid. Sensors sit silently, consuming minimal resources while collecting basic system information and detecting critical events. This provides:

* **Instant Activation, Rapid Response:** Need deep process monitoring or memory forensic capabilities? Instantly activate sleepers within the affected area, gaining full-fledged EDR visibility for targeted investigation and containment. No more waiting for manual installation during critical moments.
* **Surgical Precision:** Focus resources where they matter most. Activate sleepers only on specific endpoints or clusters suspected of involvement, reducing unnecessary data collection and analysis overload. This streamlines investigations and saves valuable time.
* **Critical Assets Under Cover:** Pre-deploy sensors in sleeper mode on high-value servers, executive machines, or sensitive data repositories. When an incident strikes, instant activation grants immediate visibility and control, safeguarding your most crucial assets.
* **Targeted Threat Hunting:** Identify potential targets based on threat intelligence or internal red teaming exercises. Pre-emptively activate sleepers in these areas, creating a proactive surveillance network to catch early signs of malicious activity.
* **Isolate and Contain:** Sleeper mode empowers swift containment. Upon detecting suspicious activity, activate neighboring sleepers to cordon off the affected area, preventing lateral movement and limiting damage.
* **Deep Dive Forensics:** Need detailed forensic disk or memory analysis? Activate the relevant sleeper for comprehensive forensic investigation, dissecting the incident and identifying root causes for future prevention.

---

# Sysmon Comparison

System Monitor, or "Sysmon", is a Windows server and device driver that monitors and logs operating system activity. It is part of the Sysinternals toolkit. More information on Sysmon can be found [here](https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon).

Many organizations deploy Sysmon and structure their detection events around Sysmon-specific event logs, which can offer granular insight into operating system changes. LimaCharlie's EDR telemetry can offer similar events, allowing you to write detections against these events directly.

A comparison of LimaCharlie vs. Sysmon is as follows:

| Sysmon Event | LimaCharlie Event |
| --- | --- |
| Event ID 1 (Process Creation) | NEW\_PROCESS |
| Event ID 3 (Network Connection) | NEW\_\*\_CONNECTION |
| Event ID 5 (Process terminated) | TERMINATE\_PROCESS |
| Event ID 6 (Driver Loaded) | MODULE\_LOAD, CODE\_IDENTITY, DRIVER\_CHANGE |
| Event ID 7 (Image loaded) | MODULE\_LOAD, CODE\_IDENTITY |
| Event ID 8 (Create remote thread) | NEW\_REMOTE\_THREAD |
| Event ID 10 (ProcessAccess) | REMOTE\_PROCESS\_HANDLE |
| Event ID 11 (FileCreate) | FILE\_CREATE |
| Event ID 12 (RegistryEvent object create and delete) | REGISTRY\_CREATE, REGISTRY\_DELETE |
| Event ID 13 (RegisterEvent value set) | REGISTRY\_WRITE |
| Event ID 14 (RegistryEvent rename) | REGISTRY\_CREATE |
| Event ID 17 (PipeEvent Created) | NEW\_NAMED\_PIPE |
| Event ID 18 (PipeEvent Connected) | OPEN\_NAMED\_PIPE |

Why not both? \*()*/

Note, LC's Endpoint Agent is easily able to [consume Sysmon events](/v2/docs/ingesting-sysmon-event-logs) as well.

## Executable Tracking

Recent updates to Sysmon also include the ability to capture and store information about binaries identified on a system. You can replicate this functionality with LimaCharlie with BinLib. More information on that can be found [here](/v2/docs/binlib).

Endpoint Detection & Response

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

---

# Sysmon Comparison

System Monitor, or "Sysmon", is a Windows server and device driver that monitors and logs operating system activity. It is part of the Sysinternals toolkit. More information on Sysmon can be found [here](https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon).

Many organizations deploy Sysmon and structure their detection events around Sysmon-specific event logs, which can offer granular insight into operating system changes. LimaCharlie's EDR telemetry can offer similar events, allowing you to write detections against these events directly.

A comparison of LimaCharlie vs. Sysmon is as follows:

| Sysmon Event | LimaCharlie Event |
| --- | --- |
| Event ID 1 (Process Creation) | NEW\_PROCESS |
| Event ID 3 (Network Connection) | NEW\_\*\_CONNECTION |
| Event ID 5 (Process terminated) | TERMINATE\_PROCESS |
| Event ID 6 (Driver Loaded) | MODULE\_LOAD, CODE\_IDENTITY, DRIVER\_CHANGE |
| Event ID 7 (Image loaded) | MODULE\_LOAD, CODE\_IDENTITY |
| Event ID 8 (Create remote thread) | NEW\_REMOTE\_THREAD |
| Event ID 10 (ProcessAccess) | REMOTE\_PROCESS\_HANDLE |
| Event ID 11 (FileCreate) | FILE\_CREATE |
| Event ID 12 (RegistryEvent object create and delete) | REGISTRY\_CREATE, REGISTRY\_DELETE |
| Event ID 13 (RegisterEvent value set) | REGISTRY\_WRITE |
| Event ID 14 (RegistryEvent rename) | REGISTRY\_CREATE |
| Event ID 17 (PipeEvent Created) | NEW\_NAMED\_PIPE |
| Event ID 18 (PipeEvent Connected) | OPEN\_NAMED\_PIPE |

Why not both? \*()*/

Note, LC's Endpoint Agent is easily able to [consume Sysmon events](/v2/docs/ingesting-sysmon-event-logs) as well.

## Executable Tracking

Recent updates to Sysmon also include the ability to capture and store information about binaries identified on a system. You can replicate this functionality with LimaCharlie with BinLib. More information on that can be found [here](/v2/docs/binlib).

---

# Table Top Exercises

Enhance your security preparedness table-top exercises with LimaCharlie. Conduct realistic multi-platform simulations, optimize incident response procedures, and empower your team to face evolving cyber threats with confidence.

## Problems with table top exercises

* **Costly and time-consuming setup:** Traditional tabletop exercises (TTX) often require dedicated infrastructure, extensive preparation, and significant resource investment, making them infrequent and burdensome.
* **Limited platform capabilities:** Many TTX platforms lack cross-platform support, realistic attack simulations, and comprehensive reporting, hindering effective incident response training.
* **Fragmented security awareness:** Traditional scenarios might not encompass multi-platform environments and evolving attack vectors, leaving security teams unprepared for real-world threats.

## LimaCharlie's solution

* **Pay-as-you-go platform:** Access LimaCharlie's fully featured capabilities with flexible, pay-as-you-go pricing. Conduct exercises as needed without costly upfront investments or long-term contracts.
* **Rapid deployment and customization:** Get started with LimaCharlie's quickly and easily. Launch realistic cyber attack simulations within minutes, tailored to your specific environment and security concerns.
* **Comprehensive multi-platform simulations:** Test your team's response to complex attack scenarios across Windows, macOS, Linux, and cloud environments, mimicking real-world threats. Leverage LimaCharlie's API to integrate your existing security tools, creating a truly holistic response simulation.
* **Detailed reporting and insights:** Gain valuable insights from post-exercise reports that analyze team performance, identify skill gaps, and offer actionable recommendations for improving your incident response procedures.
* **Customizable attack libraries:** Utilize pre-built or customize attack scenarios, based on Atomic Red Team, to run your training exercises. Based on your specific industry, threat landscape, and vulnerabilities, ensure you conduct relevant and valuable training exercises.

---

# Table Top Exercises

Enhance your security preparedness table-top exercises with LimaCharlie. Conduct realistic multi-platform simulations, optimize incident response procedures, and empower your team to face evolving cyber threats with confidence.

## Problems with table top exercises

* **Costly and time-consuming setup:** Traditional tabletop exercises (TTX) often require dedicated infrastructure, extensive preparation, and significant resource investment, making them infrequent and burdensome.
* **Limited platform capabilities:** Many TTX platforms lack cross-platform support, realistic attack simulations, and comprehensive reporting, hindering effective incident response training.
* **Fragmented security awareness:** Traditional scenarios might not encompass multi-platform environments and evolving attack vectors, leaving security teams unprepared for real-world threats.

## LimaCharlie's solution

* **Pay-as-you-go platform:** Access LimaCharlie's fully featured capabilities with flexible, pay-as-you-go pricing. Conduct exercises as needed without costly upfront investments or long-term contracts.
* **Rapid deployment and customization:** Get started with LimaCharlie's quickly and easily. Launch realistic cyber attack simulations within minutes, tailored to your specific environment and security concerns.
* **Comprehensive multi-platform simulations:** Test your team's response to complex attack scenarios across Windows, macOS, Linux, and cloud environments, mimicking real-world threats. Leverage LimaCharlie's API to integrate your existing security tools, creating a truly holistic response simulation.
* **Detailed reporting and insights:** Gain valuable insights from post-exercise reports that analyze team performance, identify skill gaps, and offer actionable recommendations for improving your incident response procedures.
* **Customizable attack libraries:** Utilize pre-built or customize attack scenarios, based on Atomic Red Team, to run your training exercises. Based on your specific industry, threat landscape, and vulnerabilities, ensure you conduct relevant and valuable training exercises.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in Adapter configuration. These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area. In this example we are transforming a detection event to pass via a custom webhook to a web application.

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using Template Strings.

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:
  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in Adapter configuration. These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using Template Strings.

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in [Adapter configuration](/v2/docs/adapter-usage). These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area of the screenshot below. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using [Template Strings](/v2/docs/template-strings-and-transforms).

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in Adapter configuration. These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area of the screenshot below. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using Template Strings.

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in [Adapter configuration](/v2/docs/adapter-usage). These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area of the screenshot below. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using [Template Strings](/v2/docs/template-strings-and-transforms).

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in Adapter configuration. These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area of the screenshot below. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using Template Strings.

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Template Strings and Transforms

Many areas of LimaCharlie support template strings and transforms.

A template string allows you to customize the value of a configuration based on the context. For example to adjust the Detection Name a D&R rule to include a value from the detection itself. Transforms can also be used to select, modify, or remove fields upon data ingestion from an Adapter.

A transform allows you to change the shape of JSON data in flight to suit better your usage. This can mean moving, renaming, removing and adding fields in JSON. For example, it can allow you to create an Output that works with `DNS_REQUEST` events, but outputs only specific fields from the event.

## Template Strings

Template strings in LimaCharlie use the format defined by "text templates" found [here](https://pkg.go.dev/text/template). A useful guide provided by Hashicorp is also available [here](https://learn.hashicorp.com/tutorials/nomad/go-template-syntax).

The most basic example for a D&R rule customizing the detection name looks like this:

```
- action: report
  name: Evil executable on {{ .routing.hostname }}
```

Template strings also support some LimaCharlie-specific functions:

* `token`: applies an MD5 hashing function on the value provided.
* `anon`: applies an MD5 hashing function on a secret seed value, plus the value provided.
* `json`: marshals the input into a JSON string representation.
* `prettyjson`: same as `json` but with indentation and newlines.
* `parsetime`: parse a time format to another.
* `split`: split a string based on a seperator param.
* `join`: join a list into a string joined by another string.
* `replace`: replace all string into the other.
* `base`: return the file name in a file path.
* `dir`: return the base directory path from a file path.

The `token` and `anon` functions can be used to partially anonymize data anywhere a template string is supported, for example:

```
- action: report
  name: 'User {{token .event.USER_NAME }} accessed a website against policy.'
```

Other examples:

* `Full Data: {{prettyjson .event.OBJECT }}`
* `Original time:{{parsetime "{\"from\":\"2006/01/02 15:04:05\", \"to\":\"2006-01-02 15:04:05 MST\"}" .event.timestamp}}`
* `Packages: {{join "," .event.PACKAGES}}`

### Template Strings and Adapter Transforms

Template strings can also be used with in conjunction the `client_options.mapping.transform` option in Adapter configuration. These allow you to modify data prior to ingestion, having control over *what* fields get ingested and resulting field names.

The following options are available in Adapter configurations:

* `+` to add a field
* `-` to remove a field

Both support template strings, meaning you can add/remove values from the JSON data to replace/supplement other fields.

For example, if we had the following data:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "d" : 3
    }
  }
}
```

And we wanted to rename the `d` value to `c` on ingestion, remove the d value, and add a field called `hostname`, we could use the following configuration:

```
...
   client_options:
     mapping:
       transform:
         +c : '{{ .webster.d }}',
         -d: nil,
         +hostname : '{{ "my-computer" }}',
```

The resulting event to be ingested would be:

```
{ "event":
  "webster" : {
     "a" : 1,
     "b" : 2,
     "c" : 3
    },
    "hostname" : "my-computer"
  }
}
```

## Transforms

With Transforms, you specify a JSON object that describes the transformation.

This object is in the shape of the final JSON you would like to transform to.

Key names are the literal key names in the output. Values support one of 3 types:

1. Template Strings, as described above. In this case, the template string will be generated and placed at the same place as the key in the transform object.
2. A `gjson` selector. The selector syntaxt is defined [here](https://github.com/tidwall/gjson/blob/master/SYNTAX.md). It makes it possible to select subsets of input object and map it within the resulting object as defined by the transform.
3. Other JSON objects which will be present in the output.

Let's look at an example, let's say this is the Input to our transform:

```
{
    "event": {
        "EVENT": {
            "EventData": {
                "AuthenticationPackageName": "NTLM",
                "FailureReason":             "%%2313",
                "IpAddress":                 "34.64.101.177",
                "IpPort":                    "0",
                "KeyLength":                 "0",
                "LmPackageName":             "-",
                "LogonProcessName":          "NtLmSsp",
                "LogonType":                 "3",
                "ProcessId":                 "0x0",
                "ProcessName":               "-",
                "Status":                    "0xc000006d",
                "SubStatus":                 "0xc0000064",
                "SubjectDomainName":         "-",
                "SubjectLogonId":            "0x0",
                "SubjectUserName":           "-",
                "SubjectUserSid":            "S-1-0-0",
                "TargetDomainName":          "",
                "TargetUserName":            "ADMINISTRADOR",
                "TargetUserSid":             "S-1-0-0",
                "TransmittedServices":       "-",
                "WorkstationName":           "-",
            },
            "System": {
                "Channel":  "Security",
                "Computer": "demo-win-2016",
                "Correlation": {
                    "ActivityID": "{F207C050-075F-0001-AFE1-ED1F3897D801}",
                },
                "EventID":       "4625",
                "EventRecordID": "2832700",
                "Execution": {
                    "ProcessID": "572",
                    "ThreadID":  "2352",
                },
                "Keywords": "0x8010000000000000",
                "Level":    "0",
                "Opcode":   "0",
                "Provider": {
                    "Guid": "{54849625-5478-4994-A5BA-3E3B0328C30D}",
                    "Name": "Microsoft-Windows-Security-Auditing",
                },
                "Security": "",
                "Task":     "12544",
                "TimeCreated": {
                    "SystemTime": "2022-07-15T22:48:24.996361600Z",
                },
                "Version": "0",
            },
        },
    },
    "routing": {
        "arch":       2,
        "did":        "b97e9d00-ca17-4afe-a9cf-27c3468d5901",
        "event_id":   "f24679e5-5484-4ca1-bee2-bfa09a5ba3db",
        "event_time": 1657925305984,
        "event_type": "WEL",
        "ext_ip":     "35.184.178.65",
        "hostname":   "demo-win-2016.c.lc-demo-infra.internal",
        "iid":        "7d23bee6-aaaa-aaaa-aaaa-c8e8cca132a1",
        "int_ip":     "10.128.0.2",
        "moduleid":   2,
        "oid":        "8cbe27f4-aaaa-aaaa-aaaa-138cd51389cd",
        "plat":       268435456,
        "sid":        "bb4b30af-ff11-4ff4-836f-f014ada33345",
        "tags": [
            "edr",
            "lc:stable",
        ],
        "this": "c5e16360c71baf3492f2dcd962d1eeb9",
    },
    "ts": "2022-07-15 22:48:25",
}
```

And this is our Transform definition:

```
{
    "message": "Interesting event from {{ .routing.hostname }}",  // a format string
    "from":    "{{ \"limacharlie\" }}",                           // a format string with only a literal value
    "dat": {                                                      // define a sub-object in the output
        "raw": "event.EVENT.EventData"                            // a "raw" key where we map a specific object from the input
    },
    "anon_ip": "{{anon .routing.int_ip }}",                       // an anonymized version of the internal IP
    "ts":   "routing.event_time",                                 // map a specific simple value
    "nope": "does.not.exist"                                      // map a value that is not present
}
```

Then the resulting Output would be:

```
{
    "dat": {
        "raw": {
            "AuthenticationPackageName": "NTLM",
            "FailureReason": "%%2313",
            "IpAddress": "34.64.101.177",
            "IpPort": "0",
            "KeyLength": "0",
            "LmPackageName": "-",
            "LogonProcessName": "NtLmSsp",
            "LogonType": "3",
            "ProcessId": "0x0",
            "ProcessName": "-",
            "Status": "0xc000006d",
            "SubStatus": "0xc0000064",
            "SubjectDomainName": "-",
            "SubjectLogonId": "0x0",
            "SubjectUserName": "-",
            "SubjectUserSid": "S-1-0-0",
            "TargetDomainName": "",
            "TargetUserName": "ADMINISTRADOR",
            "TargetUserSid": "S-1-0-0",
            "TransmittedServices": "-",
            "WorkstationName": "-"
        }
    },
    "from": "limacharlie",
    "message": "Interesting event from demo-win-2016.c.lc-demo-infra.internal",
    "nope": null,
    "ts": 1657925305984,
    "anon_ip": "e80b5017098950fc58aad83c8c14978e"
}
```

### Transforming Output Data

When passing events to an output, you have the option to transform the original event in multiple ways. When creating an output, Custom Transforms are applied in the CUSTOM TRANSFORM area of the screenshot below. In this example we are transforming a detection event to pass via a custom webhook to a web application.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(310).png)

### Examples

#### Extracting Fields from Telemetry

Let's say you have the following 4625 failed logon and you want to send similar events to an output, but only certain fields.

```
{
  "event": {
    "EVENT": {
      "EventData": {
        "AuthenticationPackageName": "NTLM",
        "FailureReason": "%%2313",
        "IpAddress": "142.99.21.14",
        # <extra fields removed>
        "TargetUserName": "administrator",
        "WorkstationName": "D-483"
      },
      "System": {
        "Channel": "Security",
        "Computer": "demo-win-2016",
        # <extra fields removed>
        "EventID": "4625",
        "EventRecordID": "22690646",
        # <extra fields removed>
        "TimeCreated": {
          "SystemTime": "2024-01-23T17:30:07.345840000Z"
        },
        "Version": "0",
        "_event_id": "4625"
      }
    }
  },
  "routing": {
    # <extra fields removed>
    "event_type": "WEL",
    "hostname": "win-2016.corp.internal",
     # <extra fields removed>
    "tags": [
      "windows"
    ],
    "this": "8873fb9fcb26e2c0d4299ce765aff77d"
  },
  "ts": "2024-01-23 17:29:33"
}
```

The following Output Transform would extract only the `IpAddress`, `TargetUserName`, `EventID`, and `SystemTime` the event was created. Notice, the newly mapped field names can be whatever you want.

```
{
    "Source IP": "event.EVENT.EventData.IpAddress",
    "Username": "event.EVENT.EventData.TargetUserName",
    "Event ID": "event.EVENT.System.EventID",
    "Happened at": "event.EVENT.System.TimeCreated.SystemTime"
}
```

The following example outputs text and specified fields using Template Strings.

```
{
  "text": "Failed logon by {{ .event.EVENT.EventData.TargetUserName }} on {{ .routing.hostname }}"
}
```

The above example would generate the following output using the provided sample WEL.

```
{
  "text": "Failed logon by administrator on win-2016.corp.internal"
}
```

### Output as String / Passthrough

The `custom_transform` in outputs can also be used to output pure text (non-JSON) from LimaCharlie. This is useful if, for example, you are ingesting syslog data, and want to forward this syslog data as-is to something else.

This is accomplished by specifying a Template String in the `custom_transform` field instead of a Transform. In those cases, when LimaCharlie determines the `custom_transform` string is not a valid Transform, it will interpret it as a Template String like:

```
{
    "custom_transform": "{{ .event.text }}"
}
```

or

```
{
    "custom_transform": "some text {{json .event.some_field }}"
}
```

### Custom Modifiers

Beyond the built-in modifiers for `gjson` (as seen in their [playground](https://gjson.dev/), LimaCharlie also implements several new modifiers:

* `parsejson`: this modifier takes no arguments, it takes in as input a string that represents a JSON object and outputs the decoded JSON object.
* `extract`: this modifier takes a single argument, `re` which is a regular expression that uses "named capture groups" (as defined in the [re2 documentation](https://github.com/google/re2/wiki/Syntax)). The group names become the keys of the output JSON object with the matching values.
* `parsetime`: this modifier takes two arguments, `from` and `to`. It will convert an input string from a given time format (as defined in the Go `time` library format [here](https://pkg.go.dev/time#pkg-constants)) and outputs the resulting time in the `to` format. Beyond the time constants from the previous link, LimaCharlie also supports a `from` format of:

  + `epoch_s`: a second based epoch timestamp
  + `epoch_ms`: a millisecond based epoch timestamp

For example:
The transform:

```
{
  "new_ts": "ts|@parsetime:{\"from\":\"2006-01-02 15:04:05\", \"to\":\"Mon, 02 Jan 2006 15:04:05 MST\"}",
  "user": "origin|@extract:{\"re\":\".*@(?P<domain>.+)\"}"
  "ctx": "event.EVENT.exec_context|@parsejson"
}
```

applied to:

```
{
  "ts": "2023-05-10 22:35:48",
  "origin": "someuser@gmail.com",
  "event": {
    "EVENT": {
      "exec_context": "{\"some\": \"embeded value\"}"
    }
  }
}
```

would result in:

```
{
  "new_ts": "Wed, 10 May 2023 22:35:48 UTC",
  "user": {
    "domain": "gmail.com\""
  },
  "ctx": {
    "some": "embeded value"
  }
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Threat Hunting

Stop settling for static defenses  become a threat hunting powerhouse with LimaCharlie! One-year historical data, intuitive exploration, and seamless rule creation empower you to uncover hidden threats, predict future attacks, and continuously optimize your security posture for maximum resilience.

## Threat hunting problems

* **Limited visibility into past activity:** Traditional security solutions focus on real-time threats, leaving hidden attacker footprints and lingering malware remnants undetected in historical data.
* **Cumbersome historical data analysis:** Complex log aggregation and analysis tools hinder efficient threat hunting investigations across vast datasets, delaying threat discovery and response.
* **Static detection and response:** The disconnect between reactive threat hunts and proactive defense leaves organizations vulnerable to future attacks from similar tactics, techniques, and procedures (TTPs).

## LimaCharlie's solution

* **Deep Dive into One Year of Data:** Explore past events, analyze suspicious activities, and uncover hidden threats with LimaCharlie's one-year historical data storage. Don't let potential attacker footprints remain invisible.
* **Effortless Exploration with Intuitive Queries:** Utilize LimaCharlie's powerful search engine and pre-built queries to navigate historical data with ease. Find connections, identify anomalies, and conduct in-depth investigations without cumbersome tools.
* **From Hunt to RuleSeamless Transformation:** Easily convert your threat hunting discoveries into actionable detection and response rules within LimaCharlie. Automate future defense against similar attacks by leveraging insights from your historical investigations, closing the loop between reactive hunting and proactive prevention.
* **Continuous OptimizationA Cycle of Resilience:** Re-run historical threat hunts with evolving queries and filters to adapt your detection and response rules as the threat landscape changes. Continuously refine your defenses based on new insights and stay ahead of adversaries.

---

# Twilio

## Overview

The [Twilio](https://www.twilio.com/) Extension allows you to send messages within Twilio. It requires you to setup the Twilio authentication in the **Integrations** section of your Organization.

Some more detailed information is available [here](https://www.twilio.com/docs/sms/send-messages).

## Setup

To start leveraging the Twilio extension, first subscribe to the `ext-twilio` add-on that can be accessed from the LimaCharlie **Marketplace**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/twilio.png)

After you have subscribed to the extension, setup the Twilio authentication in the `Secrets Manager` section of your organization.

Authentication in Twilio uses two components--a SID and a token. The LimaCharlie Twilio secret will combine both components in a single field like `SID/TOKEN`.

#### Detection & Response

Example Response portion of a rule that sends a message out via Twilio as the response action:

```
- action: extension request
  extension action: run
  extension name: ext-twilio
  extension request:
    body: '{{ .event }}'
    from: '{{ "+10123456789" }}'
    to: '{{ "+10123456789" }}'
```

*Note that the* `{{ .event }}` *in the example above is the actual text that would be sent to the number you specify.*

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Twilio

## Overview

The [Twilio](https://www.twilio.com/) Extension allows you to send messages within Twilio. It requires you to setup the Twilio authentication in the **Integrations** section of your Organization.

Some more detailed information is available [here](https://www.twilio.com/docs/sms/send-messages).

## Setup

To start leveraging the Twilio extension, first subscribe to the `ext-twilio` add-on that can be accessed from the LimaCharlie **Marketplace**.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/twilio.png)

After you have subscribed to the extension, setup the Twilio authentication in the `Secrets Manager` section of your organization.

Authentication in Twilio uses two components--a SID and a token. The LimaCharlie Twilio secret will combine both components in a single field like `SID/TOKEN`.

### Detection & Response

Example Response portion of a rule that sends a message out via Twilio as the response action:

```
- action: extension request
  extension action: run
  extension name: ext-twilio
  extension request:
    body: '{{ .event }}'
    from: '{{ "+10123456789" }}'
    to: '{{ "+10123456789" }}'
```

*Note that the* `{{ .event }}` *in the example above is the actual text that would be sent to the number you specify.*

---

# Uncovering Adversary Techniques

LimaCharlie's SecOps Cloud Platform provides a comprehensive approach to combating ransomware, focusing on early detection during the reconnaissance stage and rapid response in the event of a detonation. By gathering telemetry from a wide range of sources, enabling widespread deployment, and leveraging real-time response capabilities, LimaCharlie empowers organizations to effectively detect, stop, and mitigate ransomware attacks, minimizing damage and ensuring business continuity.

## Problems with uncovering adversary techniques

Ransomware attacks have become increasingly sophisticated and targeted, posing a significant threat to organizations of all sizes. The challenges in effectively combating ransomware include:

* **Extended dwell time:** Ransomware attacks often involve weeks or months of reconnaissance, during which malicious actors seek to identify optimal detonation points. Detecting and stopping the attack during this stage is crucial but challenging.
* **Difficulty in correlating data:** Malicious actors often move around and attempt to hide their presence, making it difficult to identify and correlate their activities across various systems and data sources.
* **Rapid spread and damage:** In the event of a successful ransomware detonation, the malware can spread rapidly, encrypting files and causing significant damage before security teams can respond.

## LimaCharlie's solution

LimaCharlie's SecOps Cloud Platform offers a comprehensive approach to combating ransomware, focusing on early detection during the reconnaissance stage and rapid response in the event of a detonation:

* **Comprehensive telemetry gathering:** LimaCharlie gathers telemetry and external artifacts from a wide range of sources, including endpoints, networks, and cloud environments. By normalizing all data to JSON and processing it through the SecOps Cloud Platform's detection, automation, and response engine, LimaCharlie gains a global view of the organization's security posture, enabling it to identify suspicious activities and correlations that may indicate a ransomware attack in progress.
* **Early detection through widespread deployment:** LimaCharlie's ability to deploy everywhere allows it to detect intruders faster than the competition, often before malicious actors can lay an effective trap. By monitoring everything from one place and leveraging advanced detection logic, LimaCharlie can identify and stop ransomware attacks during the crucial reconnaissance stage.
* **Real-time response with semi-persistent TLS connection:** In the event of a ransomware detonation, LimaCharlie's real-time, semi-persistent TLS connection with endpoints enables an unparalleled response capability. If detection logic is in place to catch a ransomware event, response actions can be taken across the entire fleet in real-time. This allows security teams to instantly isolate affected machines from the network while maintaining command and control through LimaCharlie, minimizing further damage and data exfiltration.
* **Advanced threat hunting and remediation:** With LimaCharlie, analysts responding to a ransomware event have access to all affected machines and a full year's history of telemetry. This enables them to run remediation scripts on the endpoints, kill process trees, and hunt for any malicious presence. By leveraging advanced indicators, such as FILE_TYPE_ACCESSED events, security teams can detect ransomware detonation events before the malware proliferates, significantly reducing the impact of the attack.

---

# Unit Tests

## Rules Unit Tests

A D&R rule record can optionally contain unit tests. These tests describe events that should match, and events that should not match. When a D&R rule is updated or created, LimaCharlie will simulate the rules and if the tests fail, an error is produced.

### Structure

A typical D&R rule looks like:

```
{
  "detect": {...},
  "respond": [
    {},
    {}
  ],
  "tests": {
    "match": [],
    "non_match": []
  }
}
```

The `match` and `non_match` both have the same format: they contain a list of lists of events. Each top list element is a unit test, and the content of a test is a list of events as would be seen by LimaCharlie. The reason for the test to be a list is to accomodate for [Stateful Detections](/v2/docs/stateful-rules) which operate across multiple events.

Here's an example:

```
{
  "tests": {
    "match": [
      [{"event": ...}, {"event": ...}, {"event": ...}],
      [{"event": ...}],
      [{"event": ...}]
    ],
    "non_match": [
      [{"event": ...}, {"event": ...}],
      [{"event": ...}]
    ]
  }
}
```

### Example

```
version: 3
hives:
    dr-general:
        "CobaltStrike Named Pipe Patterns":
            data:
                detect:
                    event: WEL
                    op: and
                    rules:
                      - op: and
                        rules:
                        - op: or
                          rules:
                          - case sensitive: false
                            op: is
                            path: event/EVENT/System/_event_id
                            value: '17'
                          - case sensitive: false
                            op: is
                            path: event/EVENT/System/_event_id
                            value: '18'
                        - case sensitive: false
                          op: is
                          path: event/EVENT/System/Channel
                          value: Microsoft-Windows-Sysmon/Operational
                      - op: or
                        rules:
                        - op: or
                          rules:
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \mojo.5688.8052.183894939787088877
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \mojo.5688.8052.35780273329370473
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \mypipe-f
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \mypipe-h
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \ntsvcs
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \scerpc
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \win_svc
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \spoolss
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \msrpc_
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \win\msrpc_
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \wkssvc
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \windows.update.manager
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \SearchTextHarvester
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \DserNamePipe
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \PGMessagePipe
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \MsFteWds
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \fullduplex_
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \rpc_
                        - op: or
                          rules:
                          - case sensitive: false
                            op: is
                            path: event/EVENT/EventData/PipeName
                            value: \demoagent_11
                          - case sensitive: false
                            op: is
                            path: event/EVENT/EventData/PipeName
                            value: \demoagent_22
                        - op: matches
                          path: event/EVENT/EventData/PipeName
                          re: \\f4c3[0-9a-f]{2}$
                        - op: matches
                          path: event/EVENT/EventData/PipeName
                          re: \\f53f[0-9a-f]{2}$
                        - op: and
                          rules:
                          - case sensitive: false
                            op: starts with
                            path: event/EVENT/EventData/PipeName
                            value: \Winsock2\CatalogChangeListener-
                          - case sensitive: false
                            op: ends with
                            path: event/EVENT/EventData/PipeName
                            value: -0,
                respond:
                    - action: report
                      name: CobaltStrike Named Pipe Patterns
                      metadata:
                        tags:
                        - attack.defense_evasion
                        - attack.privilege_escalation
                        - attack.t1055
                        description: Detects the creation of a named pipe with a pattern found in CobaltStrike malleable C2 profiles
                        status: stable
                        id: 29206f7e-21fd-448a-9723-5f3272f22eba
                        references:
                        - https://svch0st.medium.com/guide-to-named-pipes-and-hunting-for-cobalt-strike-pipes-dc46b2c5f575
                        - https://gist.github.com/MHaggis/6c600e524045a6d49c35291a21e10752
                        level: medium
                        author: Florian Roth, Christian Burkard
                        falsepositives:
                        - Chrome instances using the exact same pipe name "mojo.something"
                        logsource: LimaCharlie
                tests:
                    match:
                      # Test 1: CobaltStrike mojo pipe pattern
                      - - event:
                            EVENT:
                              EventData:
                                EventType: CreatePipe
                                Image: C:\Windows\system32\rundll32.exe
                                PipeName: \mojo.5688.8052.183894939787088877
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "1234"
                                RuleName: "-"
                                User: NT AUTHORITY\SYSTEM
                                UtcTime: "2025-06-17 18:00:00.000"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: testhost.domain.com
                                EventID: "17"
                                _event_id: "17"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 2: CobaltStrike demoagent pipe
                      - - event:
                            EVENT:
                              EventData:
                                EventType: ConnectPipe
                                Image: C:\Windows\explorer.exe
                                PipeName: \demoagent_11
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "5678"
                                RuleName: "-"
                                User: DOMAIN\user
                                UtcTime: "2025-06-17 18:01:00.000"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: testhost.domain.com
                                EventID: "18"
                                _event_id: "18"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 3: Regex pattern f4c3
                      - - event:
                            EVENT:
                              EventData:
                                EventType: CreatePipe
                                Image: C:\temp\malicious.exe
                                PipeName: \f4c3ab
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "9999"
                                RuleName: "-"
                                User: DOMAIN\user
                                UtcTime: "2025-06-17 18:02:00.000"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: testhost.domain.com
                                EventID: "17"
                                _event_id: "17"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 4: Winsock2 CatalogChangeListener pattern
                      - - event:
                            EVENT:
                              EventData:
                                EventType: ConnectPipe
                                Image: C:\Windows\system32\svchost.exe
                                PipeName: \Winsock2\CatalogChangeListener-123-0,
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "1111"
                                RuleName: "-"
                                User: NT AUTHORITY\SYSTEM
                                UtcTime: "2025-06-17 18:03:00.000"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: testhost.domain.com
                                EventID: "18"
                                _event_id: "18"
                          routing:
                            event_type: WEL
                            hostname: testhost
                    non_match:
                      # Test 1: SearchIndexer.exe using legitimate pipe NOT in detection patterns
                      - - event:
                            EVENT:
                              EventData:
                                EventType: ConnectPipe
                                Image: C:\WINDOWS\system32\SearchIndexer.exe
                                PipeName: \SearchFilterHost
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "11816"
                                RuleName: "-"
                                User: NT AUTHORITY\SYSTEM
                                UtcTime: "2025-06-16 20:42:20.099"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: workstation01.example.com
                                EventID: "18"
                                _event_id: "18"
                          routing:
                            event_type: WEL
                            hostname: workstation01
                      # Test 2: Different event channel (not Sysmon)
                      - - event:
                            EVENT:
                              EventData:
                                PipeName: \mojo.5688.8052.183894939787088877
                              System:
                                Channel: Security
                                EventID: "18"
                                _event_id: "18"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 3: Wrong event ID (not 17 or 18)
                      - - event:
                            EVENT:
                              EventData:
                                PipeName: \demoagent_11
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                EventID: "1"
                                _event_id: "1"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 4: Legitimate Windows pipe not in detection patterns
                      - - event:
                            EVENT:
                              EventData:
                                EventType: ConnectPipe
                                Image: C:\Windows\system32\lsass.exe
                                PipeName: \lsass
                                ProcessGuid: "{a6385ccd-7fc6-6850-1702-000000001700}"
                                ProcessId: "700"
                                RuleName: "-"
                                User: NT AUTHORITY\SYSTEM
                                UtcTime: "2025-06-17 18:05:00.000"
                              System:
                                Channel: Microsoft-Windows-Sysmon/Operational
                                Computer: testhost.domain.com
                                EventID: "18"
                                _event_id: "18"
                          routing:
                            event_type: WEL
                            hostname: testhost
                      # Test 5: Non-WEL event type
                      - - event:
                            PROCESS_ID: 1234
                            FILE_PATH: \Device\NamedPipe\mojo.test
                          routing:
                            event_type: NEW_NAMED_PIPE
                            hostname: testhost
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: "Detects the creation of a named pipe with a pattern found in CobaltStrike malleable C2 profiles"
```

---

# Usage Alerts

The usage alerts Extension allows you to create, maintain, & automatically refresh usage alert conditions for an Organization.

For example, you can create a usage alert rule that will fire a detection when artifact downloads have reached a 1GB threshold in the last 30 days (43200 minutes). This alert will be saved as a managed rule. When the threshold is reached, a detection will be created with the following `cat`:

`Usage alert - Output data over threshold - 1024 MB in 30.00 days`

These alert rules can be managed across tenants using the Infrastructure as Code extension.

Every hour, LimaCharlie will sync all of the usage alert rules in the configuration. They can also be manually synced by clicking the `Sync Usage Alert Rules` button on the extension page. When a usage alert rule is added, it will **not** be automatically synced immediately, unless you click on `Sync Usage Alert Rules`.

**NOTE**: The maximum timeframe is currently 43200 minutes (30 days).

## Usage - GUI

To define a new usage alert, simply click on the `Add New Usage Alert` button in the extension UI. Give it a name, like `Output data over threshold`, select a SKU (in this case, `output_data`), a timeframe, a limit, and click `Save`. 

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(275).png)

If you want it to be added immediately, click on the `Sync Usage Alert Rules` button. Otherwise, it will get pushed automatically at the next hour interval.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(278).png)

This will create a managed D&R rule on the backend in the `dr-managed` hive and will sync automatically every hour.

```
hives:
    dr-managed:
        Output data over threshold:
            data:
                detect:
                    event: billing_record
                    op: and
                    rules:
                        - op: is
                          path: event/record/cat
                          value: output
                        - op: is
                          path: event/record/k
                          value: bytes_tx
                    target: billing
                respond:
                    - action: report
                      name: Usage alert - Output data over threshold - 1024 MB in 24.00 hours
                      suppression:
                        count_path: event/record/v
                        keys:
                            - output
                            - bytes_tx
                            - ext-usage-alerts
                            - Output data over threshold
                        max_count: 1.073741824e+09
                        min_count: 1.073741824e+09
                        period: 43200m
```

## Usage - Infrastructure as Code

If you are managing your organizations via infrastructure as code, you can also configure these rules in the `extension_config` hive.

```
hives:
    extension_config:
        ext-usage-alerts:
            data:
                usage_alert_rules:
                    - enabled: true
                      limit: 1024
                      name: Output data over threshold
                      sku: output_data
                      timeframe: 43200
            usr_mtd:
                enabled: true
                expiry: 0
                tags: []
                comment: ""
```

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# VDI & Virtual Machine Templates

The LimaCharlie Endpoint Agent can be installed in template-based environments whether they're VMs or VDIs.

The methodology is the same as described above, but you need to be careful to stage the Endpoint Agent install properly in your templates.

The most common mistake is to install the Sensor directly in the template, and then instantiate the rest of the infrastructure from this template. This will result in "cloned sensors", sensors running using the same Sensor ID on different hosts/VMs/Containers.

If these occur, a [sensor_clone](/v2/docs/reference-platform-events#sensorclone) event will be generated as well as an error in your dashboard. If this happens you have two choices:

1. Fix the installation process and re-deploy.
2. Run a de-duplication process with a Detection & Response rule [like this](/v2/docs/detection-and-response-examples#deduplicate-cloned-sensors).

Preparing sensors to run properly from templates can be done by creating a special `hcp_vdi` (macOS and Linux) or `hcp_vdi.dat` (Windows) file in the relevant configuration directory:

* Windows: `%SYSTEMROOT%\system32\`
* macOS: `/usr/local/`
* Linux: usually `/etc/` but fundamentally the current working directory of the sensor execution.

The contents of the `hcp_vdi` file should be a string representation of the second-based epoch timestamp when you want the sensors to begin enrolling. For example if the current time is `1696876542`, setting a value of `1696882542` will mean the sensor will only attempt to enroll in 10 minutes in the future. This allows you to install the sensor without risking it enrolling right away before the base image is created.

A shortcut for creating this file is to invoke the LimaCharlie EDR binary (like `lc_sensor.exe`) with the `-t` option, which will create a `hcp_vdi.dat` file with a value +1 day. This is usually plenty of time to finish the creation of the base image, submit it to a VDI platform (which often boots up the image) etc. The next day, any machine generated from this base image will start enrolling.

Example `hcp_vdi.dat` file content:

```
1696882542
```

Note that if a sensor is already enrolled, the presence of the `hcp_vdi` file will be completely ignored.

## Key Terms

**Endpoint Agents** are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, **Sensors** send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, a **Sensor ID** is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately linked to specific devices or endpoints.

---

# Velociraptor

## Overview

[Velociraptor](https://github.com/Velocidex/Velociraptor) is an open source endpoint visibility tool that includes power digital forensic, incident response, and incident triage capabilities. LimaCharlie can be used to deploy Velociraptor at scale, allowing for easy artifact collection and incident analysis.

The interface defines 2 main actions:

1. **Show Artifact** - allows you to inspect the VQL artifacts available for collection
2. **Collect Artifact** - allows you to run an artifact collection on one or more endpoints

### Show Artifact

Simply choose an artifact from the list to inspect it's contents.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor-ext-1.png)

Result of the action

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor-ext-2.png)

### Collect Artifact

This allows you to collect one or more Velociraptor [Artifacts](https://docs.velociraptor.app/artifact_references/) from one or more endpoints via the Endpoint Agent.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor-ext-3.png)

Velociraptor will generate a ZIP file with all collected data, which is automatically ingested into LimaCharlie's Artifact system for download.

#### Arguments

* **Artifacts** - Select one or more Velociraptor artifacts you wish to collect
* **Sensor Selector** - Select either a single sensor by selecting it's Sensor ID from the dropdown or use a [Sensor Selector Expression](/v2/docs/reference-sensor-selector-expressions) to cast a wider net such as `plat==windows`
* **Arguments (optional)** - These are optional arguments (or parameters) passed directly to the Velociraptor Artifact. For instance, if you wanted to run a collection for [Windows.KapeFiles.Targets](https://github.com/Velocidex/velociraptor/blob/master/artifacts/definitions/Windows/KapeFiles/Targets.yaml) and wanted to specify the [KapeTriage](https://github.com/Velocidex/velociraptor/blob/5db9bc46cc79013da1bbaf8c493a263eb1ca64b4/artifacts/definitions/Windows/KapeFiles/Targets.yaml#L412-L414) targets for collection, you would specify `KapeTriage=Y` in the **Arguments** since this is a boolean parameter for the `Windows.KapeFiles.Targets` artifact.
* **Collection Seconds (optional)** - Define how long (in seconds) the Extension will wait for a targeted endpoint to come online and be processed for collection.
* **Retention Days (optional)** - Define how long the collected artifact will be retained by the platform.
* **Ignore SSL Errors (optional)** - Tells the endpoint to ignore SSL errors while running and collecting. This can be useful if the endpoint is behind a MITM proxy or firewall performing SSL interception.

## Monitoring Collections

You are able to track Velociraptor hunts by viewing the Timeline for the `ext-velociraptor` sensor.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor-ext-4.png)

Once you see `artifact_uploaded` in the timeline, you can expect to find the artifact on the "Artifacts" screen.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor-ext-5.png)

## Automating Collection Retrieval

Let's say you wanted to automatically fetch new Velociraptor collections and send somewhere else for storage/processing. This can be accomplished via rules which watch for the artifact upload and send to a tailored output.

Example D&R rule

```
# Detection
op: is
path: routing/log_type
target: artifact_event
value: velociraptor

# Response
- action: output
  name: artifacts-tailored
  suppression:
    is_global: false
    keys:
        - '{{ .event.original_path }}'
        - '{{ .routing.log_id }}'
    max_count: 1
    period: 1m
- action: report
  name: VR artifact ingested
```

To see how you could use something like this to automate post-processing of Velociraptor triage collections, check out this [open source example](https://github.com/shortstack/lcvr-to-timesketch) which sends KAPE Triage acquisitions to a webhook which then retrieves the collection for processing via [Plaso](https://github.com/log2timeline/plaso/) and into [Timesketch](https://github.com/google/timesketch).

To see how you can send Velociraptor data to BigQuery for further analysis, see this [tutorial](/v2/docs/velociraptor-to-bigquery).

## Using Velociraptor in D&R Rules

If you want to trigger a Velociraptor collection as a response to one of your detections, you can configure an extension request in the respond block of a rule.

This example will kick off the KAPE files Velociraptor artifact to collect event logs from the system involved in the detection.

```
- action: extension request
  extension action: collect
  extension name: ext-velociraptor
  extension request:
    artifact_list: ['Windows.KapeFiles.Targets']
    sid: '{{ .routing.sid }}' # Use a sensor selector OR a sid, **not both**
    sensor_selector: '' # Use a sensor selector OR a sid, **not both**
    args: '{{ "EventLogs=Y" }}'
    collection_ttl: 3600 # 1 hour - collection_ttl is specified in seconds
    retention_ttl: 7 # retention_ttl is specified in days
    ignore_cert: false
```

### Migrating D&R Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy Velociraptor service, preview the change and execute the conversion required in the rule "response".

Command line to preview Velociraptor rule conversion:

```
limacharlie extension convert_rules --name ext-velociraptor
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute Velociraptor rule conversion:

```
limacharlie extension convert_rules --name ext-velociraptor --no-dry-run
```

---

# WEL Monitoring

LimaCharlie's SecOps Cloud Platform transforms Windows Event Log monitoring by providing real-time visibility, streamlined infrastructure, and powerful detection and response capabilities. Effectively monitor and protect your Windows environments, ensuring rapid detection and response to potential security incidents.

## WEL monitoring problems

* **Limited real-time visibility:** Traditional WEL monitoring solutions often rely on periodic log collection, resulting in delayed visibility into potential security incidents, limiting real-time visibility.
* **Complex and costly infrastructure:** Forwarding WEL data to a centralized monitoring system typically requires additional infrastructure, such as log collectors and forwarders, which can be complex to set up and maintain, as well as costly to scale.
* **Difficulty in creating custom detection rules:** Writing custom rules to detect malicious behavior in WEL data can be challenging, especially when dealing with large volumes of logs and a lack of standardized formats.

## LimaCharlie's solution

* **Real-time WEL ingestion:** LimaCharlie's Sensor enables direct, real-time importation of WEL data, eliminating the need for complex forwarding infrastructure and reducing costs and management overhead.
* **Powerful Detection & Response (D&R) engine**: Ingested WEL data is automatically indexed against common indicators of compromise (IoCs) and processed through LimaCharlie's advanced Detection and Response engine, enabling rapid detection of malicious activity.
* **Flexible and customizable rule creation:** With WEL data structured as JSON, security teams can easily create custom D&R rules to detect and respond to specific Windows events as they occur, tailoring the monitoring process to their unique needs and environment.
* **Historical log analysis:** Import historical event log data from disk, empowering teams to conduct in-depth investigations and gain valuable context around endpoint activity.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# WEL Monitoring

LimaCharlie's SecOps Cloud Platform transforms Windows Event Log monitoring by providing real-time visibility, streamlined infrastructure, and powerful detection and response capabilities. Effectively monitor and protect your Windows environments, ensuring rapid detection and response to potential security incidents.

## WEL monitoring problems

* **Limited real-time visibility:** Traditional WEL monitoring solutions often rely on periodic log collection, resulting in delayed visibility into potential security incidents, limiting real-time visibility.
* **Complex and costly infrastructure:** Forwarding WEL data to a centralized monitoring system typically requires additional infrastructure, such as log collectors and forwarders, which can be complex to set up and maintain, as well as costly to scale.
* **Difficulty in creating custom detection rules:** Writing custom rules to detect malicious behavior in WEL data can be challenging, especially when dealing with large volumes of logs and a lack of standardized formats.

## LimaCharlie's solution

* **Real-time WEL ingestion:** LimaCharlie's Sensor enables direct, real-time importation of WEL data, eliminating the need for complex forwarding infrastructure and reducing costs and management overhead.
* **Powerful Detection & Response engine:** Ingested WEL data is automatically indexed against common indicators of compromise (IoCs) and processed through LimaCharlie's advanced Detection and Response engine, enabling rapid detection of malicious activity.
* **Flexible and customizable rule creation:** With WEL data structured as JSON, security teams can easily create custom D&R rules to detect and respond to specific Windows events as they occur, tailoring the monitoring process to their unique needs and environment.
* **Historical log analysis:** Import historical event log data from disk, empowering teams to conduct in-depth investigations and gain valuable context around endpoint activity.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# YARA

The [YARA](https://github.com/Yara-Rules/rules) Extension is designed to help you with all aspects of YARA scanning. It takes what is normally a manual piecewise process, provides a framework and automates it. Once configured, YARA scans can be run on demand for a particular endpoint or continuously in the background across your entire fleet.

Yara configurations are synchronized with sensors every few minutes.

There are three main sections to the YARA job:

* Sources
* Rules
* Scan

## Where Does My YARA Scan?

Automated YARA scanners in LimaCharlie will run on all files loaded in memory (e.g. exe, dll, etc), and on the memory itself.

Files on disk can be scanned using a Sensor command. You can trigger a Manual Scan that's run on-demand by:

* Clicking the Run YARA scan button on the sensor details page,
* Clicking the Scan button on the YARA Scanners page
* Using the console
* Within the Response section of a rule (sample below)
* Using the LimaCharlie API

## Rules

This is where you define your YARA rule(s). You can copy and paste your YARA rules into the `Rule` box, or you can define sources via the [ext-yara-manager](/v2/docs/ext-yara-manager). Sources can be either direct links (URLs) to a given YARA rule (or directory of rules) or [ARLs](/v2/docs/reference-authentication-resource-locator) to a YARA rule.

## Scanners

Scanners define which sets of sensors should be scanned with which sets of YARA rules.

Filter Tags are tags that must ALL be present on a sensor for it to match (AND condition), while the platform of the sensor much match one of the platforms in the filter (OR condition).

To apply YARA rules to scan an endpoint (or set of endpoints), you must select the platform or tags, and then add the YARA rules you would like to run.

## Using Yara in D&R Rules

If you want to trigger a Yara scan as a response to one of your detections, you can configure an extension request in the respond block of a rule. A Yara scan request can be executed with a blank selector OR Sensor ID. However, one of them must be specified.

```
- action: extension request
  extension action: scan
  extension name: ext-yara
  extension request:
		sources: [ ]# Specify Yara Rule sources as strings
		selector: ''
        sid: '{{ .routing.sid }}' # Use a sensor selector OR a sid, **not both**
		yara_scan_ttl: 86400 # "Default: 1 day (86,400 seconds)"
```

### Migrating D&R Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy Yara service, preview the change and execute the conversion required in the rule "response".

Command line to preview Yara rule conversion:

```
limacharlie extension convert_rules --name ext-yara
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute Yara rule conversion:

```
limacharlie extension convert_rules --name ext-yara --no-dry-run
```

---

# YARA

The [YARA](https://github.com/Yara-Rules/rules) Extension is designed to help you with all aspects of YARA scanning. It takes what is normally a manual piecewise process, provides a framework and automates it. Once configured, YARA scans can be run on demand for a particular endpoint or continuously in the background across your entire fleet.

Yara configurations are synchronized with sensors every few minutes.

There are three main sections to the YARA job:

* Sources
* Rules
* Scan

## Where Does My YARA Scan?

Automated YARA scanners in LimaCharlie will run on all files loaded in memory (e.g. exe, dll, etc), and on the memory itself.

Files on disk can be scanned using a Sensor command. You can trigger a Manual Scan that's run on-demand by:

* Clicking the Run YARA scan button on the sensor details page,
* Clicking the Scan button on the YARA Scanners page
* Using the console
* Within the Response section of a rule (sample below)
* Using the LimaCharlie API

## Rules

This is where you define your YARA rule(s). You can copy and paste your YARA rules into the `Rule` box, or you can define sources via the [ext-yara-manager](/v2/docs/ext-yara-manager). Sources can be either direct links (URLs) to a given YARA rule (or directory of rules) or [ARLs](/v2/docs/reference-authentication-resource-locator) to a YARA rule.

## Scanners

Scanners define which sets of sensors should be scanned with which sets of YARA rules.

Filter Tags are tags that must ALL be present on a sensor for it to match (AND condition), while the platform of the sensor much match one of the platforms in the filter (OR condition).

To apply YARA rules to scan an endpoint (or set of endpoints), you must select the platform or tags, and then add the YARA rules you would like to run.

## Using Yara in D&R Rules

If you want to trigger a Yara scan as a response to one of your detections, you can configure an extension request in the respond block of a rule. A Yara scan request can be executed with a blank selector OR Sensor ID. However, one of them must be specified.

```
- action: extension request
  extension action: scan
  extension name: ext-yara
  extension request:
		sources: [ ]# Specify Yara Rule sources as strings
		selector: ''
        sid: '{{ .routing.sid }}' # Use a sensor selector OR a sid, **not both**
		yara_scan_ttl: 86400 # "Default: 1 day (86,400 seconds)"
```

### Migrating D&R Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy Yara service, preview the change and execute the conversion required in the rule "response".

Command line to preview Yara rule conversion:

```
limacharlie extension convert_rules --name ext-yara
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute Yara rule conversion:

```
limacharlie extension convert_rules --name ext-yara --no-dry-run
```

---

# YARA Manager

The [YARA](https://github.com/Yara-Rules/rules) manager Extension allows you to reference external YARA rules (rules maintained in GitHub, for example) to use in your YARA scans within LimaCharlie.

YARA rule sources defined in the YARA manager configuration will be synced every 24 hours, and can be manually synced by clicking the `Manual Sync` button on the extension page.

If you add rule sources and want them to become available immediately, you will need to click the `Manual Sync` button to trigger the initial sync of the rules.

Rule sources can be either direct links (URLs) to a given YARA rule or [ARLs](/v2/docs/reference-authentication-resource-locator).

## Option 1: Predefined YARA rules

LimaCharlie provides a list of YARA rule repositories, available in the configuration menu. To leverage these rules select "Predefined" and a list of LimaCharlie and Community rules will populate. By selecting one or more of these repositories, the respective rules will be automatically imported and will appear in your YARA rules under Automation  YARA Rules.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(322).png)

## Option 2: Publicly available YARA rules

An example of setting up a rule using this repo: [Yara-Rules](https://github.com/Yara-Rules/rules)

For an `Email and General Phishing Exploit` rule we could use the following URL, which is a link to a single YARA rule.

<https://raw.githubusercontent.com/Yara-Rules/rules/master/email/Email_generic_phishing.yar>

For creating a rule out of multiple YARA rules, we could use the following ARL, which is a link to a directory of YARA rules.

`[github,Yara-Rules/rules/email]`

Giving the rule configuration a name, the URL or ARL, and clicking the Save button will create the new rule source to sync to your YARA rules.

## Option 3: Private YARA Repository

To use a YARA rule from a private Gihub repository you will need to make use of an [Authentication Resource Locator](/v2/docs/reference-authentication-resource-locator).

### Step 1: Create a token in GitHub

In GitHub go to *Settings* and click *Developer settings* in the left hand side bar.

Next click *Personal access token* followed by *Generate new token*. Select repo permissions and finally *Generate token*.

### Step 2: Connect LimaCharlie to your GitHub repository

Inside of LimaCharlie, click on *Yara Manager* in the left hand menu. Then click *Add New Yara Configuration*.

Give your rule a name and then use the token you generated with the following format linked to your repo.

`[github,my-org/my-repo-name/path/to/rule.yar,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

or

`[github,my-org/my-repo-name/path/to/rules_directory,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

---

# YARA Manager

The [YARA](https://github.com/Yara-Rules/rules) manager Extension allows you to reference external YARA rules (rules maintained in GitHub, for example) to use in your YARA scans within LimaCharlie.

YARA rule sources defined in the YARA manager configuration will be synced every 24 hours, and can be manually synced by clicking the `Manual Sync` button on the extension page.

If you add rule sources and want them to become available immediately, you will need to click the `Manual Sync` button to trigger the initial sync of the rules.

Rule sources can be either direct links (URLs) to a given YARA rule or [ARLs](/v2/docs/reference-authentication-resource-locator).

## Option 1: Predefined YARA rules

LimaCharlie provides a list of YARA rule repositories, available in the configuration menu. To leverage these rules select "Predefined" and a list of LimaCharlie and Community rules will populate. By selecting one or more of these repositories, the respective rules will be automatically imported and will appear in your YARA rules under Automation  YARA Rules.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(322).png)

## Option 2: Publicly available YARA rules

An example of setting up a rule using this repo: [Yara-Rules](https://github.com/Yara-Rules/rules)

For an `Email and General Phishing Exploit` rule we could use the following URL, which is a link to a single YARA rule.

<https://raw.githubusercontent.com/Yara-Rules/rules/master/email/Email_generic_phishing.yar>

For creating a rule out of multiple YARA rules, we could use the following ARL, which is a link to a directory of YARA rules.

`[github,Yara-Rules/rules/email]`

Giving the rule configuration a name, the URL or ARL, and clicking the Save button will create the new rule source to sync to your YARA rules.

## Option 3: Private YARA Repository

To use a YARA rule from a private Github repository you will need to make use of an [Authentication Resource Locator](/v2/docs/reference-authentication-resource-locator).

### Step 1: Create a token in GitHub

In GitHub go to *Settings* and click *Developer settings* in the left hand side bar.

Next click *Personal access token* followed by *Generate new token*. Select repo permissions and finally *Generate token*.

### Step 2: Connect LimaCharlie to your GitHub repository

Inside of LimaCharlie, click on *Yara Manager* in the left hand menu. Then click *Add New Yara Configuration*.

Give your rule a name and then use the token you generated with the following format linked to your repo.

`[github,my-org/my-repo-name/path/to/rule.yar,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

or

`[github,my-org/my-repo-name/path/to/rules_directory,token,bfuihferhf8erh7ubhfey7g3y4bfurbfhrb]`

LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

---

# Zeek

[Zeek](https://zeek.org/) is a comprehensive platform for network traffic analysis and intrusion detection.

Once enabled, this extension allows you to generate Zeek logs from packet capture (PCAP) files collected via Artifacts. The resulting Zeek log files are subsequently parsed and pushed into the `ext-zeek` Sensor timeline as JSON. You can create detection & response rules to automate based on Zeek log data.

LimaCharlie will automatically kick off Zeek based on the artifact ID provided in a rule action.

## Zeek Extension Pricing

While it is Free to enable the Zeek extension, pricing is applied to processed PCAPs at a rate of $0.02/GB.

## Configuration

To enable the Zeek extension, navigate to the [Zeek extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-zeek) in the marketplace. Select the Organization you wish to enable the extension for, and select Subscribe.

When enabled, you may configure the response of a D&R rule to run Zeek against an artifact event. Here is an example D&R rule:

**Detect:**

```
artifact type: pcap
event: ingest
op: exists
path: /
target: artifact_event
```

**Respond:**

```
- action: extension request
  extension action: run_on
  extension name: ext-zeek
  extension request:
    artifact_id: '{{ .routing.log_id }}'
    retention: 30
```

## Results

```
/opt/zeek/bin/zeek -C LogAscii::use_json=T --no-checksums --readfile /path/to/your.pcap
```

Upon running Zeek, several JSON log files are generated. The log files are parsed and pushed into the `ext-zeek` sensor timeline.

## Usage

### Via Automatic PCAP Collection

**Note: This is only available on Linux sensors**

Enable PCAP collection on your Linux sensors via a PCAP capture rule within the artifact collection extension.

For example, if you have an interface `ens4` and would like to gather PCAPs of network traffic on that interface on TCP port 80, you would craft the following rule.

Once ~30MB of traffic has been collected, a PCAP will be uploaded as an artifact in LimaCharlie. Subsequent PCAPs will continue to be uploaded as additional PCAPs as they hit the size threshold.

All PCAPs uploaded will trigger the [D&R rule below](#dr-rule).

### Via Manual PCAP Upload

If you have already generated a PCAP on a system or systems, you can manually ingest those as artifacts by running the following in your sensor console:

```
artifact_get --file /path/to/your.pcap --type pcap
```

This will trigger the [D&R rule below](#dr-rule).

### D&R Rule

**Detect:**

```
artifact type: pcap
event: ingest
op: exists
path: /
target: artifact_event
```

**Respond:**

```
- action: extension request
  extension action: run_on
  extension name: ext-zeek
  extension request:
    artifact_id: '{{ .routing.log_id }}'
    retention: 30
```

### Migrating D&R Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy zeek service, preview the change and execute the conversion required in the rule "response".

Command line to preview zeek rule conversion:

```
limacharlie extension convert_rules --name ext-zeek
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute zeek rule conversion:

```
limacharlie extension convert_rules --name ext-zeek --no-dry-run
```

## Definitions

**Extensions**: LimaCharlie Extensions allow users to expand and customize their security environments by integrating third-party tools, automating workflows, and adding new capabilities. Organizations subscribe to Extensions, which are granted specific permissions to interact with their infrastructure. Extensions can be private or public, enabling tailored use or broader community sharing. This framework supports scalability, flexibility, and secure, repeatable deployments.

**Sensors**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Zeek

[Zeek](https://zeek.org/) is a comprehensive platform for network traffic analysis and intrusion detection.

Once enabled, this extension allows you to generate Zeek logs from packet capture (PCAP) files collected via Artifacts. The resulting Zeek log files are subsequently parsed and pushed into the `ext-zeek` Sensor timeline as JSON. You can create detection & response rules to automate based on Zeek log data.

LimaCharlie will automatically kick off Zeek based on the artifact ID provided in a rule action.

> **Zeek Extension Pricing**
>
> While it is Free to enable the Zeek extension, pricing is applied to processed PCAPs at a rate of $0.02/GB.

## Configuration

To enable the Zeek extension, navigate to the [Zeek extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-zeek) in the marketplace. Select the Organization you wish to enable the extension for, and select Subscribe.

When enabled, you may configure the response of a D&R rule to run Zeek against an artifact event. Here is an example D&R rule:

**Detect:**

```
artifact type: pcap
event: ingest
op: exists
path: /
target: artifact_event
```

**Respond:**

```
- action: extension request
  extension action: run_on
  extension name: ext-zeek
  extension request:
    artifact_id: '{{ .routing.log_id }}'
    retention: 30
```

## Results

```
/opt/zeek/bin/zeek -C LogAscii::use_json=T --no-checksums --readfile /path/to/your.pcap
```

Upon running Zeek, several JSON log files are generated. The log files are parsed and pushed into the `ext-zeek` sensor timeline.

![Screenshot 2024-02-20 1.04.52 PM.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Screenshot%202024-02-20%201.04.52%20PM.png)

## Usage

### Via Automatic PCAP Collection

**Note: This is only available on Linux sensors**

Enable PCAP collection on your Linux sensors via a PCAP capture rule within the artifact collection extension.

For example, if you have an interface `ens4` and would like to gather PCAPs of network traffic on that interface on TCP port 80, you would craft the following rule.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/zeek-2.png)

Once ~30MB of traffic has been collected, a PCAP will be uploaded as an artifact in LimaCharlie. Subsequent PCAPs will continue to be uploaded as additional PCAPs as they hit the size threshold.

All PCAPs uploaded will trigger the [D&R rule below](#dr-rule).

### Via Manual PCAP Upload

If you have already generated a PCAP on a system or systems, you can manually ingest those as artifacts by running the following in your sensor console:

```
artifact_get --file /path/to/your.pcap --type pcap
```

This will trigger the [D&R rule below](#dr-rule).

### D&R Rule

**Detect:**

```
artifact type: pcap
event: ingest
op: exists
path: /
target: artifact_event
```

**Respond:**

```
- action: extension request
  extension action: run_on
  extension name: ext-zeek
  extension request:
    artifact_id: '{{ .routing.log_id }}'
    retention: 30
```

### Migrating D&R Rule from legacy Service to new Extension

***LimaCharlie is migrating away from Services to a new capability called Extensions. Support of legacy services will end on June 30, 2024.***

The [Python CLI](https://github.com/refractionPOINT/python-limacharlie) gives you a direct way to assess if any rules reference legacy zeek service, preview the change and execute the conversion required in the rule "response".

Command line to preview zeek rule conversion:

```
limacharlie extension convert_rules --name ext-zeek
```

A dry-run response (default) will display the rule name being changed, a JSON of the service request rule and a JSON of the incoming extension request change.

To execute the change in the rule, explicitly set `--dry-run` flag to `--no-dry-run`

Command line to execute zeek rule conversion:

```
limacharlie extension convert_rules --name ext-zeek --no-dry-run
```

---

# Outputs & Integrations

# Adding Outputs to an Allow List

At LimaCharlie, we rely on infrastructure with auto-scalers, and thus do not have static IPs nor a CIDR that you can rely on for an allow list (or "whitelisting").

Typically, the concern around adding IPs to an allow list for Outputs is based on wanting to limit abuse and ensure that data from webhooks is truly coming from LimaCharlie and not other sources. To address this, we provide a `secret_key` parameter that can be used as a *shared secret* between LimaCharlie and your webhook receiver. When we issue a webhook, we include a `lc-signature` header that is an HMAC of the content of the webhook using the shared `secret_key`.

---

# Adding Outputs to an Allow List

At LimaCharlie, we rely on infrastructure with auto-scalers, and thus do not have static IPs nor a CIDR that you can rely on for an allow list (or "whitelisting").

Typically, the concern around adding IPs to an allow list for Outputs is based on wanting to limit abuse and ensure that data from webhooks is truly coming from LimaCharlie and not other sources. To address this, we provide a `secret_key` parameter that can be used as a *shared secret* between LimaCharlie and your webhook receiver. When we issue a webhook, we include a `lc-signature` header that is an HMAC of the content of the webhook using the shared `secret_key`.

---

# Azure Storage Blob

Output events and detections to a Blob Container in Azure Storage Blobs.

## Configuration Parameters

* `secret_key`: the secret access key for the Blob Container.
* `blob_container`: the name of the Blob Container to upload to.
* `account_name`: the account name used to authenticate in Azure.

## Example

```
blob_container: testlcdatabucket
account_name: lctestdata
secret_key: dkndsgnlngfdlgfd
```

---

# Cost Effective SIEM Alternative

LimaCharlie's SecOps Cloud Platform provides a cost-effective and flexible alternative or supplement to traditional Security Information and Event Management (SIEM) offering essential capabilities while addressing the challenges of high costs, vendor lock-in, and complexity. By leveraging LimaCharlie's interoperability, automation, and detection and response capabilities, security teams can optimize their operations and maintain a robust posture without the high costs and limitations of legacy SIEM solutions.

## SIEM problems

The capabilities of SIEM solutions are essential for managing logs, correlating events, monitoring and alerting, and storing telemetry data. However, traditional SIEMs often present several challenges for organizations:

* **High costs:** SIEMs are typically very expensive to implement and maintain, with costs escalating as data volumes grow and additional features are required.
* **Vendor lock-in:** Many SIEMs are proprietary, closed systems that make it difficult for organizations to switch providers or integrate with other security tools.
* **Complexity:** SIEMs can be complex to set up and manage, requiring specialized skills and resources that may strain already overburdened security teams.

## LimaCharlie's solution

LimaCharlie's SecOps Cloud Platform offers a cost-effective alternative to traditional SIEMs, providing essential capabilities while addressing the challenges of high costs, vendor lock-in, and complexity:

* **Cost savings through flexible data management:** LimaCharlie provides one year of free telemetry storage in a fully searchable format, reducing the need to store all data in expensive SIEMs. The platform's ability to classify, filter, and route telemetry data intelligently allows organizations to send only critical data to their SIEM, further reducing costs.
* **Interoperability and customization:** Built with interoperability in mind, LimaCharlie seamlessly integrates with a wide range of security tools and platforms, enabling organizations to create custom workflows and avoid vendor lock-in. The platform's open architecture and extensive API support make it easy to integrate with existing security infrastructure.
* **Automation and ease of use:** LimaCharlie's Detection, Automation, and Response Engine enables security teams to create sophisticated detection rulesets and automate response actions, reducing alert fatigue and simplifying security operations. The SecOps Cloud Platform's powerful query language (LCQL) makes it easy for security professionals to access and analyze telemetry data without the complexity of traditional SIEMs.
* **Advanced capabilities:** LimaCharlie offers advanced threat hunting and integration with third-party threat intelligence platforms, providing security teams with the context and insights they need to identify and respond to threats effectively.

---

# Google Cloud Storage

Output events and detections to a GCS bucket.

## Looking for Google Chronicle?

If you already use Google Chronicle, we make it easy to send telemetry you've collected in LimaCharlie to Chronicle. You can get that set up by creating an Output in LimaCharlie to a GCS bucket.

## Configuration Parameters

* `bucket`: the path to the GCS bucket.
* `secret_key`: the secret json key identifying a service account.
* `sec_per_file`: the number of seconds after which a file is cut and uploaded.
* `is_compression`: if set to "true", data will be gzipped before upload.
* `is_indexing`: if set to "true", data is uploaded in a way that makes it searchable.
* `dir`: the directory prefix where to output the files on the remote host.

## Example

```
bucket: my-bucket-name
secret_key: {
  "type": "service_account",
  "project_id": "my-lc-data",
  "private_key_id": "11b6f4173dedabcdefb779e4afae6d88ddce3cc1",
  "private_key": "-----BEGIN PRIVATE KEY-----\n.....\n-----END PRIVATE KEY-----\n",
  "client_email": "my-service-writer@my-lc-data.iam.gserviceaccount.com",
  "client_id": "102526666608388828174",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/my-service-writer%40my-lc-data.iam.gserviceaccount.com"
}
is_indexing: "true"
is_compression: "true"
```

---

# Cost Effective SIEM Alternative

Traditional Security Information and Event Management (SIEM) solutions have long been the cornerstone of enterprise security operations. However, their high costs, complexity, and rigid architectures often create barriers for organizations seeking effective security monitoring. LimaCharlie offers a modern, cost-effective alternative that addresses these challenges while providing superior flexibility and scalability.

## The SIEM Cost Challenge

Traditional SIEMs typically charge based on:
- **Data ingestion volume**: Often $1-3+ per GB of logs ingested
- **Users/seats**: Per-analyst licensing fees
- **Storage**: Additional costs for log retention
- **Professional services**: Implementation and maintenance support

For many organizations, these costs can quickly escalate to hundreds of thousands or even millions of dollars annually, with a significant portion spent on data that provides minimal security value.

## LimaCharlie's Approach

LimaCharlie fundamentally reimagines the economics of security operations:

### 1. **Pay Only for What Matters**

Instead of charging for raw log volume, LimaCharlie uses a sensor-based pricing model. You pay for endpoints and infrastructure being monitored, not the volume of telemetry they generate. This creates predictable costs while enabling unlimited data collection from monitored assets.

### 2. **Efficient Data Architecture**

LimaCharlie's cloud-native architecture is designed for efficiency:
- **Smart retention**: Keep hot data accessible, archive what you need for compliance
- **On-demand processing**: Analyze data in real-time without expensive storage overhead
- **Selective enrichment**: Apply expensive operations only where needed

### 3. **Bring Your Own Analytics**

Rather than forcing you into a proprietary analytics engine, LimaCharlie lets you:
- Stream data to your existing data lake or SIEM if desired
- Use built-in detection and response capabilities
- Apply custom logic through Detection & Response (D&R) rules
- Integrate with open-source tools and frameworks

## Cost Comparison Example

**Scenario**: Mid-sized organization with 1,000 endpoints, moderate cloud infrastructure

### Traditional SIEM:
- Data ingestion: ~500 GB/day  30 days  $2/GB = $30,000/month
- Storage retention (90 days): $5,000/month
- User licenses (5 analysts): $2,500/month
- **Total**: ~$37,500/month ($450,000/year)

### LimaCharlie:
- Endpoint sensors (1,000): Based on usage tier
- Cloud infrastructure monitoring: Based on resources
- Unlimited data ingestion from monitored assets
- No per-user fees
- **Total**: Typically 60-80% cost reduction

## Beyond Cost: Additional Benefits

### **Speed to Value**
- Deploy in minutes, not months
- No hardware or infrastructure to maintain
- Immediate access to detection and response capabilities

### **Flexibility**
- API-first architecture for custom integrations
- Modular services (EDR, NDR, vulnerability management)
- No vendor lock-in for analytics or storage

### **Scalability**
- Elastic cloud infrastructure
- No performance degradation with data growth
- Global deployment options

### **Modern Detection Capabilities**
- Real-time detection and response
- YARA scanning across telemetry
- Custom D&R rules in simple syntax
- Pre-built integration with threat intelligence

## Implementation Strategy

Organizations can adopt LimaCharlie in several ways:

### **1. Complete SIEM Replacement**
Replace traditional SIEM entirely with LimaCharlie's detection and response platform.

### **2. SIEM Augmentation**
Use LimaCharlie for high-volume, low-value data sources while keeping existing SIEM for specific use cases.

### **3. Hybrid Approach**
- Collect all telemetry in LimaCharlie
- Perform initial detection and filtering
- Forward only relevant alerts/data to existing SIEM
- Dramatically reduce SIEM ingestion costs

### **4. Greenfield Deployment**
Start new security operations on LimaCharlie's modern platform without legacy technical debt.

## Getting Started

1. **Assess Current Costs**: Calculate your total SIEM spend including licensing, storage, and professional services
2. **Pilot Deployment**: Start with a subset of infrastructure to validate effectiveness
3. **Measure Results**: Compare detection capabilities, analyst efficiency, and total cost
4. **Scale Gradually**: Expand coverage as you validate the platform

## Conclusion

LimaCharlie provides a cost-effective alternative to traditional SIEMs without sacrificing security effectiveness. By rethinking the economic model and leveraging modern cloud architecture, organizations can significantly reduce costs while improving their security posture and operational flexibility.

The question isn't whether you can afford to switch to LimaCharlieit's whether you can afford not to.

---

# Output Billing

LimaCharlie aims to bill outputs at cost. This means that as a default outputs are billed accordingly to the [published pricing](https://limacharlie.io/pricing).

An exception to this is outputs that use Google Cloud Platform mechanism where the destination region is the same as the one the relevant LimaCharlie datacenter lives in. In those cases, outputs are not billed.

Here is a list of the relevant regions for the various LimaCharlie datacenter.

* USA: `us-central1`
* Canada: `northamerica-northeast1`
* Europe: `europe-west4`
* UK: `europe-west2`
* India: `asia-south1`
* Australia: `australia-southeast1`

The supported GCP mechanism for free output are:

* `gcs`
* `pubsub`
* `bigquery`

Google Cloud Platform general region list: <https://cloud.google.com/about/locations>

IP ranges of GCP resources per region change over time. Google publishes these ranges as a JSON file here: <https://www.gstatic.com/ipranges/cloud.json>

---

# Outputs

## Overview

LimaCharlie provides multiple output options, referred to as streams, for you to send data from LimaCharlie to other destination(s). We provide native support for output with multiple different providers, or "destinations". The diagram below provides some basic examples of where data is sourced from and where data can be sent to.

![Flow of Data within LimaCharlie](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image.png)

Outputs should be thought of in two capacities: Streams and Destinations. A *stream* is what you are sending, whereas a *destination* is where you are sending it to. We will look at both in detail.

## Streams

Streams define which events will be sent to an output destination.

Available streams include:

* `event` : The bulk of data events coming from sensors. *Note: this will be very verbose*
* `detect` : Alerts, as generated by the `report` action in [detection and response rules](/v2/docs/detection-and-response).
* `audit` : Events generated by the LimaCharlie platform, such as access control.
* `deployment` : Events about your deployment, like Sensor enrollments or cloned sensors.
* `artifact` : Meta-events reporting on newly-ingested files through the Artifact Collection mechanism.
* `tailored` : Only events specifically flagged for outputs sent to this stream.

## Destinations

LimaCharlie integrates with several providers, such as S3, Google Cloud, or Slack, as [Output Destinations](/v2/docs/output-destinations).

> **Allow Lists**
>
> Looking to add LimaCharlie outputs to an allow list? See more details [here](/v2/docs/output-allowlisting).

Destinations are the recipients of LimaCharlie streams. Oftentimes, users will rely on LimaCharlie for 365 data retention, while pushing high-fidelity alerts or other platform logs to another source for subsequent auditing or ticketing. As such, we have created native and/or easy-to-use destination options.

> **Missing a destination?**
>
> If you need support for a destination we haven't integrated yet, let us know by jumping in the [LimaCharlie Community Discourse](https://community.limacharlie.com/).

### Configuring destinations

Every destination will have both general and specific parameters. Destinations can be configured via the LimaCharlie GUI, API, or command-line.

#### General Parameters

All destinations can be configured with the following options:

* `is_flat`: take the json output and flatten the whole thing to a flat structure.
* `is_payload_as_string`: converts the payload (`event` or `detect` components) of events and detections into a JSON string instead of a JSON object.
* `inv_id`: only send events matching the investigation id to this output (event stream only).
* `tag`: only send events from sensors with this Tag to this output (event stream only).
* `cat`: only send detections from this category to this output (detect stream only).
* `cat_black_list`: only send detections that do not match the prefixes in this list (newline-separated).
* `event_white_list`: only send event of the types in this list (newline-separated, event and audit streams only).
* `event_black_list`: only send event not of the types in this list (newline-separated, event and audit streams only).
* `is_delete_on_failure`: if an error occurs during output, delete the output automatically.
* `is_prefix_data`: wrap JSON events in a dictionary with the event\_type as the key and original event as value.
* `sample_rate`: limits data sent to Output to be 1/sample\_rate.
* `custom_transform`: a [template and transforms](/v2/docs/template-strings-and-transforms) to apply to the JSON data as a last output step.

#### Specific Parameters

If you are configuring destinations using the LimaCharlie UI, required options must be provided before the output can be created.

### Output Destinations

See [Output Destinations](/v2/docs/output-destinations) for a list of supported destinations.

## Transforming Output Data

To learn how you can manipulate data prior to sending to your Output Destination, read about [Transforming Output Data](/v2/docs/template-strings-and-transforms#transformingoutputdata).

## Testing Outputs

The easiest way to test if the outputs are configured correctly is to set the stream to `Audit` which will send auditing events about activity around the management of the platform in the cloud. You can then edit the same output or make any other change on the platform, which will trigger an audit event to be sent.

After you have confirmed that the output configurations works, you can switch the data stream from `Audit` to the one you are looking to use.

If you are running into an error configuring an output, the error details will be listed in the Platform Logs section under Errors, with the key that looks like `outputs/OUTPUT_NAME`.

If an output fails, it gets disabled temporarily to avoid spam. It will be re-enabled automatically after a while, or you can force it to be re-enabled by updating the configuration.

## Use Cases

There are multiple use cases or integration strategies for shipping telemetry to and from the LimaCharlie platform. Some common approaches we have seen:

**All data over batched files via SFTP, Splunk or ELK consumes the received files for ingestion.**

```
Sensor ---> LC (All Streams) ---> SFTP ---> ( Splunk | ELK )
```

**All data streamed in real-time via Syslog, Splunk or ELK receive directly via an open Syslog socket.**

```
Sensor ---> LC (All Streams) ---> Syslog( TCP+SSL) ---> ( Splunk | ELK )
```

**All data over batched files stored on Amazon S3, Splunk or ELK consumes the received files remotely for ingestion.**

```
Sensor ---> LC (All Streams) ---> Amazon S3 ---> ( Splunk | ELK )
```

**Bulk events are uploaded to Amazon S3 for archiving, while alerts and auditing events are sent in real-time to Splunk via Syslog.** *Note: This has the added benefit of reducing Splunk license cost while keeping the raw events available for analysis at a lower cost.*

```
Sensor ---> LC (Event Stream) ---> Amazon S3
       +--> LC (Alert+Audit Streams) ---> Syslog (TCP+SSL) ---> Splunk
```

## IP Sources

Outputs from the LimaCharlie cloud do not come from a single predictible IP address due to the highly distributed nature of the cloud.

An approximation can be made using the blocks of IP addresses published by Google Cloud Platform [here](https://www.gstatic.com/ipranges/cloud.json).

The following LimaCharlie datacenters map to the following GCP regions:

* USA: `us-central1`
* Canada: `northamerica-northeast1`
* Europe: `europe-west4`
* UK: `europe-west2`
* India: `asia-south1`

---

# Syslog

## Syslog (TCP)

Output events and detections to a syslog target.

* `dest_host`: the IP or DNS and port to connect to, format `www.myorg.com:514`.
* `is_tls`: if `true` will output over TCP/TLS.
* `is_strict_tls`: if `true` will enforce validation of TLS certs.
* `is_no_header`: if `true` will not emit a Syslog header before every message. This effectively turns it into a TCP output.
* `structured_data`: arbitrary field to include in syslog "Structured Data" headers. Sometimes useful for cloud SIEMs integration.

Example:

```
dest_host: storage.corp.com
is_tls: "true"
is_strict_tls: "true"
is_no_header: "false"
```

---

# Testing Outputs

The easiest way to test if the outputs are configured correctly is to set the stream to `Audit` which will send auditing events about activity around the management of the platform in the cloud. You can then edit the same output or make any other change on the platform, which will trigger an audit event to be sent.

After you have confirmed that the output configurations works, you can switch the data stream from `Audit` to the one you are looking to use.

If you are running into an error configuring an output, the error details will be listed in the Platform Logs section under Errors, with the key that looks like `outputs/OUTPUT_NAME`.

If an output fails, it gets disabled temporarily to avoid spam. It will be re-enabled automatically after a while, or you can force it to be re-enabled by updating the configuration.

---

# Webhook

Output individually each event, detection, audit, deployment or artifact through a POST webhook.

## Configuration Parameters

* `dest_host`: the IP or DNS, port and page to HTTP(S) POST to, format `https://www.myorg.com:514/whatever`.
* `secret_key`: an arbitrary shared secret used to compute an HMAC (SHA256) signature of the webhook to verify authenticity. [See "Webhook Details" section.](https://doc.limacharlie.io/docs/documentation/ZG9jOjE5MzExMTY-outputs#webhook-details)
* `auth_header_name` and `auth_header_value`: set a specific value to a specific HTTP header name in the outgoing webhooks.

## Examples

### Basic Webhook Configuration

```
dest_host: https://webhooks.corp.com/new_detection
secret_key: this-is-my-secret-shared-key
auth_header_name: x-my-special-auth
auth_header_value: 4756345846583498
```

### Google Chat Webhook

Example [hook to Google Chat](https://developers.google.com/chat/how-tos/webhooks):

```
dest_host: https://chat.googleapis.com/v1/spaces/AAAA4-AAAB/messages?key=afsdfgfdgfE6vySjMm-dfdssss&token=pBh2oZWr7NTSj9jisenfijsnvfisnvijnfsdivndfgyOYQ%3D
secret_key: gchat-hook-sig42
custom_transform: |
   {
      "text": "Detection {{ .cat }} on {{ .routing.hostname }}: {{ .link }}"
   }
```

---

# Webhook (Bulk)

Output batches of events, detections, audits, deployments or artifacts through a POST webhook.

## Configuration Parameters

* `dest_host`: the IP or DNS, port and page to HTTP(S) POST to, format `https://www.myorg.com:514/whatever`.
* `secret_key`: an arbitrary shared secret used to compute an HMAC (SHA256) signature of the webhook to verify authenticity. This is a required field. [See "Webhook Details" section.](https://doc.limacharlie.io/docs/documentation/ZG9jOjE5MzExMTY-outputs#webhook-details)
* `auth_header_name` and `auth_header_value`: set a specific value to a specific HTTP header name in the outgoing webhooks.
* `sec_per_file`: the number of seconds after which a file is cut and uploaded.
* `is_no_sharding`: do not add a shard directory at the root of the files generated.

## Example

```
dest_host: https://webhooks.corp.com/new_detection
secret_key: this-is-my-secret-shared-key
auth_header_name: x-my-special-auth
auth_header_value: 4756345846583498
```

---

# Platform & API

# API Keys

LimaCharlie Cloud has a concept of API keys. Those are secret keys that can be created and named, and then in turn be used to retrieve a JWT that can be associated with the LC REST API at https://api.limacharlie.io.

This allows construction of headless applications able to securely acquire time-restricted REST authentication tokens it can then use.

The list of available permissions can be programmatically retrieved from this URL: <https://app.limacharlie.io/owner_permissions>

## Managing

The API Keys are managed through the Organization view of the https://limacharlie.io web interface.

## Getting a JWT

Simply issue an HTTP POST such as:

```bash
curl -X POST "https://jwt.limacharlie.io" -H "Content-Type: application/x-www-form-urlencoded" -d "oid=<YOUR_OID>&secret=<YOUR_API_KEY>"
```

where the `oid` parameter is the Organization ID as found through the web interface and the `secret` parameter is the API key.

The return value is a simple JSON response with a `jwt` component which is the JSON web token. This token is only valid for one hour to limit the possible damage of a leak, and make the deletion of the API keys easier.

Response:

```json
{ "jwt": "<JWT_VALUE_HERE>" }
```

### User API Keys

User API keys are to generate JSON web tokens (JWTs) for the REST API. In contrast to Organization API keys, the User API keys are associated with a specific user and provide the exact same access across all organizations.

This makes User API Keys very powerful but also riskier to manage. Therefore we recommend using Organization API keys whenever possible.

The User API keys can be used through all the same interfaces as the Organization API keys. The only difference is how you get the JWT. Instead of giving an `oid` parameter to `https://jwt.limacharlie.io/`, provide it with a `uid` parameter available through the LimaCharlie web interface.

```bash
curl -X POST "https://jwt.limacharlie.io" -H "Content-Type: application/x-www-form-urlencoded" -d "uid=<YOUR_USER_ID>&secret=<YOUR_API_KEY>"
```

In some instances, the JWT resulting from a User API key may be to large for normal API use, in which case you will get an `HTTP 413 Payload too large` from the API gateway. In those instances, also provide an `oid` (on top of the `uid`) to the `jwt.limacharlie.io` REST endpoint to get a JWT valid only for that organization.

```bash
curl -X POST "https://jwt.limacharlie.io" -H "Content-Type: application/x-www-form-urlencoded" -d "oid=<YOUR_OID>&uid=<YOUR_USER_ID>&secret=<YOUR_API_KEY>"
```

You may also use a User API Key to get the list of organizations available to it by querying the following REST endpoint:

```
https://app.limacharlie.io/user_key_info?secret=<YOUR_USER_API_KEY>&uid=<YOUR_USER_ID>&with_names=true
```

### Ingestion Keys

The [artifact collection](/v2/docs/artifacts) in LC requires Ingestion Keys, which can be managed through the REST API section of the LC web interface. Access to manage these Ingestion Keys requires the `ingestkey.ctrl` permission.

## Python

A simple [Python API](https://github.com/refractionpoint/python-limacharlie/) is also provided that simplifies usage of the REST API by taking care of the API Key -> JWT exchange as necessary and wraps the functionality into nicer objects.

## Privileges

API Keys have several on-off privileges available.

To see a full list, see the "REST API" section of your organization.

Making a REST call will fail with a `401` if your API Key / token is missing some privileges and the missing privilege will be specified in the error.

## Required Privileges

Below is a list of privileges required for some common tasks.

### Go Live

When "going Live" through the web UI, the following is required by the user:

* `output.*`: for the creation of the real-time output via HTTP to the browser.
* `sensor.task`: to send the commands (both manually for the console and to populate the various tabs) to the Sensor.

## Flair

API Keys may have "flair" as part of the key name. A flair is like a tag surrounded by `[]`. Although it is not required, we advise to put the flair at the end of the API key name for readability.

For example:
`orchestration-key[bulk]` is a key with a `bulk` flair.

Flairs are used to modify the behavior of an API key or provide some usage hints to various systems in LimaCharlie.

The following flairs are currently supported:

* `bulk`: indicates to the REST API that this key is meant to do a large amount of calls, the API gateway tweaks the API call limits accordingly.
* `segment`: indicates that only resources created by this key will be visible by this key. This is useful to provide access to a 3rd party in a limited fashion.

## Allowed IP Range

When creating an API key, you can optionally include an `allowed_ip_range`, which should be a [CIDR notation](https://aws.amazon.com/what-is/cidr/) IP range from which the API key can be used. Any use of the API key from a different IP address will fail. This is currently only configurable when creating an API key via the API and not in the UI.

## Glossary

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

**Organization ID**: In LimaCharlie, an Organization ID is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

**Sensors**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# AWS CloudTrail

[AWS CloudTrail](https://docs.aws.amazon.com/cloudtrail/) logs allow you to monitor AWS deployments. CloudTrail logs can provide granular visibility into AWS instances and can be used within [D&R rules](/v2/docs/detection-and-response) to identify AWS abuse.

This Adapter allows you to ingest AWS CloudTrail events via either an [S3 bucket](https://aws.amazon.com/s3/) or [SQS message queue](https://aws.amazon.com/sqs/).

CloudTrail events can be addressed in LimaCharlie as the `aws` platform.

## Adapter Deployment

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

CloudTrail logs can be collected via a cloud-to-cloud Adapter, or via the CLI Adapter. Furthermore, within each option, there is a choice of collecting logs from an S3 bucket or an SQS message queue.

## Cloud-to-Cloud Adapter

Within the LimaCharlie web application, you can create an AWS CloudTrail Cloud Connector using the `+ Add Sensor` option.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28160%29.png)

After providing an Installation Key, you will be guided through connecting either an S3 bucket or SQS queue to ingest AWS CloudTrail events.

### Collecting AWS CloudTrail Logs via an S3 Bucket

If collecting CloudTrail logs via an S3 bucket, you will need the following parameters:

* `bucket_name` - The name of the S3 bucket holding the data)
* `secret_key` - The API key for AWS that has access to the respective bucket.
* `access_key` - The AWS access key for the API key

The following sample configuration can be used to create an S3 CLI Adapter for AWS CloudTrail events:

```
s3:
  client_options:
    hostname: aws-cloudtrail-logs
    identity:
      installation_key: <INSTALLATION_KEY>
      oid: <OID>
    platform: aws
    sensor_seed_key: super-special-seed-key
  bucket_name: <S3_BUCKET_NAME>
  secret_key: <S3_SECRET_KEY>
  access_key: <S3_ACCESS_KEY>
```

### Collecting AWS CloudTrail Logs via an SQS Queue

If collecting CloudTrail logs via an SQS queue, you will need the following parameters:

* `secret_key` - The API key for AWS that has access to the respective bucket.
* `access_key` - The AWS access key for the API key
* `region` - The AWS region where the SQS instance lives
* `queue_url` - The URL to the SQS instance

The following sample configuration can be used to create an SQS CLI Adapter for AWS CloudTrail events:

```
sqs:
  client_options:
    hostname: aws-cloudtrail-logs
    identity:
      installation_key: <INSTALLATION_KEY>
      oid: <OID>
    platform: aws
    sensor_seed_key: super-special-seed-key
  region: <SQS_REGION>
  secret_key: <SQS_SECRET_KEY>
  access_key: <SQS_ACCESS_KEY>
  queue_url: <SQS_QUEUE_URL>
```

---

# Adapter Examples

## Stdin

This example is similar to the Syslog example above, except it uses the CLI Adapter and receives the data from the CLI's STDIN interface. This method is perfect for ingesting arbitrary logs on disk or from other applications locally.

```
./lc_adapter stdin client_options...
```

## Stdin JSON

This example is similar to the Stdin example above, except it assumes the data being read is JSON, not just text. If your data source is already JSON, it's much simpler to let LimaCharlie do the JSON parsing directly.

```
./lc_adapter stdin client_options...
```

## Windows Event Logs

This example shows collecting Windows Event Logs (wel) from a Windows box natively (and therefore is only available using the Windows Adapter). This is useful for cases where you'd like to collect WEL without running the LimaCharlie Windows Agent.

---

# Adapter Tutorials

## Tutorial: Creating a Webhook Adapter

LimaCharlie supports webhooks as a telemetry ingestion method. Webhooks are technically cloud Adapters, as they cannot be deployed on-prem or through the downloadable Adapter binary. Webhook adapters are created by enabling a webhook through the platform.

## Tutorial: Ingesting Google Cloud Logs

With LimaCharlie, you can easily ingest Google Cloud logs for further processing and automation. This article covers the following high-level steps of shipping logs from GCP into LimaCharlie:

- Create a Log Sink to Pubsub in GCP
- Create a Subscription to the Pubsub topic
- Configure LimaCharlie to ingest from the subscription

## Tutorial: Ingesting Telemetry from Cloud-Based External Sources

LimaCharlie allows for ingestion of logs or telemetry from any external source in real-time. It includes built-in parsing for popular formats, with the option to define your own for custom sources. There are two ways to ingest logs or telemetry from external sources:

- Using webhooks for push-based ingestion
- Using adapters for pull-based ingestion from cloud services

---

# Adapter Usage

The Adapter can be used to access many different sources and many different event types. The main mechanisms specifying the source and type of events are:

1. Adapter Type: this indicates the technical source of the events, like `syslog` or S3 buckets.
2. Platform: the platform indicates the type of events that are acquired from that source, like `text` or `carbon_black`.

Depending on the Adapter Type specified, configurations that can be specified will change. Running the adapter with no command line arguments will list all available Adapter Types and their configurations.

Configurations can be provided to the adapter in one of three ways:

1. By specifying a configuration file.
2. By specifying the configurations via the command line in the format `config-name=config-value`.
3. By specifying the configurations via the environment variables in the format `config-name=config-value`.

Here's an example config as a config file for an adapter using the `file` method of collection:

```
file: // The root of the config is the adapter collection method.
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient3
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/syslog
```

## Multi-Adapter

It is possible to execute multiple instances of adapters of the same type within the same adapter process, for example to have a single adapter process monitor files in multiple directories with slightly different configurations.

This is achieved by using a configuration file (as described above) with multiple YAML "documents" within like this:

```
file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient1
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir1/*

---

file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient2
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir2/*

---

file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient3
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir3/*
```

## Runtime Configuration

The Adapter runtime supports some custom behaviors to make it more suitable for specific deployment scenarios:

* `healthcheck`: an integer that specifies a port to start an HTTP server on that can be used for healthchecks.

## Core Configuration

All Adapter types support the same `client_options`, plus type-specific configurations. The following configurations are *required* for every Adapter:

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.
* `client_options.hostname`: a hostname for the adapter.

### Example

Using inline parameters:

```
./lc-adapter file file_path=/path/to/logs.json \
  client_options.identity.installation_key=<INSTALLATION KEY> \
  client_options.identity.oid=<ORG ID> \
  client_options.platform=json \
  client_options.sensor_seed_key=<SENSOR SEED KEY> \
  client_options.mapping.event_type_path=<EVENT TYPE FIELD> \
  client_options.hostname=<HOSTNAME>
```

Using Docker:

```
docker run -d --rm -it -p 4404:4404/udp refractionpoint/lc-adapter syslog \
  client_options.identity.installation_key=<INSTALLATION KEY> \
  client_options.identity.oid=<ORG ID> \
  client_options.platform=cef \
  client_options.hostname=<HOSTNAME> \
  client_options.sensor_seed_key=<SENSOR SEED KEY> \
  port=4404 \
  iface=0.0.0.0 \
  is_udp=true
```

Using a configuration file:

```
./lc-adapter file config_file.yaml
```

## Parsing and Mapping

### Transformation Order

Data sent via USP can be formatted in many different ways. Data is processed in a specific order as a pipeline:

1. Regular Expression with named capture groups parsing a string into a JSON object.
2. Built-in (in the cloud) LimaCharlie parsers that apply to specific `platform` values (like `carbon_black`).
3. The various "extractors" defined, like `EventTypePath`, `EventTimePath`, `SensorHostnamePath` and `SensorKeyPath`.
4. Custom `Mappings` directives provided by the client.

### Configurations

The following configurations allow you to customize the way data is ingested by the platform, including mapping and redefining fields such as the event type path and time.

* `client_options.mapping.parsing_re`: regular expression with [named capture groups](https://github.com/StefanSchroeder/Golang-Regex-Tutorial/blob/master/01-chapter2.markdown#named-matches). The name of each group will be used as the key in the converted JSON parsing.
* `client_options.mapping.parsing_grok:`  grok pattern parsing for structured data extraction from unstructured log messages. Grok patterns combine regular expressions with predefined patterns to simplify log parsing and field extraction.
* `client_options.mapping.sensor_key_path`: indicates which component of the events represent unique sensor identifiers.
* `client_options.mapping.hostname`: indicates which component of the event represents the hostname of the resulting Sensor in LimaCharlie.
* `client_options.mapping.event_type_path`: indicates which component of the event represents the Event Type of the resulting event in LimaCharlie. It also supports template strings based on each event.
* `client_options.mapping.event_time_path`: indicates which component of the event represents the Event Time of the resulting event in LimaCharlie.
* `client_options.mapping.rename_only`: *deprecated*
* `client_options.mapping.mappings`: *deprecated*
* `client_options.mapping.transform`: a Transform to apply to events.
* `client_options.mapping.drop_fields`: a list of field paths to be dropped from the data before being processed and retained.

> **Note:** The `client_options.mapping.rename_only` and `client_options.mapping.mappings` fields have been deprecated in favor of `client_options.mapping.transform`.

### Parsing

#### Named Group Parsing

If the data ingested in LimaCharlie is text (a syslog line for example), you may automatically parse it into a JSON format. To do this, you need to define one of the following:

* a grok pattern, using the `client_options.mapping.parsing_grok` option
* a regular expression, using the `client_options.mapping.parsing_re` option

#### Grok Patterns

##### Basic Syntax

Grok patterns use the following syntax:

The grok pattern line must start with **message:** , followed by the patterns, as in the example below

* `%{PATTERN_NAME:field_name}` - Extract a pattern into a named field
* `%{PATTERN_NAME}` - Match a pattern without extraction

Custom patterns can be defined using the pattern name as a key

##### Built-in Patterns

LimaCharlie includes standard Grok patterns for common data types:

* `%{IP:field_name}` - IP addresses (IPv4/IPv6)
* `%{NUMBER:field_name}` - Numeric values
* `%{WORD:field_name}` - Single words (no whitespace)
* `%{DATA:field_name}` - Any data up to delimiter
* `%{GREEDYDATA:field_name}` - All remaining data
* `%{TIMESTAMP_ISO8601:field_name}` - ISO 8601 timestamps
* `%{LOGLEVEL:field_name}` - Log levels (DEBUG, INFO, WARN, ERROR)

**Example Firewall Log Record:**

```
2024-01-01 12:00:00 ACCEPT TCP 192.168.1.100:54321 10.0.0.5:443 packets=1 bytes=78
```

**LimaCharlie Configuration to Match Firewall Log:**

```
client_options:
  mapping:
    parsing_grok:
      message: '%{TIMESTAMP_ISO8601:timestamp} %{WORD:action} %{WORD:protocol} %{IP:src_ip}:%{NUMBER:src_port} %{IP:dst_ip}:%{NUMBER:dst_port} packets=%{NUMBER:packets} bytes=%{NUMBER:bytes}'
    event_type_path: "action"
    event_time_path: "timestamp"
```

**Fields Extracted by the Above Configuration:**

```
{
  "timestamp": "2024-01-01 12:00:00",
  "action": "ACCEPT",
  "protocol": "TCP",
  "src_ip": "192.168.1.100",
  "src_port": "54321",
  "dst_ip": "10.0.0.5",
  "dst_port": "443",
  "packets": "1",
  "bytes": "78"
}
```

#### Regular Expressions

**With this log line as an example:**

```
Nov 09 10:57:09 penguin PackageKit[21212]: daemon quit
```

**you could apply the following regular expression as** `parsing_re`**:**

```
(?P<date>... \d\d \d\d:\d\d:\d\d) (?P<host>.+) (?P<exe>.+?)\[(?P<pid>\d+)\]: (?P<msg>.*)
```

which would result in the following event in LimaCharlie:

```
{
  "date": "Nov 09 10:57:09",
  "host": "penguin",
  "exe": "PackageKit",
  "pid": "21212",
  "msg": "daemon quit"
}
```

#### Key/Value Parsing

Alternatively you can specify a regular expression that does NOT contain Named Groups, like this:

```
(?:<\d+>\s*)?(\w+)=(".*?"|\S+)
```

When in this mode, LimaCharlie assumes the regular expression will generate a list of matches where each match has 2 submatches, and submatch index 1 is the Key name, and submatch index 2 is the value. This is compatible with logs like CEF for example where the log could look like:

```
<20>hostname=my-host log_name=http_logs timestamp=....
```

which would end up generating:

```
{
  "hostname" : "my-host",
  "log_name": "http_logs",
  "timestamp": "..."
}
```

#### Extraction

LimaCharlie has a few core constructs that all events and sensors have.
Namely:

* Sensor ID
* Hostname
* Event Type
* Event Time

You may specify certain fields from the JSON logs to be extracted into these common fields.

This process is done by specifying the "path" to the relevant field in the JSON data. Paths are like a directory path using `/` for each sub directory except that in our case, they describe how to get to the relevant field from the top level of the JSON.

For example, using this event:

```
{
  "a": "x",
  "b": "y",
  "c": {
    "d": {
      "e": "z"
    }
  }
}
```

The following paths would yield the following results:

* `a`: `x`
* `b`: `y`
* `c/d/e`: `z`

The following extractors can be specified:

* `client_options.mapping.sensor_key_path`: indicates which component of the events represent unique sensor identifiers.
* `client_options.mapping.sensor_hostname_path`: indicates which component of the event represents the hostname of the resulting Sensor in LimaCharlie.
* `client_options.mapping.event_type_path`: indicates which component of the event represents the Event Type of the resulting event in LimaCharlie. It also supports template strings based on each event.
* `client_options.mapping.event_time_path`: indicates which component of the event represents the Event Time of the resulting event in LimaCharlie.

### Indexing

Indexing occurs in one of 3 ways:

1. By the built-in indexer for specific platforms like Carbon Black.
2. By a generic indexer applied to all fields if no built-in indexer was available.
3. Optionally, user-specific indexing guidelines.

#### User Defined Indexing

An Adapter can be configured to do custom indexing on the data it feeds.

This is done by setting the `indexing` element in the `client_options`. This field contains a list of index descriptors.

An index descriptor can have the following fields:

* `events_included`: optionally, a list of event\_type that this descriptor applies to.
* `events_excluded`: optionally, a list of event\_type this descriptor *does not* apply to.
* `path`: the element path this descriptor targets, like `user/metadata/user_id`.
* `regexp`: optionally, a regular expression used on the `path` field to extract the item to index, like `email: (.+)`.
* `index_type`: the category of index the value extracted belongs to, like `user` or `file_hash`.

Here is an example of a simple index descriptor:

```
events_included:
  - PutObject
path: userAgent
index_type: user
```

Put together in a client option, you could have:

```
{
  "client_options": {
    ...,
    "indexing": [{
      "events_included": ["PutObject"],
      "path": "userAgent",
      "index_type": "user"
    }, {
      "events_included": ["DelObject"],
      "path": "original_user/userAgent",
      "index_type": "user"
    }]
  }
}
```

#### Supported Indexes

This is the list of currently supported index types:

* `file_hash`
* `file_path`
* `file_name`
* `domain`
* `ip`
* `user`
* `service_name`
* `package_name`

### Sensor IDs

USP Clients generate LimaCharlie Sensors at runtime. The ID of those sensors (SID) is generated based on the Organization ID (OID) and the Sensor Seed Key.

This implies that if want to re-key an IID (perhaps it was leaked), you may replace the IID with a new valid one. As long as you use the same OID and Sensor Seed Key, the generated SIDs will be stable despite the IID change.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Adapter Usage

The Adapter can be used to access many different sources and many different event types. The main mechanisms specifying the source and type of events are:

1. Adapter Type: this indicates the technical source of the events, like `syslog` or S3 buckets.
2. Platform: the platform indicates the type of events that are acquired from that source, like `text` or `carbon_black`.

Depending on the Adapter Type specified, configurations that can be specified will change. Running the adapter with no command line arguments will list all available Adapter Types and their configurations.

Configurations can be provided to the adapter in one of three ways:

1. By specifying a configuration file.
2. By specifying the configurations via the command line in the format `config-name=config-value`.
3. By specifying the configurations via the environment variables in the format `config-name=config-value`.

Here's an example config as a config file for an adapter using the `file` method of collection:

```
file: // The root of the config is the adapter collection method.
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient3
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/syslog
```

## Multi-Adapter

It is possible to execute multiple instances of adapters of the same type within the same adapter process, for example to have a single adapter process monitor files in multiple directories with slightly different configurations.

This is achieved by using a configuration file (as described above) with multiple YAML "documents" within like this:

```
file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient1
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir1/*

---

file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient2
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir2/*

---

file:
  client_options:
    identity:
      installation_key: e9a3bcdf-efa2-47ae-b6df-579a02f3a54d
      oid: 8cbe27f4-bfa1-4afb-ba19-138cd51389cd
    platform: json
    sensor_seed_key: testclient3
    mapping:
      event_type_path: syslog-events
  file_path: /var/log/dir3/*
```

## Runtime Configuration

The Adapter runtime supports some custom behaviors to make it more suitable for specific deployment scenarios:

* `healthcheck`: an integer that specifies a port to start an HTTP server on that can be used for healthchecks.

## Core Configuration

All Adapter types support the same `client_options`, plus type-specific configurations. The following configurations are *required* for every Adapter:

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.
* `client_options.hostname`: a hostname for the adapter.

### Example

Using inline parameters:

```
./lc-adapter file file_path=/path/to/logs.json \
  client_options.identity.installation_key=<INSTALLATION KEY> \
  client_options.identity.oid=<ORG ID> \
  client_options.platform=json \
  client_options.sensor_seed_key=<SENSOR SEED KEY> \
  client_options.mapping.event_type_path=<EVENT TYPE FIELD> \
  client_options.hostname=<HOSTNAME>
```

Using Docker:

```
docker run -d --rm -it -p 4404:4404/udp refractionpoint/lc-adapter syslog \
  client_options.identity.installation_key=<INSTALLATION KEY> \
  client_options.identity.oid=<ORG ID> \
  client_options.platform=cef \
  client_options.hostname=<HOSTNAME> \
  client_options.sensor_seed_key=<SENSOR SEED KEY> \
  port=4404 \
  iface=0.0.0.0 \
  is_udp=true
```

Using a configuration file:

```
./lc-adapter file config_file.yaml
```

## Parsing and Mapping

### Transformation Order

Data sent via USP can be formatted in many different ways. Data is processed in a specific order as a pipeline:

1. Regular Expression with named capture groups parsing a string into a JSON object.
2. Built-in (in the cloud) LimaCharlie parsers that apply to specific `platform` values (like `carbon_black`).
3. The various "extractors" defined, like `EventTypePath`, `EventTimePath`, `SensorHostnamePath` and `SensorKeyPath`.
4. Custom `Mappings` directives provided by the client.

### Configurations

The following configurations allow you to customize the way data is ingested by the platform, including mapping and redefining fields such as the event type path and time.

* `client_options.mapping.parsing_re`: regular expression with [named capture groups](https://github.com/StefanSchroeder/Golang-Regex-Tutorial/blob/master/01-chapter2.markdown#named-matches). The name of each group will be used as the key in the converted JSON parsing.
* `client_options.mapping.parsing_grok:`  grok pattern parsing for structured data extraction from unstructured log messages. Grok patterns combine regular expressions with predefined patterns to simplify log parsing and field extraction.
* `client_options.mapping.sensor_key_path`: indicates which component of the events represent unique sensor identifiers.
* `client_options.mapping.hostname`: indicates which component of the event represents the hostname of the resulting Sensor in LimaCharlie.
* `client_options.mapping.event_type_path`: indicates which component of the event represents the Event Type of the resulting event in LimaCharlie. It also supports template strings based on each event.
* `client_options.mapping.event_time_path`: indicates which component of the event represents the Event Time of the resulting event in LimaCharlie.
* `client_options.mapping.rename_only`: *deprecated*
* `client_options.mapping.mappings`: *deprecated*
* `client_options.mapping.transform`: a Transform to apply to events.
* `client_options.mapping.drop_fields`: a list of field paths to be dropped from the data before being processed and retained.

> **Note:** The `client_options.mapping.rename_only` and `client_options.mapping.mappings` fields have been deprecated in favor of `client_options.mapping.transform`.

### Parsing

#### Named Group Parsing

If the data ingested in LimaCharlie is text (a syslog line for example), you may automatically parse it into a JSON format. To do this, you need to define one of the following:

* a grok pattern, using the `client_options.mapping.parsing_grok` option
* a regular expression, using the `client_options.mapping.parsing_re` option

#### Grok Patterns

##### Basic Syntax

Grok patterns use the following syntax:

The grok pattern line must start with **message:** , followed by the patterns, as in the example below

* `%{PATTERN_NAME:field_name}` - Extract a pattern into a named field
* `%{PATTERN_NAME}` - Match a pattern without extraction

Custom patterns can be defined using the pattern name as a key

##### Built-in Patterns

LimaCharlie includes standard Grok patterns for common data types:

* `%{IP:field_name}` - IP addresses (IPv4/IPv6)
* `%{NUMBER:field_name}` - Numeric values
* `%{WORD:field_name}` - Single words (no whitespace)
* `%{DATA:field_name}` - Any data up to delimiter
* `%{GREEDYDATA:field_name}` - All remaining data
* `%{TIMESTAMP_ISO8601:field_name}` - ISO 8601 timestamps
* `%{LOGLEVEL:field_name}` - Log levels (DEBUG, INFO, WARN, ERROR)

**Example Firewall Log Record:**

```
2024-01-01 12:00:00 ACCEPT TCP 192.168.1.100:54321 10.0.0.5:443 packets=1 bytes=78
```

**LimaCharlie Configuration to Match Firewall Log:**

```
client_options:
  mapping:
    parsing_grok:
      message: '%{TIMESTAMP_ISO8601:timestamp} %{WORD:action} %{WORD:protocol} %{IP:src_ip}:%{NUMBER:src_port} %{IP:dst_ip}:%{NUMBER:dst_port} packets=%{NUMBER:packets} bytes=%{NUMBER:bytes}'
    event_type_path: "action"
    event_time_path: "timestamp"
```

**Fields Extracted by the Above Configuration:**

```
{
  "timestamp": "2024-01-01 12:00:00",
  "action": "ACCEPT",
  "protocol": "TCP",
  "src_ip": "192.168.1.100",
  "src_port": "54321",
  "dst_ip": "10.0.0.5",
  "dst_port": "443",
  "packets": "1",
  "bytes": "78"
}
```

#### Regular Expressions

**With this log line as an example:**

```
Nov 09 10:57:09 penguin PackageKit[21212]: daemon quit
```

**you could apply the following regular expression as** `parsing_re`**:**

```
(?P<date>... \d\d \d\d:\d\d:\d\d) (?P<host>.+) (?P<exe>.+?)\[(?P<pid>\d+)\]: (?P<msg>.*)
```

which would result in the following event in LimaCharlie:

```
{
  "date": "Nov 09 10:57:09",
  "host": "penguin",
  "exe": "PackageKit",
  "pid": "21212",
  "msg": "daemon quit"
}
```

#### Key/Value Parsing

Alternatively you can specify a regular expression that does NOT contain Named Groups, like this:

```
(?:<\d+>\s*)?(\w+)=(".*?"|\S+)
```

When in this mode, LimaCharlie assumes the regular expression will generate a list of matches where each match has 2 submatches, and submatch index 1 is the Key name, and submatch index 2 is the value. This is compatible with logs like CEF for example where the log could look like:

```
<20>hostname=my-host log_name=http_logs timestamp=....
```

which would end up generating:

```
{
  "hostname" : "my-host",
  "log_name": "http_logs",
  "timestamp": "..."
}
```

#### Extraction

LimaCharlie has a few core constructs that all events and sensors have.
Namely:

* Sensor ID
* Hostname
* Event Type
* Event Time

You may specify certain fields from the JSON logs to be extracted into these common fields.

This process is done by specifying the "path" to the relevant field in the JSON data. Paths are like a directory path using `/` for each sub directory except that in our case, they describe how to get to the relevant field from the top level of the JSON.

For example, using this event:

```
{
  "a": "x",
  "b": "y",
  "c": {
    "d": {
      "e": "z"
    }
  }
}
```

The following paths would yield the following results:

* `a`: `x`
* `b`: `y`
* `c/d/e`: `z`

The following extractors can be specified:

* `client_options.mapping.sensor_key_path`: indicates which component of the events represent unique sensor identifiers.
* `client_options.mapping.sensor_hostname_path`: indicates which component of the event represents the hostname of the resulting Sensor in LimaCharlie.
* `client_options.mapping.event_type_path`: indicates which component of the event represents the Event Type of the resulting event in LimaCharlie. It also supports template strings based on each event.
* `client_options.mapping.event_time_path`: indicates which component of the event represents the Event Time of the resulting event in LimaCharlie.

### Indexing

Indexing occurs in one of 3 ways:

1. By the built-in indexer for specific platforms like Carbon Black.
2. By a generic indexer applied to all fields if no built-in indexer was available.
3. Optionally, user-specific indexing guidelines.

#### User Defined Indexing

An Adapter can be configured to do custom indexing on the data it feeds.

This is done by setting the `indexing` element in the `client_options`. This field contains a list of index descriptors.

An index descriptor can have the following fields:

* `events_included`: optionally, a list of event\_type that this descriptor applies to.
* `events_excluded`: optionally, a list of event\_type this descriptor *does not* apply to.
* `path`: the element path this descriptor targets, like `user/metadata/user_id`.
* `regexp`: optionally, a regular expression used on the `path` field to extract the item to index, like `email: (.+)`.
* `index_type`: the category of index the value extracted belongs to, like `user` or `file_hash`.

Here is an example of a simple index descriptor:

```
events_included:
  - PutObject
path: userAgent
index_type: user
```

Put together in a client option, you could have:

```
{
  "client_options": {
    ...,
    "indexing": [{
      "events_included": ["PutObject"],
      "path": "userAgent",
      "index_type": "user"
    }, {
      "events_included": ["DelObject"],
      "path": "original_user/userAgent",
      "index_type": "user"
    }]
  }
}
```

#### Supported Indexes

This is the list of currently supported index types:

* `file_hash`
* `file_path`
* `file_name`
* `domain`
* `ip`
* `user`
* `service_name`
* `package_name`

### Sensor IDs

USP Clients generate LimaCharlie Sensors at runtime. The ID of those sensors (SID) is generated based on the Organization ID (OID) and the Sensor Seed Key.

This implies that if want to re-key an IID (perhaps it was leaked), you may replace the IID with a new valid one. As long as you use the same OID and Sensor Seed Key, the generated SIDs will be stable despite the IID change.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Adapters

The LimaCharlie Adapter provides real-time ingestion of logs and other telemetry. Adapters allow you to send *any* data to LimaCharlie, which becomes an observable telemetry stream. All ingested data is recognized as a first-class data source, allowing you to write [detection & response rules](/v2/docs/detection-and-response) directly against Adapter events or [output](/v2/docs/outputs) Adapter data to other destinations.

The LimaCharlie Adapter:

* Supports ingestion of any structured data, such as JSON, Syslog, or CEFL
* Can be deployed on-prem or via a cloud-to-cloud connector
* Ingest log data without the need for an Endpoint Agent.
* Can run alongside the Endpoint Agent to allow for additional telemetry collection.

For certain well known Adapter sources, we offer built-in mapping and recognition of events. This allows you to ingest a known source without the need to parse event structure yourself. Well-known Adapter types include cloud platforms, various third-party applications, and sources like Windows Event Logs.

Key concepts of Adapters include [deployment options](/v2/docs/adapter-deployment) and [usage/configurations](/v2/docs/adapter-usage).

## Text Adapters

Text-based Adapters facilitate the ingestion of *any* structured text into LimaCharlie. Events are ingested and normalized as JSON text events, however custom mapping options allow you to customize fields and data structures as you see fit. Ingested data is also observed via detection and response rules, allowing you to ingest any data source and automate on those events.

## Pre-defined Adapters

LimaCharlie has pre-defined Adapter types that offer built-in mapping and guided adapter setups. Note that our guided setups often utilize common ingestion methods, however are designed to help you quickly deploy an Adapter for frequent log sources.

For example, AWS CloudTrail *and* Amazon GuardDuty logs are available for ingestion from either an AWS S3 bucket or Simple Queue Service (SQS) events. Thus, the web app "helper" walks you through setting up either one of those sources, depending on your needs and architecture.

> **Cloud Adapter Sinks**
> 
> Note - certain cloud-to-cloud adapters, such as AWS S3 and Google Cloud Storage ingest data as a sink, meaning blobs will be deleted as they are consumed. The ingestion API will require the ability to delete objects in these adapters. To avoid any errors, we recommend creating a dedicated bucket (with appropriate permissions) to ingest logs into LimaCharlie.

For other data streams, where unique connector details are required (e.g. Office 365 or Slack), we will provide guidance on establishing those connections. More information on pre-defined Adapters can be found [here](/v2/docs/adapter-types).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

---

# Adapters as a Service

In some cases, users may need to install the LimaCharlie Adapter with persistence, to ensure that data collection survives a reboot and/or other disruptions.

To accommodate this need, the LimaCharlie adapter can be installed as a service.

## Service Installation

### Windows

To install the Windows LimaCharlie adapter as a service, insert the `-install:<service_name>` flag in the command line, following the adapter executable name.

For example:

`./lc_adapter.exe azure_event_hub client_options.identity.installation_key=...`

would be replaced with

`./lc_adapter.exe -install:azure_collection azure_event_hub client_options.identity.installation_key=...`

This would create a service named `azure_collection` with the adapter config.

Remember, adapter configurations can be provided via two methods:

* In the command line, as part of a list of flags
* Via a YAML config file

**Note:** The service will point to `lc_adapter.exe` based on its path at the creation of the service. If you wish to move the adapter to a permanent location, please do so before creating the service.

### Linux / systemd

To install a LimaCharlie adapter as a service on a Linux system with systemd, you will need a service file, the adapter binary, and your adapter command.

#### Adapter Binary

Download one of the adapter binaries and apply the necessary permissions:

```
wget -O /path/to/adapter-directory/lc-adapter $ADAPTER_BINARY_URL
chmod +x /path/to/adapter-directory/lc-adapter
```

#### Service File - /etc/systemd/system/limacharlie-adapter-name.service

You will replace `$ADAPTER_COMMAND` in the service file with your actual adapter command below.

```
[Unit]
Description=LC Adapter Name
After=network.target

[Service]
Type=simple
ExecStart=$ADAPTER_COMMAND
WorkingDirectory=/path/to/adapter-directory
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=lc-adapter-name

[Install]
WantedBy=multi-user.target
```

#### Adapter Command

Your adapter command may differ depending on your use case--this is an example of a file adapter to ingest logs from a JSON file.

```
/path/to/adapter-directory/lc-adapter file file_path=/path/to/logs.json client_options.identity.installation_key=<INSTALLATION KEY> client_options.identity.oid=<ORG ID> client_options.platform=json client_options.sensor_seed_key=<SENSOR SEED KEY> client_options.mapping.event_type_path=<EVENT TYPE FIELD> client_options.hostname=<HOSTNAME>
```

#### Enable and Start the Service

```
sudo systemctl enable lc-adapter-name
sudo systemctl start lc-adapter-name
sudo systemctl status lc-adapter-name
```

## Service Uninstallation

### Windows

To remove a Windows LimaCharlie Adapter service, use the `-remove:<service_name>` flag.

### Linux

If your service is running with a systemd script, you can disable and remove it with the following:

```
sudo systemctl stop lc-adapter-name
sudo systemctl disable lc-adapter-name
sudo rm /etc/systemd/system/lc-adapter-name.service
sudo rm /path/to/adapter-directory/lc-adapter
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# EVTX

## Overview

This Adapter allows you to ingest and convert a `.evtx` file into LimaCharlie. The `.evtx` files are the binary format used by Microsoft for Windows Event Logs. This is useful to ingest historical Windows Event Logs, for example during an Incident Response (IR) engagement.

For real-time collection of Windows Event Logs, see the [Windows Event Logs](/v2/docs/ingesting-windows-event-logs) documentation.

## Configurations

Adapter Type: `evtx`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `file_path`: path to the `.evtx` file to ingest.

## API Doc

See the [unofficial documentation on EVTX](https://www.giac.org/paper/gcia/2999/evtx-windows-event-logging/115806).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# File

## Overview

This Adapter allows you to ingest logs from a file, either as a one time operation or by following its output (like `tail -f`). A more detailed guide to file collection can be found in the [Log Collection Guide](/v2/docs/logcollectionguide).

### Configuration

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

Adapter type `file`:

* `file_path`: simple file pattern like `./files_*.txt`
* `no_follow`: if `true`, the file content will be sent, but additions to the file will not be reported
* `inactivity_threshold`: the number of seconds after which an unmodified file becomes ignored
* `backfill`: if `true`, a single pass at all the matching files will be made to ingest them, useful for historical ingestion
* `serialize_files`: if `true`, files will be ingested one at a time, useful for very large number of files that could blow up memory

### CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter file client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=text \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
file_path=/path/to/file
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Google Cloud Pubsub

## Overview

This Adapter allows you to ingest events from a Google Cloud Pubsub subscription.

## Configurations

Adapter Type: `pubsub`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `sub_name`: the name of the subscription to subscribe to.
* `service_account_creds`: the string version of the JSON credentials for a (Google) Service Account to use accessing the subscription.
* `project_name`: project name where the `sub_name` exists.

### CLI Deployment

This example assumes that the Adapter is running from a host that has [default credentials](https://cloud.google.com/docs/authentication/production) (via the `GOOGLE_APPLICATION_CREDENTIALS` environment variable) setup. If it's not the case you will need to use `service_account_creds` to provide the contents of the JSON credentials of the GCP Service Account to use.

```
./lc_adapter pubsub client_options.identity.installation_key=f5eaaaad-575a-498e-bfc2-5f83e249a646 \
    client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
    client_options.platform=gcp \
    sub_name=usp \
    project_name=monitored-proj \
    client_options.sensor_seed_key=gcplogs
```

Here's the breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `pubsub`: the method the Adapter should use to collect data locally.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=gcp`: this indicates that the data read is logs from Google Cloud Platform.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `sub_name=usp`: the Subscription name to consume the logs from.
* `project_name=monitored-proj`: the GCP Project name this Subscription belongs to.

### Infrastructure as Code Deployment

```
# Google Cloud Pub/Sub Specific Docs: https://docs.limacharlie.io/docs/adapter-types-google-cloud-pubsub

sensor_type: "pubsub"
pubsub:
  sub_name: "your-pubsub-subscription-name"
  project_name: "your-gcp-project-id"
  service_account_creds: "hive://secret/gcp-pubsub-service-account"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_PUBSUB"
    platform: "json"
    sensor_seed_key: "gcp-pubsub-sensor"
    mapping:
      # Map Pub/Sub message to sensor fields
      sensor_hostname_path: "attributes.hostname"
      event_type_path: "attributes.eventType"
      event_time_path: "publishTime"
    indexing: []
  # Optional configuration
  max_ps_buffer: 1048576  # 1MB buffer (optional)
```

## API Doc

See the [official documentation](https://cloud.google.com/pubsub).

---

# Google Cloud Pubsub

## Overview

This Adapter allows you to ingest events from a Google Cloud Pubsub subscription.

## Configurations

Adapter Type: `pubsub`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `sub_name`: the name of the subscription to subscribe to.
* `service_account_creds`: the string version of the JSON credentials for a (Google) Service Account to use accessing the subscription.
* `project_name`: project name where the `sub_name` exists.

### CLI Deployment

This example assumes that the Adapter is running from a host that has [default credentials](https://cloud.google.com/docs/authentication/production) (via the `GOOGLE_APPLICATION_CREDENTIALS` environment variable) setup. If it's not the case you will need to use `service_account_creds` to provide the contents of the JSON credentials of the GCP Service Account to use.

```
./lc_adapter pubsub client_options.identity.installation_key=f5eaaaad-575a-498e-bfc2-5f83e249a646 \
    client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
    client_options.platform=gcp \
    sub_name=usp \
    project_name=monitored-proj \
    client_options.sensor_seed_key=gcplogs
```

Here's the breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `pubsub`: the method the Adapter should use to collect data locally.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=gcp`: this indicates that the data read is logs from Google Cloud Platform.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `sub_name=usp`: the Subscription name to consume the logs from.
* `project_name=monitored-proj`: the GCP Project name this Subscription belongs to.

### Infrastructure as Code Deployment

```yaml
# Google Cloud Pub/Sub Specific Docs: https://docs.limacharlie.io/docs/adapter-types-google-cloud-pubsub

sensor_type: "pubsub"
pubsub:
  sub_name: "your-pubsub-subscription-name"
  project_name: "your-gcp-project-id"
  service_account_creds: "hive://secret/gcp-pubsub-service-account"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_PUBSUB"
    platform: "json"
    sensor_seed_key: "gcp-pubsub-sensor"
    mapping:
      # Map Pub/Sub message to sensor fields
      sensor_hostname_path: "attributes.hostname"
      event_type_path: "attributes.eventType"
      event_time_path: "publishTime"
    indexing: []
  # Optional configuration
  max_ps_buffer: 1048576  # 1MB buffer (optional)
```

## API Doc

See the [official documentation](https://cloud.google.com/pubsub).

---

# Google Cloud Storage

## Overview

This Adapter allows you to ingest files/blobs stored in Google Cloud Storage (GCS).

Note that this adapter operates as a sink by default, meaning it will "consume" files from the GCS bucket by deleting them once ingested.

## Configurations

Adapter Type: `gcs`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `bucket_name`: the name of the bucket to ingest from.
* `service_account_creds`: the string version of the JSON credentials for a (Google) Service Account to use accessing the bucket.
* `prefix`: only ingest files with a given path prefix.
* `single_load`: if `true`, the adapter will not operate as a sink, it will ingest all files in the bucket once and will then exit.

### Infrastructure as Code Deployment

```
# Google Cloud Storage (GCS) Specific Docs: https://docs.limacharlie.io/docs/adapter-types-gcs

sensor_type: "gcs"
gcs:
  bucket_name: "your-gcs-bucket-for-limacharlie-logs"
  service_account_creds: "hive://secret/gcs-service-account"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_GCS"
    platform: "json"
    sensor_seed_key: "gcs-log-processor"
    mapping:
      sensor_hostname_path: "resource.labels.instance_id"
      event_type_path: "logName"
      event_time_path: "timestamp"
    indexing: []
  # Optional configuration
  prefix: "security_logs/firewall/"  # Filter by path prefix
  parallel_fetch: 5                  # Parallel downloads
  single_load: false                 # Continuous processing
```

## API Doc

See the [official documentation](https://cloud.google.com/storage).

---

# Google Cloud Storage

## Overview

This Adapter allows you to ingest files/blobs stored in Google Cloud Storage (GCS).

Note that this adapter operates as a sink by default, meaning it will "consume" files from the GCS bucket by deleting them once ingested.

## Configurations

Adapter Type: `gcs`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `bucket_name`: the name of the bucket to ingest from.
* `service_account_creds`: the string version of the JSON credentials for a (Google) Service Account to use accessing the bucket.
* `prefix`: only ingest files with a given path prefix.
* `single_load`: if `true`, the adapter will not operate as a sink, it will ingest all files in the bucket once and will then exit.

### Infrastructure as Code Deployment

```
# Google Cloud Storage (GCS) Specific Docs: https://docs.limacharlie.io/docs/adapter-types-gcs

sensor_type: "gcs"
gcs:
  bucket_name: "your-gcs-bucket-for-limacharlie-logs"
  service_account_creds: "hive://secret/gcs-service-account"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_GCS"
    platform: "json"
    sensor_seed_key: "gcs-log-processor"
    mapping:
      sensor_hostname_path: "resource.labels.instance_id"
      event_type_path: "logName"
      event_time_path: "timestamp"
    indexing: []
  # Optional configuration
  prefix: "security_logs/firewall/"  # Filter by path prefix
  parallel_fetch: 5                  # Parallel downloads
  single_load: false                 # Continuous processing
```

## API Doc

See the [official documentation](https://cloud.google.com/storage).

---

# Google Workspace

[Google Workspace](https://workspace.google.com/) provides various communication, collaboration, and productivity applications for businesses of all sizes. [Google Workspace audit logs](https://cloud.google.com/logging/docs/audit/gsuite-audit-logging) provide data to help track "Who did what, where, and when?".

Google Workspace Audit logs can be ingested via a Google Cloud Platform, deployed as a cloud-to-cloud LimaCharlie Adapter. Events will be ingested and observed via the `gcp` platform.

## Adapter Deployment

Prior to ingesting Google Workspace Audit logs in LimaCharlie, you must first configure logs to be written to GCP. Afterwards, a cloud-to-cloud GCP Adapter can be deployed to ingest these events into LimaCharlie.

The following steps help prepare this:

**Step 1: Enable Platform Sharing in Google Workspace**

In the Google Workspace admin console navigate to [Account -> Account Settings -> Legal and Compliance](https://admin.google.com/u/1/ac/companyprofile/legal)

Verify that under "Sharing options", `Google Cloud Platform Sharing Options` is set to Enabled.

For further details, refer to [Google's documentation on Audit logs for Google Workspace](https://cloud.google.com/logging/docs/audit/gsuite-audit-logging)

**Step 2: Verify logs appear in Google Cloud Platform**

In the GCP Console go to the [Logs Explorer](https://console.cloud.google.com/logs/query). Ensure you're at the organization level (and not in a particular folder).

From the Resources drop-down, choose `Audited Resource`, then press Apply.

You should see logging details related to Google Workspace, under the following log name(s):

`logName:admin.googleapis.com`

**Step 3: Create a cloud-to-cloud GCP Adapter**

Once Google Workspace Audit logs are pushed to GCP, events can be ingested via either [Google Cloud Storage](/v2/docs/adapter-types-google-cloud-storage) or [Google Cloud Pubsub](/v2/docs/adapter-types-google-cloud-pubsub). Utilize the appropriate documentation to set up the desired Adapter.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Google Workspace

[Google Workspace](https://workspace.google.com/) provides various communication, collaboration, and productivity applications for businesses of all sizes. [Google Workspace audit logs](https://cloud.google.com/logging/docs/audit/gsuite-audit-logging) provide data to help track "Who did what, where, and when?".

Google Workspace Audit logs can be ingested via a Google Cloud Platform, deployed as a cloud-to-cloud LimaCharlie Adapter. Events will be ingested and observed via the `gcp` platform.

## Adapter Deployment

Prior to ingesting Google Workspace Audit logs in LimaCharlie, you must first configure logs to be written to GCP. Afterwards, a cloud-to-cloud GCP Adapter can be deployed to ingest these events into LimaCharlie.

The following steps help prepare this:

**Step 1: Enable Platform Sharing in Google Workspace**

In the Google Workspace admin console navigate to [Account -> Account Settings -> Legal and Compliance](https://admin.google.com/u/1/ac/companyprofile/legal)

Verify that under "Sharing options", `Google Cloud Platform Sharing Options` is set to Enabled.

For further details, refer to [Google's documentation on Audit logs for Google Workspace](https://cloud.google.com/logging/docs/audit/gsuite-audit-logging)

**Step 2: Verify logs appear in Google Cloud Platform**

In the GCP Console go to the [Logs Explorer](https://console.cloud.google.com/logs/query). Ensure you're at the organization level (and not in a particular folder).

From the Resources drop-down, choose `Audited Resource`, then press Apply.

You should see logging details related to Google Workspace, under the following log name(s):

`logName:admin.googleapis.com`

**Step 3: Create a cloud-to-cloud GCP Adapter**

Once Google Workspace Audit logs are pushed to GCP, events can be ingested via either [Google Cloud Storage](/v2/docs/adapter-types-google-cloud-storage) or [Google Cloud Pubsub](/v2/docs/adapter-types-google-cloud-pubsub). Utilize the appropriate documentation to set up the desired Adapter.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# HubSpot

## Overview

This Adapter allows you to connect to HubSpot to fetch [account activity logs](https://developers.hubspot.com/docs/guides/api/settings/account-activity-api).

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `hubspot`

* `access_token`: your HubSpot access token

### Manual Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment#adapter-binaries).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter hubspot client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
client_options.mappings.event_type_path=category \
access_token=$ACCESS_TOKEN
```

### Infrastructure as Code Deployment

```
sensor_type: hubspot
  hubspot:
    access_token: "YOUR_HUBSPOT_PRIVATE_APP_ACCESS_TOKEN"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_HUBSPOT"
      destination:
        hostname: "input.limacharlie.io"
        port: 443
        is_tls: true
      net:
        identity_timeout: 30
        request_timeout: 30
        heartbeat_timeout: 120
      indexing: []
```

## API Doc

See the official [documentation](https://developers.hubspot.com/docs/reference/api/settings/account-activity-api).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# IIS Logs

Microsoft's Internet Information Services (IIS) web server is a web server commonly found on Microsoft Windows servers. This Adapter assists with sending IIS web logs to LimaCharlie via the Adapter binary.

Telemetry Platform (if applicable): `iis`

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

IIS web logs often have a standardized schema, unless manually changed by administrators. The `iis` platform in LimaCharlie expects the following structure:

`#Fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) cs(Referer) sc-status sc-substatus sc-win32-status time-taken`

**Log Structure**

If your IIS logs are a different structure from above, please let us know and we can assist in customizing the parser!

The structure of these fields is as follows:

| Field Name | Explanation |
| --- | --- |
| date | Date of log entry |
| time | Time of log entry |
| s-ip | The IP address of the web server |
| cs-method | The method of request from the client |
| cs-uri-stem | The URI requested by the client |
| cs-uri-query | The query added to the URI in the client request |
| s-port | The server port) |
| cs-username | The client username (if provided) |
| c-ip | The IP address of the client |
| cs-user-agent | The user-agent of the client |
| cs-referer | The referer that directed the client to the site |
| sc-status | The service status code |
| sc-substatus | The service substatus code (if applicable) |
| sc-win32-status | The Windows status code |
| time-taken | The time taken to render the request resource(s) |

## Configuration File

IIS logs are typically stored "on disk" of the web server, in files that roll daily. Thus, collecting IIS web logs would be done with a binary Adapter that can monitor specific IIS log folder(s) for new files. The Adapter type would be `file`, while the platform is `iis`.

The following configuration file can be used as a starter to monitor IIS web log directories. Replace any values with `< >` characters with values unique to your Organization and/or deployment. *Do not include the* `<` *or* `>` *characters in your config file!*

*Please customize according to your environment/LimaCharlie organization*

```
file:
  client_options:
    identity:
      installation_key: <installation key>
      oid: <organization id>
    platform: iis
    sensor_seed_key: <sensor_seed_key>
    // The following will map the timestamp of the event to the timestamp in the web log. Remove if you'd prefer to keep the event time as the time of ingestion.
    mapping:
      event_time_path: ts
  file_path: <C:\path\to\web\logs\u*.log>
  no_follow: false
```

A few notes about the IIS platform parser:

* The server IP address (identified in the logs as `s-ip` will be used as the hostname within LimaCharlie.
* The `date` and `time` fields are combined to a single field represented as `ts`. The above configuration uses this field as the event time, unless removed.
* The `sensor_seed_key` can be any value of your choosing, please make sure it's unique per web server.
* You can specify multiple configurations in one file if you wish to collect logs from multiple folders.
* The `no_follow: false` specification ensures that the Adapter monitors for new files and/or writes to existing files. You can exclude this option if you are going to ingest "dead" log files.
* All IIS events will be represented as `IIS_WEBLOG` in the Adapter telemetry.

If you have any questions about collecting IIS web logs, please reach out to the LimaCharlie team.

Once the config file is set, you can run the Adapter on Windows with the following command (assuming the file is named `config.yaml`):

`<adapter_name>.exe file config.yaml`

## Example Event

```
{
    "c-ip": "192.168.1.11",
    "cs-method": "GET",
    "cs-referer)": "-",
    "cs-uri-query": "-",
    "cs-uri-stem": "/path/to/my/web/page",
    "cs-user-agent": "Mozilla/5.0+(Windows+NT+10.0;+Win64;+x64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/128.0.0.0+Safari/537.36",
    "cs-username": "-",
    "s-ip": "192.168.1.10",
    "s-port": "99",
    "sc-status": "401",
    "sc-substatus": "2",
    "sc-win32-status": "5",
    "time-taken": "143",
    "ts": "2024-09-05 12:36:14"
}
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# IP Geolocation

With the `ip-geo` [add-on](https://app.limacharlie.io/add-ons/detail/ip-geo) subscribed, it can be used as an API-based lookup.

```
event: CONNECTED
op: lookup
resource: hive://lookup/ip-geo
path: routing/ext_ip
metadata_rules:
  op: is
  value: true
  path: country/is_in_european_union
```

Step-by-step, this rule will do the following:

* Upon seeing a `CONNECTED` event, retrieve the `routing/ext_ip` value and send it to MaxMind via the `api/ip-geo` resource
* Upon receiving a response from `api/ip-geo`, evaluate it using `metadata_rules` to see if the country associated with the IP is located in the EU

The format of the metadata returned is documented [here](https://github.com/maxmind/MaxMind-DB-Reader-python) and looks like this:

```
{
  "country": {
    "geoname_id": 2750405,
    "iso_code": "NL",
    "is_in_european_union": true,
    "names": {
      "ru": "\u041d\u0438\u0434\u0435\u0440\u043b\u0430\u043d\u0434\u044b",
      "fr": "Pays-Bas",
      "en": "Netherlands",
      "de": "Niederlande",
      "zh-CN": "\u8377\u5170",
      "pt-BR": "Holanda",
      "ja": "\u30aa\u30e9\u30f3\u30c0\u738b\u56fd",
      "es": "Holanda"
    }
  },
  "location": {
    "latitude": 52.3824,
    "accuracy_radius": 100,
    "time_zone": "Europe/Amsterdam",
    "longitude": 4.8995
  },
  "continent": {
    "geoname_id": 6255148,
    "code": "EU",
    "names": {
      "ru": "\u0415\u0432\u0440\u043e\u043f\u0430",
      "fr": "Europe",
      "en": "Europe",
      "de": "Europa",
      "zh-CN": "\u6b27\u6d32",
      "pt-BR": "Europa",
      "ja": "\u30e8\u30fc\u30ed\u30c3\u30d1",
      "es": "Europa"
    }
  },
  "registered_country": {
    "geoname_id": 2750405,
    "iso_code": "NL",
    "is_in_european_union": true,
    "names": {
      "ru": "\u041d\u0438\u0434\u0435\u0440\u043b\u0430\u043d\u0434\u044b",
      "fr": "Pays-Bas",
      "en": "Netherlands",
      "de": "Niederlande",
      "zh-CN": "\u8377\u5170",
      "pt-BR": "Holanda",
      "ja": "\u30aa\u30e9\u30f3\u30c0\u738b\u56fd",
      "es": "Holanda"
    }
  }
}
```

The geolocation data comes from GeoLite2 data created by [MaxMind](http://www.maxmind.com).

---

# IP Geolocation

With the `ip-geo` [add-on](https://app.limacharlie.io/add-ons/detail/ip-geo) subscribed, it can be used as an API-based lookup.

```
event: CONNECTED
op: lookup
resource: hive://lookup/ip-geo
path: routing/ext_ip
metadata_rules:
  op: is
  value: true
  path: country/is_in_european_union
```

Step-by-step, this rule will do the following:

* Upon seeing a `CONNECTED` event, retrieve the `routing/ext_ip` value and send it to MaxMind via the `api/ip-geo` resource
* Upon receiving a response from `api/ip-geo`, evaluate it using `metadata_rules` to see if the country associated with the IP is located in the EU

The format of the metadata returned is documented [here](https://github.com/maxmind/MaxMind-DB-Reader-python) and looks like this:

```
{
  "country": {
    "geoname_id": 2750405,
    "iso_code": "NL",
    "is_in_european_union": true,
    "names": {
      "ru": "\u041d\u0438\u0434\u0435\u0440\u043b\u0430\u043d\u0434\u044b",
      "fr": "Pays-Bas",
      "en": "Netherlands",
      "de": "Niederlande",
      "zh-CN": "\u8377\u5170",
      "pt-BR": "Holanda",
      "ja": "\u30aa\u30e9\u30f3\u30c0\u738b\u56fd",
      "es": "Holanda"
    }
  },
  "location": {
    "latitude": 52.3824,
    "accuracy_radius": 100,
    "time_zone": "Europe/Amsterdam",
    "longitude": 4.8995
  },
  "continent": {
    "geoname_id": 6255148,
    "code": "EU",
    "names": {
      "ru": "\u0415\u0432\u0440\u043e\u043f\u0430",
      "fr": "Europe",
      "en": "Europe",
      "de": "Europa",
      "zh-CN": "\u6b27\u6d32",
      "pt-BR": "Europa",
      "ja": "\u30e8\u30fc\u30ed\u30c3\u30d1",
      "es": "Europa"
    }
  },
  "registered_country": {
    "geoname_id": 2750405,
    "iso_code": "NL",
    "is_in_european_union": true,
    "names": {
      "ru": "\u041d\u0438\u0434\u0435\u0440\u043b\u0430\u043d\u0434\u044b",
      "fr": "Pays-Bas",
      "en": "Netherlands",
      "de": "Niederlande",
      "zh-CN": "\u8377\u5170",
      "pt-BR": "Holanda",
      "ja": "\u30aa\u30e9\u30f3\u30c0\u738b\u56fd",
      "es": "Holanda"
    }
  }
}
```

The geolocation data comes from GeoLite2 data created by [MaxMind](http://www.maxmind.com).

---

# IT Glue

## Overview

This Adapter allows you to connect to IT Glue to fetch activity logs.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `itglue`

* `token`: your API key/token for IT Glue

### Infrastructure as Code Deployment

```
# Adapter Documentation: https://docs.limacharlie.io/docs/adapter-types
# For Cloud Sensor configurations, use:
#        token: "hive://secret/itglue-api-token"

sensor_type: "itglue"
itglue:
  token: "hive://secret/itglue-api-token"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_ITGLUE"
    hostname: "itglue-adapter"
    platform: "json"
    sensor_seed_key: "itglue-audit-sensor"
    mapping:
      sensor_hostname_path: "attributes.resource_name"
      event_type_path: "attributes.action"
      event_time_path: "attributes.created_at"
    indexing: []
```

## API Doc

See the official [documentation](https://api.itglue.com/developer/).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# IT Glue

## Overview

This Adapter allows you to connect to IT Glue to fetch activity logs.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `itglue`

* `token`: your API key/token for IT Glue

### Infrastructure as Code Deployment

```
# Adapter Documentation: https://docs.limacharlie.io/docs/adapter-types
# For Cloud Sensor configurations, use:
#        token: "hive://secret/itglue-api-token"

sensor_type: "itglue"
itglue:
  token: "hive://secret/itglue-api-token"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_ITGLUE"
    hostname: "itglue-adapter"
    platform: "json"
    sensor_seed_key: "itglue-audit-sensor"
    mapping:
      sensor_hostname_path: "attributes.resource_name"
      event_type_path: "attributes.action"
      event_time_path: "attributes.created_at"
    indexing: []
```

## API Doc

See the official [documentation](https://api.itglue.com/developer/).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Adapter

The adapter service is a core component of LimaCharlie that allows you to ingest data from various external sources and transform it into telemetry that can be processed by the platform. Adapters act as data connectors that can pull from or listen to different services, normalize the data, and forward it to your LimaCharlie organization.

## Overview

Adapters provide a flexible way to integrate third-party services, cloud platforms, and custom data sources into LimaCharlie. Each adapter type is designed to handle specific data sources and protocols, making it easy to centralize security telemetry from across your infrastructure.

## Supported Adapter Types

LimaCharlie supports a wide variety of adapter types for different data sources:

### Cloud & Infrastructure
- **AWS S3** - Ingest data from S3 buckets
- **AWS SQS** - Consume messages from SQS queues
- **Google Cloud Storage** - Pull data from GCS buckets
- **Google Cloud Pub/Sub** - Subscribe to Pub/Sub topics

### Email & Communication
- **IMAP** - Monitor email accounts via IMAP
- **Mimecast** - Ingest Mimecast email security logs
- **Sublime Security** - Integrate Sublime email detection platform

### Identity & Access Management
- **Okta** - Collect Okta system logs and events
- **1Password** - Monitor 1Password events
- **Microsoft 365** - Ingest M365 audit logs

### Security Tools
- **Microsoft Defender** - Collect Defender alerts and events
- **Sophos** - Ingest Sophos endpoint and firewall logs
- **PandaDoc** - Monitor document activity

### IT Management
- **IT Glue** - Sync IT documentation and asset data

### Generic Ingestion
- **File** - Monitor and ingest from file sources
- **Syslog** - Receive syslog messages
- **Stdin** - Ingest data from standard input
- **Stdin JSON** - Ingest JSON-formatted data from standard input
- **EVTX** - Parse Windows Event Log files

## Key Concepts

### Data Transformation
Adapters normalize data from various sources into LimaCharlie's standardized telemetry format, making it easier to write detection rules and perform analysis across different data sources.

### Flexible Deployment
Adapters can run in different environments:
- Cloud-hosted by LimaCharlie
- Self-hosted on your infrastructure
- As containerized services

### Configuration
Each adapter type has its own configuration schema that defines:
- Authentication credentials
- Data source location
- Transformation rules
- Output destinations
- Filtering and sampling options

## Getting Started

To start using adapters:

1. Choose the appropriate adapter type for your data source
2. Configure the adapter with necessary credentials and settings
3. Deploy the adapter (cloud or self-hosted)
4. Verify data is flowing into your LimaCharlie organization

## Related Documentation

- [Adapter Usage](/docs/en/adapter-usage) - How to configure and deploy adapters
- Individual adapter type documentation (see links above for specific adapters)

---

# Config Hive: Secrets

Secrets are encrypted key-value pairs that can be used in Detection & Response rules and other LimaCharlie components. They provide a secure way to store sensitive information like API keys, passwords, and tokens.

## Overview

Secrets are stored encrypted at rest and are only decrypted when needed during rule execution. This ensures sensitive data is never exposed in logs or rule definitions.

## Managing Secrets

### Creating Secrets

Secrets can be created through the web interface or API:

1. Navigate to **Config Hive**  **Secrets**
2. Click **Add Secret**
3. Provide a name and value
4. Save the secret

### Using Secrets in Rules

Reference secrets in Detection & Response rules using the following syntax:

```
{{ secret://secret-name }}
```

**Example:**

```yaml
- action: report
  metadata:
    api_key: "{{ secret://my-api-key }}"
```

### API Management

Secrets can be managed via the LimaCharlie API:

```bash
# Create a secret
limacharlie secret set my-secret "secret-value"

# List secrets (values are not displayed)
limacharlie secret list

# Delete a secret
limacharlie secret del my-secret
```

## Best Practices

1. **Unique Names**: Use descriptive, unique names for secrets
2. **Rotation**: Regularly rotate sensitive secrets
3. **Least Privilege**: Only grant access to secrets that are needed
4. **Audit**: Monitor secret usage through audit logs

## Security Considerations

- Secrets are encrypted at rest using industry-standard encryption
- Secret values are never logged or displayed after creation
- Access to secrets is controlled by organization permissions
- Secret usage is audited and can be tracked

---

# Config Hive: Lookups

Lookups are reference tables that can be queried during rule execution. They allow you to maintain lists of indicators, configurations, or other data that can be checked dynamically.

## Overview

Lookups provide a way to:
- Store lists of known indicators (IPs, domains, hashes)
- Maintain configuration values
- Create dynamic reference tables
- Perform real-time lookups during detection

## Types of Lookups

### Static Lookups

Static lookups are key-value pairs stored directly in LimaCharlie:

```yaml
key: value
ip-address: 192.168.1.1
domain: example.com
```

### Dynamic Lookups

Dynamic lookups query external sources in real-time (requires API integration).

## Managing Lookups

### Creating Lookups

1. Navigate to **Config Hive**  **Lookups**
2. Click **Add Lookup**
3. Provide a name and add key-value pairs
4. Save the lookup table

### Using Lookups in Rules

Reference lookups in Detection & Response rules:

```yaml
- action: report
  lookup:
    path: lookup://threat-ips
    key: "{{ .event.IP_ADDRESS }}"
```

**Example Rule:**

```yaml
event: NETWORK_CONNECTIONS
op: lookup
path: lookup://blocked-ips
key: "{{ .event.IP_ADDRESS }}"
action: report
metadata:
  detection: "Connection to blocked IP"
```

### API Management

```bash
# Create a lookup
limacharlie lookup set threat-ips 192.168.1.1 malicious

# Query a lookup
limacharlie lookup get threat-ips 192.168.1.1

# Delete a lookup entry
limacharlie lookup del threat-ips 192.168.1.1
```

## Use Cases

1. **Threat Intelligence**: Maintain lists of malicious indicators
2. **Allow Lists**: Track known-good entities
3. **Configuration**: Store environment-specific settings
4. **Enrichment**: Add context to events

## Best Practices

- Keep lookup tables focused and organized
- Regularly update lookup data
- Use descriptive names and keys
- Monitor lookup performance for large tables

---

# API Keys

API keys provide programmatic access to the LimaCharlie platform. They enable automation, integration, and custom tooling.

## Overview

API keys authenticate requests to the LimaCharlie API and can be scoped with specific permissions.

## Creating API Keys

1. Navigate to **Access**  **API Keys**
2. Click **Create API Key**
3. Provide a name and description
4. Select permissions (scopes)
5. Save and securely store the key

**Important**: API keys are only displayed once during creation. Store them securely.

## Permission Scopes

API keys can be granted various permission levels:

- **Read**: View organization data
- **Write**: Modify configurations
- **Admin**: Full administrative access
- **Sensor**: Sensor-specific operations
- **Custom**: Granular permission selection

## Using API Keys

### Authentication

Include the API key in the Authorization header:

```bash
curl -H "Authorization: Bearer YOUR_API_KEY" \
  https://api.limacharlie.io/v1/...
```

### SDK Usage

```python
from limacharlie import Manager

manager = Manager(api_key="YOUR_API_KEY")
```

## Security Best Practices

1. **Least Privilege**: Grant minimum necessary permissions
2. **Rotation**: Regularly rotate API keys
3. **Storage**: Never commit keys to source control
4. **Monitoring**: Track API key usage and audit logs
5. **Revocation**: Immediately revoke compromised keys

## API Key Management

### Listing Keys

View all API keys in your organization (values are masked).

### Revoking Keys

Immediately revoke compromised or unused keys.

### Auditing

Monitor API key usage through audit logs to detect unauthorized access.

---

# LimaCharlie SDK & CLI

The LimaCharlie SDK and CLI provide programmatic and command-line access to the platform.

## SDK (Python)

### Installation

```bash
pip install limacharlie
```

### Basic Usage

```python
from limacharlie import Manager

# Initialize manager
manager = Manager(api_key="YOUR_API_KEY")

# Get organization
org = manager.organization("your-org-id")

# List sensors
sensors = org.sensors()
for sensor in sensors:
    print(sensor.sid, sensor.hostname)

# Create D&R rule
rule = {
    "name": "example-rule",
    "detect": {...},
    "respond": [...]
}
org.dr_rule_create(rule)
```

### Common Operations

```python
# Sensor management
sensor = org.sensor("sensor-id")
sensor.task("os_version")

# Event queries
events = org.get_events(event_type="NETWORK_CONNECTIONS", limit=100)

# Outputs
outputs = org.outputs()
```

## CLI

### Installation

```bash
pip install limacharlie
```

### Configuration

```bash
# Configure credentials
limacharlie configure
```

### Common Commands

```bash
# List sensors
limacharlie sensor list

# Get sensor info
limacharlie sensor info SENSOR_ID

# Task a sensor
limacharlie sensor task SENSOR_ID os_version

# Manage D&R rules
limacharlie dr list
limacharlie dr add rule.yaml
limacharlie dr del rule-name

# Secrets management
limacharlie secret set my-secret "value"
limacharlie secret list

# Lookup management
limacharlie lookup set table-name key value
```

## Documentation

- [Python SDK Documentation](https://github.com/refractionPOINT/python-limacharlie)
- [API Reference](https://doc.limacharlie.io/docs/api)

---

# Secure Annex

Secure Annex provides encrypted storage for sensitive files and data within LimaCharlie.

## Overview

Secure Annex allows you to:
- Store sensitive files securely
- Reference files in Detection & Response rules
- Manage encrypted artifacts
- Maintain compliance with data security requirements

## Features

- **Encryption**: All data encrypted at rest
- **Access Control**: Granular permissions
- **Versioning**: Track file changes
- **Integration**: Use in rules and automations

## Usage

### Uploading Files

1. Navigate to **Secure Annex**
2. Click **Upload File**
3. Select file and provide metadata
4. Save

### Referencing in Rules

```yaml
- action: report
  file: annex://sensitive-config.json
```

### API Access

```python
# Upload file
org.annex_upload("file.txt", content)

# Download file
content = org.annex_download("file.txt")

# List files
files = org.annex_list()
```

## Best Practices

1. **Encryption**: Store only sensitive data in Annex
2. **Access Control**: Limit access to necessary personnel
3. **Versioning**: Track changes for audit purposes
4. **Cleanup**: Remove obsolete files regularly

---

# Config Hive: Yara

YARA rules can be used in LimaCharlie for file and memory scanning to detect malware and suspicious patterns.

## Overview

LimaCharlie supports YARA rules for:
- File scanning
- Memory scanning
- Process analysis
- Artifact detection

## Managing YARA Rules

### Adding YARA Rules

1. Navigate to **Config Hive**  **YARA**
2. Click **Add Rule**
3. Paste your YARA rule
4. Save and enable

### YARA Rule Format

```yara
rule ExampleRule {
    meta:
        description = "Example YARA rule"
        author = "Security Team"
    strings:
        $str1 = "malicious_string"
        $str2 = { 6D 61 6C 77 61 72 65 }
    condition:
        any of them
}
```

## Using YARA in Rules

### File Scanning

```yaml
event: NEW_DOCUMENT
op: yara_scan
path: "{{ .event.FILE_PATH }}"
ruleset: custom-rules
action: report
```

### Memory Scanning

```yaml
event: NEW_PROCESS
op: yara_scan_process
pid: "{{ .event.PROCESS_ID }}"
ruleset: memory-rules
action: isolate
```

## Best Practices

1. **Testing**: Test rules before deployment
2. **Performance**: Optimize rules to avoid performance impact
3. **Maintenance**: Regularly update rule sets
4. **Documentation**: Document rule purpose and logic

## Integration with Detection

YARA results can trigger Detection & Response rules:

```yaml
event: YARA_DETECTION
target: artifact
op: is
value: suspicious-file
action: report
```

---

# Config Hive: Detection & Response Rules

Detection & Response (D&R) rules are the core of LimaCharlie's detection engine. They define how to detect threats and respond automatically.

## Overview

D&R rules consist of:
- **Detect**: Conditions that trigger the rule
- **Respond**: Actions to take when triggered

## Rule Structure

```yaml
name: example-rule
detect:
  event: NETWORK_CONNECTIONS
  op: and
  rules:
    - op: is
      path: event/NETWORK_ACTIVITY/DESTINATION/IP_ADDRESS
      value: 192.168.1.1
respond:
  - action: report
    metadata:
      detection: "Connection to suspicious IP"
  - action: isolate
```

## Common Operators

- `is`: Exact match
- `contains`: Substring match
- `starts with`: Prefix match
- `ends with`: Suffix match
- `matches`: Regex match
- `greater than`: Numeric comparison
- `less than`: Numeric comparison

## Common Actions

- `report`: Create detection alert
- `isolate`: Network isolate sensor
- `task`: Execute sensor task
- `tag`: Add sensor tag
- `webhook`: Send webhook notification

## Example Rules

### Detect Suspicious Process

```yaml
name: suspicious-process
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: contains
      path: event/FILE_PATH
      value: "powershell.exe"
    - op: contains
      path: event/COMMAND_LINE
      value: "downloadstring"
respond:
  - action: report
    metadata:
      detection: "Suspicious PowerShell execution"
```

### Detect Network Connection

```yaml
name: suspicious-connection
detect:
  event: NETWORK_CONNECTIONS
  op: lookup
  path: lookup://threat-ips
  key: "{{ .event.DESTINATION.IP_ADDRESS }}"
respond:
  - action: report
  - action: isolate
```

## Best Practices

1. **Testing**: Test in report-only mode first
2. **Documentation**: Comment rule purpose
3. **Tuning**: Iterate to reduce false positives
4. **Performance**: Keep rules efficient
5. **Organization**: Use naming conventions

---

# Config Hive: Cloud Sensors

Cloud Sensors extend LimaCharlie's visibility to cloud environments and SaaS applications.

## Overview

Cloud Sensors provide:
- Cloud infrastructure monitoring (AWS, Azure, GCP)
- SaaS application logging (O365, GSuite)
- Cloud storage monitoring
- Identity and access management visibility

## Supported Platforms

### AWS
- CloudTrail logs
- VPC Flow Logs
- GuardDuty findings
- S3 access logs

### Azure
- Activity logs
- Network Security Group logs
- Security Center alerts

### GCP
- Cloud Audit logs
- VPC Flow Logs
- Security Command Center findings

### SaaS
- Office 365
- Google Workspace
- Okta
- Other via API

## Configuration

### AWS Setup

1. Navigate to **Config Hive**  **Cloud Sensors**
2. Select **AWS**
3. Provide IAM credentials or role ARN
4. Select log sources
5. Save configuration

```json
{
  "platform": "aws",
  "role_arn": "arn:aws:iam::123456789:role/LimaCharlie",
  "log_sources": ["cloudtrail", "vpc_flow"]
}
```

### Azure Setup

```json
{
  "platform": "azure",
  "tenant_id": "...",
  "client_id": "...",
  "client_secret": "{{ secret://azure-secret }}",
  "log_sources": ["activity", "nsg_flow"]
}
```

## Using Cloud Data in Rules

```yaml
event: AWS_CLOUDTRAIL
detect:
  op: is
  path: event/eventName
  value: "DeleteBucket"
respond:
  - action: report
    metadata:
      detection: "AWS S3 bucket deleted"
```

## Best Practices

1. **Least Privilege**: Use minimal required permissions
2. **Cost Management**: Monitor ingestion costs
3. **Filtering**: Ingest only necessary logs
4. **Integration**: Correlate cloud and endpoint events

---

# API

## Config Hive: Secrets

Config Hive Secrets provide a secure way to store and manage sensitive information like API keys, passwords, and tokens within LimaCharlie. These secrets can be referenced in Detection & Response rules, integrations, and other configurations without exposing the actual values.

### Creating Secrets

Secrets are stored in the Config Hive and can be created through:
- The web interface under Config Hive > Secrets
- The LimaCharlie SDK/CLI
- Direct API calls

### Using Secrets

Reference secrets in your configurations using the syntax:
```
{{ secret.SECRET_NAME }}
```

Secrets are resolved at runtime and never exposed in logs or rule outputs.

### Best Practices

- Use descriptive names for secrets
- Rotate secrets regularly
- Limit access using RBAC controls
- Never commit secrets to version control

## Config Hive: Lookups

Lookups are key-value stores that enable you to maintain lists of indicators, configuration data, or reference information that can be queried during detection and response operations.

### Common Use Cases

- IP allowlists/denylists
- Known good/bad hashes
- User account mappings
- Custom threat intelligence feeds
- Configuration parameters

### Creating Lookups

Lookups can be created and managed through:
- Web interface: Config Hive > Lookups
- LimaCharlie SDK/CLI
- API endpoints

### Querying Lookups

Use lookups in D&R rules with the `lookup()` function:

```yaml
op: lookup
path: event/IP_ADDRESS
resource: 'lcr://lookup/LOOKUP_NAME'
```

### Lookup Operations

- **Add/Update**: Insert or modify entries
- **Delete**: Remove entries
- **Bulk Import**: Upload CSV or JSON data
- **TTL**: Set expiration times for entries

## API Keys

LimaCharlie uses API keys for programmatic access to the platform. API keys enable automation, integration with external tools, and SDK/CLI usage.

### Creating API Keys

1. Navigate to Organization Settings > API Keys
2. Click "Create API Key"
3. Set permissions and expiration
4. Save the key securely (shown only once)

### API Key Types

- **User Keys**: Associated with a specific user account
- **Service Keys**: For automated systems and integrations
- **Installation Keys**: For sensor deployment

### Permissions

API keys can be scoped with specific permissions:
- Read-only access
- Write access to specific resources
- Full administrative access
- Custom permission sets

### Security Best Practices

- Use service keys for automation
- Set expiration dates
- Rotate keys regularly
- Use minimum required permissions
- Store keys securely (environment variables, secrets managers)
- Never commit keys to version control

### Using API Keys

Include the API key in request headers:

```bash
curl -H "Authorization: Bearer YOUR_API_KEY" \
  https://api.limacharlie.io/v1/...
```

Or use with the SDK:

```python
from limacharlie import Manager

manager = Manager(api_key="YOUR_API_KEY", oid="YOUR_ORG_ID")
```

## LimaCharlie SDK & CLI

The LimaCharlie SDK and CLI provide programmatic access to the platform for automation, integration, and advanced operations.

### Installation

**Python SDK:**
```bash
pip install limacharlie
```

**CLI:**
```bash
pip install limacharlie-cli
```

### SDK Usage

```python
from limacharlie import Manager

# Initialize
manager = Manager(api_key="YOUR_API_KEY", oid="YOUR_ORG_ID")

# List sensors
sensors = manager.sensors()

# Get specific sensor
sensor = manager.sensor("SENSOR_ID")

# Task sensor
sensor.task("history_dump_recent", {"hours": 24})
```

### CLI Usage

**Authentication:**
```bash
# Set credentials
limacharlie login --api-key YOUR_API_KEY --oid YOUR_ORG_ID

# Or use environment variables
export LC_API_KEY=YOUR_API_KEY
export LC_OID=YOUR_ORG_ID
```

**Common Commands:**
```bash
# List sensors
limacharlie sensors list

# Get sensor details
limacharlie sensor get SENSOR_ID

# Task sensor
limacharlie sensor task SENSOR_ID history_dump_recent

# Deploy D&R rules
limacharlie dr push rules.yaml

# Query events
limacharlie search "event_type:NEW_PROCESS" --days 1
```

### Configuration Management

```bash
# Export configuration
limacharlie hive export config.yaml

# Import configuration
limacharlie hive import config.yaml

# Sync configurations
limacharlie hive sync
```

### Advanced Features

- Batch sensor operations
- Event streaming
- Automated response workflows
- Integration with CI/CD pipelines
- Custom automation scripts

### Documentation

Full SDK documentation: [https://doc.limacharlie.io](https://doc.limacharlie.io)

## Secure Annex

Secure Annex is LimaCharlie's encrypted cloud storage service for sensitive data and artifacts collected from endpoints.

### Features

- **Encrypted Storage**: End-to-end encryption
- **Artifact Collection**: Store files, memory dumps, forensic data
- **Retention Policies**: Automatic expiration
- **Access Control**: RBAC integration
- **Audit Logging**: Complete access history

### Collecting Artifacts

Task sensors to upload artifacts to Secure Annex:

```yaml
- action: artifact_get
  artifact: path/to/file
  upload_to_annex: true
```

Via SDK:
```python
sensor.task("artifact_get", {
    "artifact": "/path/to/file",
    "upload_to_annex": True
})
```

### Retrieving Artifacts

```python
# List artifacts
artifacts = manager.annexes()

# Download artifact
artifact = manager.annex("ARTIFACT_ID")
data = artifact.download()
```

Via CLI:
```bash
limacharlie annex list
limacharlie annex download ARTIFACT_ID
```

### Retention Policies

Set automatic expiration:
- 24 hours
- 7 days
- 30 days
- 90 days
- Custom duration

### Use Cases

- Memory dump analysis
- Suspicious file collection
- Forensic investigations
- Malware sample collection
- Log file preservation

## Config Hive: Yara

Config Hive Yara enables you to deploy and manage Yara rules for file and memory scanning across your fleet.

### Creating Yara Rules

Store Yara rules in Config Hive for centralized management:

```yara
rule SuspiciousPowerShell {
    meta:
        description = "Detects suspicious PowerShell patterns"
        author = "Security Team"
    
    strings:
        $s1 = "IEX" nocase
        $s2 = "DownloadString" nocase
        $s3 = "-EncodedCommand"
    
    condition:
        2 of them
}
```

### Deploying Yara Rules

**Via Web Interface:**
1. Navigate to Config Hive > Yara
2. Create or upload rule
3. Apply to organization or specific tags

**Via SDK:**
```python
manager.add_yara_rule("rule_name", yara_source)
```

**Via CLI:**
```bash
limacharlie hive yara add rule_name rule_file.yara
```

### Scanning Operations

**File Scanning:**
```python
sensor.task("yara_scan", {
    "file_path": "C:\\suspect.exe",
    "rules": ["rule_name"]
})
```

**Memory Scanning:**
```python
sensor.task("yara_scan", {
    "pid": 1234,
    "rules": ["rule_name"]
})
```

### Automated Scanning

Trigger Yara scans automatically in D&R rules:

```yaml
detect:
  event: NEW_DOCUMENT
  op: ends with
  path: event/FILE_PATH
  value: .exe

respond:
  - action: task
    command: yara_scan
    investigation: true
    file_path: "{{ event.FILE_PATH }}"
```

### Performance Considerations

- Use targeted rules to minimize CPU impact
- Limit concurrent scans
- Scan specific paths rather than entire filesystems
- Use rule namespaces for organization

## Config Hive: Detection & Response Rules

Detection & Response (D&R) rules are the core of LimaCharlie's automated threat detection and response capabilities.

### Rule Structure

```yaml
detect:
  # Detection logic
  event: EVENT_TYPE
  op: is
  path: event/FIELD
  value: TARGET_VALUE

respond:
  # Response actions
  - action: report
    name: rule_name
  - action: task
    command: deny_tree
```

### Detection Operators

- `is` / `is not`: Exact match
- `contains` / `not contains`: Substring match
- `starts with` / `ends with`: Prefix/suffix match
- `matches` / `not matches`: Regex match
- `exists`: Field presence check
- `greater than` / `less than`: Numeric comparison
- `lookup`: Query Config Hive lookup

### Common Event Types

- `NEW_PROCESS`: Process creation
- `NETWORK_CONNECTIONS`: Network activity
- `DNS_REQUEST`: DNS queries
- `FILE_CREATE` / `FILE_DELETE`: File operations
- `REGISTRY_CREATE` / `REGISTRY_DELETE`: Registry changes
- `NEW_DOCUMENT`: Document downloads
- `USER_OBSERVED`: User activity

### Response Actions

- `report`: Generate detection alert
- `task`: Execute sensor command
- `add tag`: Tag the sensor
- `remove tag`: Remove sensor tag
- `isolate`: Network isolate sensor
- `webhook`: Send to external system

### Example Rules

**Detect Suspicious PowerShell:**
```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: contains
      path: event/COMMAND_LINE
      value: powershell
    - op: contains
      path: event/COMMAND_LINE
      value: "-EncodedCommand"

respond:
  - action: report
    name: suspicious_powershell
  - action: task
    command: deny_tree
```

**Detect Lateral Movement:**
```yaml
detect:
  event: NEW_PROCESS
  op: and
  rules:
    - op: is
      path: event/FILE_PATH
      value: psexec.exe
    - op: is
      path: event/PARENT/FILE_PATH
      value: services.exe

respond:
  - action: report
    name: lateral_movement_detected
  - action: isolate
```

### Rule Management

**Import/Export:**
```bash
limacharlie dr export rules.yaml
limacharlie dr import rules.yaml
```

**Version Control:**
Store D&R rules in Git and deploy via CI/CD:
```bash
limacharlie dr push rules/ --org-name prod
```

### Testing Rules

Use the D&R rule tester in the web interface to validate rules against historical events before deployment.

## Config Hive: Cloud Sensors

Cloud Sensors enable monitoring of cloud infrastructure and services including AWS, Azure, GCP, and other cloud platforms.

### Supported Platforms

- **AWS**: CloudTrail, GuardDuty, S3 access logs
- **Azure**: Activity logs, Security Center
- **GCP**: Cloud Logging, Security Command Center
- **Office 365**: Audit logs
- **Okta**: System logs

### Setting Up Cloud Sensors

**AWS CloudTrail:**
1. Navigate to Config Hive > Cloud Sensors
2. Select AWS CloudTrail
3. Provide IAM credentials or assume role ARN
4. Configure S3 bucket and SNS topic
5. Apply to organization

**Azure:**
1. Select Azure integration
2. Configure Azure AD application
3. Grant required permissions
4. Configure Event Hub connection

### Cloud Event Detection

D&R rules can detect cloud events:

```yaml
detect:
  event: CLOUD_AWS
  op: and
  rules:
    - op: is
      path: event/eventName
      value: ConsoleLogin
    - op: is
      path: event/responseElements/ConsoleLogin
      value: Failure

respond:
  - action: report
    name: aws_console_login_failure
  - action: webhook
    url: "{{ secret.SLACK_WEBHOOK }}"
```

### Common Use Cases

- Unauthorized access attempts
- Configuration changes
- Resource creation/deletion
- Permission modifications
- API abuse detection
- Compliance monitoring

### Best Practices

- Enable CloudTrail/logging in all regions
- Use dedicated IAM roles with minimal permissions
- Configure alerts for critical events
- Integrate with SIEM for correlation
- Regular review of cloud activity
- Automated response to policy violations

---

# LimaCharlie SDK & CLI

## Go

The Go library is a simple abstraction to the [LimaCharlie.io REST API](https://api.limacharlie.io/). The REST API currently supports many more functions. If it's missing a function available in the REST API that you would like to use, let us know at support@limacharlie.io.

* Repo - <https://github.com/refractionPOINT/go-limacharlie>

### Getting Started

#### Authentication

You can use Client Options to declare your client/org, or you can use environment variables.

**Using Environment Variables:**

* `LC_OID`: Organization ID
* `LC_API_KEY`: your LC API KEY
* `LC_UID`: optional, your user ID

```
package main

import (
	"fmt"

	"github.com/refractionPOINT/go-limacharlie/limacharlie"
)

func main() {
    client, err := limacharlie.NewClientFromLoader(limacharlie.ClientOptions{}, nil, &limacharlie.EnvironmentClientOptionLoader{})
    if err != nil {
        fmt.Println(err)
    }

    org, _ := limacharlie.NewOrganization(client)
    fmt.Printf("Hello, this is %s", org.GetOID())
}
```

**Using Client Options:**

```
package main

import (
	"fmt"

	"github.com/refractionPOINT/go-limacharlie/limacharlie"
)

func main() {
    clientOptions = limacharlie.ClientOptions{
        OID: "MY_OID",
        APIKey: "MY_API_KEY",
        UID: "MY_UID",
    }
    org, _ := limacharlie.NewOrganizationFromClientOptions(clientOptions, nil)
    fmt.Printf("Hello, this is %s", org.GetOID())
}
```

### SDK

#### Examples

```
package main

import (
	"fmt"

	"github.com/refractionPOINT/go-limacharlie/limacharlie"
)

func main() {
    client, err := limacharlie.NewClientFromLoader(limacharlie.ClientOptions{}, nil, &limacharlie.EnvironmentClientOptionLoader{})
    if err != nil {
        fmt.Println(err)
    }

    org, _ := limacharlie.NewOrganization(client)

    // List all sensors
    sensors, err := org.ListSensors()
    if err != nil {
        fmt.Println(err)
    }
    for sid, sensor := range sensors {
        fmt.Printf("%s - %s", sid, sensor.Hostname)
    }

    // List D&R rules from Hive
    hiveClient := limacharlie.NewHiveClient(org)
    rules, _ := hiveClient.List(limacharlie.HiveArgs{
        HiveName:     "dr-general",
        PartitionKey:  org.GetOID(),
    })
    for rule_name, _ := range rules {
        fmt.Println(rule_name)
    }

    // Add D&R rule to Hive
    enabled := true
    case_sensitive := false
    if _, err := hiveClient.Add(limacharlie.HiveArgs{
        HiveName:     "dr-general",
        PartitionKey: org.GetOID(),
        Key:          "test_rule_name",
        Enabled:      &enabled,
        Data: limacharlie.Dict{
            "detect": limacharlie.Dict{
                "event":            "NEW_PROCESS",
                "op":               "is",
                "path":             "event/COMMAND_LINE",
                "value":            "whoami",
                "case sensitive":   &case_sensitive,
            },
            "respond": []limacharlie.Dict{{
                "action": "report",
                "name":   "whoami detection",
            }},
        },
    }); err != nil {
        fmt.Println(err)
    }

    // List extensions
    extensions, _ := org.Extensions()
    for _, extension_name := range extensions {
        fmt.Println(extension_name)
    }

    // Subscribe to extension
    subscription_request := org.SubscribeToExtension("binlib")
    if subscription_request != nil {
        fmt.Println(subscription_request)
    }

    // List payloads
    payloads, _ := org.Payloads()
    for payload, _ := range payloads {
        fmt.Println(payload)
    }

    // List installation keys
    installation_keys, _ := org.InstallationKeys()
    for _, key := range installation_keys {
        fmt.Println(key.Description)
    }

    // Create installation key
    key_request, _ := org.AddInstallationKey(InstallationKey{
		Description: "my-test-key",
		Tags:        []string{"tag", "another-tag"},
	})

}
```

## Python

The Python library is a simple abstraction to the [LimaCharlie.io REST API](https://api.limacharlie.io/). The REST API currently supports many more functions. If it's missing a function available in the REST API that you would like to use, let us know at support@limacharlie.io.

* Repo - <https://github.com/refractionpoint/python-limacharlie>

### Getting Started

#### Installing

##### PyPi (pip)

The library and the CLI is available as a Python package on PyPi (<https://pypi.org/project/limacharlie/>). It can be installed using pip as shown below.

```
pip install limacharlie
```

##### Docker Image

In addition to the PyPi distribution we also offer a pre-built Docker image on DockerHub (<https://hub.docker.com/r/refractionpoint/limacharlie>).

```
docker run refractionpoint/limacharlie:latest whoami

# Using a specific version (Docker image tag matches the library version)
docker run refractionpoint/limacharlie:4.9.13 whoami

# If you already have a credential file locally, you can mount it inside the Docker container
docker run -v ${HOME}/.limacharlie:/root/.limacharlie:ro refractionpoint/limacharlie:latest whoami
```

#### Credentials

Authenticating to use the SDK / CLI can be done in a few ways.

**Option 1 - Logging In**
The simplest is to login to an Organization using an [API key](https://doc.limacharlie.io/docs/documentation/docs/api_keys.md).

Use `limacharlie login` to store credentials locally. You will need an `OID` (Organization ID) and an API key, and (optionally) a `UID` (User ID), all of which you can get from the Access Management --> REST API section of the web interface.

The login interface supports named environments, or a default one used when no environment is selected.

To list available environments:

```
limacharlie use
```

Setting a given environment in the current shell session can be done like this:

```
limacharlie use my-dev-org
```

You can also specify a `UID` (User ID) during login to use a *user* API key representing the total set of permissions that user has (see User Profile in the web interface).

**Option 2 - Environment Variables**
You can use the `LC_OID` and `LC_API_KEY` and `LC_UID` environment variables to replace the values used logging in. The environment variables will be used if no other credentials are specified.

### SDK

The root of the functionality in the SDK is from the `Manager` object. It holds the credentials and is tied to a specific LimaCharlie Organization.

You can authenticate the `Manager` using an `oid` (and optionally a `uid`), along with either a `secret_api_key` or `jwt` directly. Alternatively you can just use an environment name (as specified in `limacharlie login`). If no creds are provided, the `Manager` will try to use the default environment and credentials.

#### Importing

```
import limacharlie

YARA_SIG = 'https://raw.githubusercontent.com/Yara-Rules/rules/master/Malicious_Documents/Maldoc_PDF.yar'

# Create an instance of the SDK.
mgr = limacharlie.Manager()

# Get a list of all the sensors in the current Organization.
all_sensors = mgr.sensors()

# Select the first sensor in the list.
sensor = all_sensors[0]

# Tag this sensor with a tag for 10 minutes.
sensor.tag( 'suspicious', ttl = 60 * 10 )

# Send a task to the sensor (unidirectionally, not expecting a response).
sensor.task( 'os_processes' )

# Send a yara scan to that sensor for processes "evil.exe".
sensor.task( 'yara_scan -e *evil.exe ' + YARA_SIG )
```

#### Use of gevent

Note that the SDK uses the `gevent` package which sometimes has issues with other packages that operate at a low level in python. For example, Jupyter notebooks may see freezing on importing `limacharlie` and require a tweak to load:

```
{
 "display_name": "IPython 2 w/gevent",
 "language": "python",
 "argv": [
  "python",
  "-c", "from gevent.monkey import patch_all; patch_all(thread=False); from ipykernel.kernelapp import main; main()",
  "-f",
  "{connection_file}"
 ]
}
```

### Components

#### Manager

This is a the general component that provides access to the managing functions of the API like querying sensors online, creating and removing Outputs etc.

#### Firehose

The `Firehose` is a simple object that listens on a port for LimaCharlie.io data. Under the hood it creates a Syslog Output on limacharlie.io pointing to itself and removes it on shutdown. Data from limacharlie.io is added to `firehose.queue` (a `gevent Queue`) as it is received.

It is a basic building block of automation for limacharlie.io.

#### Spout

Much like the `Firehose`, the Spout receives data from LimaCharlie.io, the difference is that the `Spout` does not require opening a local port to listen actively on. Instead it leverages `stream.limacharlie.io` to receive the data stream over HTTPS.

A `Spout` is automatically created when you instantiate a `Manager` with the `is_interactive = True` and `inv_id = XXXX` arguments in order to provide real-time feedback from tasking sensors.

#### Sensor

This is the object returned by `manager.sensor( sensor_id )`.

It supports a `task`, `hostname`, `tag`, `untag`, `getTags` and more functions. This is the main way to interact with a specific sensor.

The `task` function sends a task to the sensor unidirectionally, meaning it does not receive the response from the sensor (if any). If you want to interact with a sensor in real-time, use the interactive mode (as mentioned in the `Spout`) and use either the `request` function to receive replies through a `FutureResults` object or the `simpleRequest` to wait for the response and receive it as a return value.

#### Artifacts

The `Artifacts` is a helpful class to upload [artifacts](/v2/docs/artifacts) to LimaCharlie without going through a sensor.

#### Extensions

The `Extensions` can be used to subscribe to and manage extensions within your org.

```
import limacharlie
from limacharlie import Extension

mgr = limacharlie.Manager()
ext = Extension(mgr)
ext.subscribe('binlib')
```

#### Payloads

The `Payloads` can be used to manage various executable [payloads](/v2/docs/payloads) accessible to sensors.

#### Replay

The `Replay` object allows you to interact with [Replay](/v2/docs/replay) jobs managed by LimaCharlie. These allow you to re-run [D&R Rules](/v2/docs/detection-and-response) on historical data.

Sample command line to query one sensor:

```
limacharlie-replay --sid 9cbed57a-6d6a-4af0-b881-803a99b177d9 --start 1556568500 --end 1556568600 --rule-content ./test_rule.txt
```

Sample command line to query an entire organization:

```
limacharlie-replay --entire-org --start 1555359000 --end 1556568600 --rule-name my-rule-name
```

#### Search

The `Search` object allows you to perform an IOC search across multiple organizations.

#### SpotCheck

The `SpotCheck` object (sometimes called Fleet Check) allows you to manage an active (query sensors directly as opposed to searching on indexed historical data) search for various IOCs on an organization's sensors.

#### Configs

The `Configs` is used to retrieve an organization's configuration as a config file, or apply an existing config file to an organization. This is the concept of Infrastructure as Code.

#### Webhook

The `Webhook` object demonstrates handling [webhooks emitted by the LimaCharlie cloud](/v2/docs/tutorial-creating-a-webhook-adapter), including verifying the shared-secret signing of the webhooks.

### Examples:

* [Basic Manager Operations](https://github.com/refractionPOINT/python-limacharlie/blob/master/samples/demo_manager.py)
* [Basic Firehose Operations](https://github.com/refractionPOINT/python-limacharlie/blob/master/samples/demo_firehose.py)
* [Basic Spout Operations](https://github.com/refractionPOINT/python-limacharlie/blob/master/samples/demo_spout.py)
* [Basic Integrated Operations](https://github.com/refractionPOINT/python-limacharlie/blob/master/samples/demo_interactive_sensor.py)
* [Sample Configs](https://github.com/refractionPOINT/python-limacharlie/tree/master/limacharlie/sample_configs)

### Command Line Interface

Many of the objects available as part of the LimaCharlie Python SDK also support various command line interfaces.

#### Query

[LimaCharlie Query Language (LCQL)](/v2/docs/lcql) provides a flexible, intuitive and interactive way to explore your data in LimaCharlie.

```
limacharlie query --help
```

#### ARLs

[Authenticated Resource Locators (ARLs)](/v2/docs/reference-authentication-resource-locator) describe a way to specify access to a remote resource, supporting many methods, including authentication data, and all that within a single string.

ARLs can be used in the [YARA manager](/v2/docs/ext-yara-manager) to import rules from GitHub repositories and other locations.

Testing an ARL before applying it somewhere can be helpful to shake out access or authentication errors beforehand. You can test an ARL and see what files are fetched, and their contents, by running the following command:

```
limacharlie get-arl -a [github,Yara-Rules/rules/email]
```

#### Firehose

Listens on interface `1.2.3.4`, port `9424` for incoming connections from LimaCharlie.io. Receives only events from hosts tagged with `fh_test`.

```
python -m limacharlie.Firehose 1.2.3.4:9424 event -n firehose_test -t fh_test --oid c82e5c17-d519-4ef5-a4ac-caa4a95d31ca
```

#### Spout

Behaves similarly to the Firehose, but instead of listening from an internet accessible port, it connects to the `stream.limacharlie.io` service to stream the output over HTTPS. This means the Spout allows you to get ad-hoc output like the Firehose, but it also works through NATs and proxies.

It is MUCH more convenient for short term ad-hoc outputs, but it is less reliable than a Firehose for very large amounts of data.

```
python -m limacharlie.Spout event --oid c82e5c17-d519-4ef5-a4ac-caa4a95d31ca
```

#### Configs

The `fetch` command will get a list of the Detection & Response rules in your organization and will write them to the config file specified or the default config file `lc_conf.yaml` in YAML format.

```
limacharlie configs fetch --oid c82e5c17-d519-4ef5-a4ac-c454a95d31ca`
```

Then `push` can upload the rules specified in the config file (or the default one) to your organization. The optional `--force` argument will remove active rules not found in the config file. The `--dry-run` simulates the sync and displays the changes that would occur.

The `--config` allows you to specify an alternate config file and the `--api-key` allows you to specify a file on disk where the API should be read from (otherwise, of if `-` is specified as a file, the API Key is read from STDIN).

```
limacharlie configs push --dry-run --oid c82e5c17-d519-4ef5-a4ac-c454a95d31ca --config /path/to/template.yaml --all --ignore-inaccessible
```

All these capabilities are also supported directly by the `limacharlie.Configs` object.

The Sync functionality currently supports all common useful configurations. The `--no-rules` and `--no-outputs` flags can be used to ignore one or the other in config files and sync. Additional flags are also supported, see `limacharlie configs --help`.

To understand better the config format, do a `fetch` from your organization. Notice the use of the `include` statement. Using this statement you can combine multiple config files together, making it ideal for the management of complex rule sets and their versioning.

#### Spot Checks

Used to perform Organization-wide checks for specific indicators of compromise. Available as a custom API `SpotCheck` object or as a module from the command line. Supports many types of IoCs like file names, directories, registry keys, file hashes and YARA signatures.

```
python -m limacharlie.SpotCheck --no-macos --no-linux --tags vip --file c:\\evil.exe`
```

For detailed usage:

```
python -m limacharlie.SpotCheck --help
```

#### Search

Shortcut utility to perform IOC searches across all locally configured organizations.

```
limacharlie search --help
```

#### Extensions

Shortcut utility to manage extensions.

```
limacharlie extension --help
```

#### Artifact Upload

Shortcut utility to upload and retrieve [Artifacts](/v2/docs/artifacts) within LimaCharlie with just the CLI (no agent).

```
limacharlie artifacts --help
```

#### Artifact Download

Shortcut utility to download [Artifact Collection](/v2/docs/artifacts) in LimaCharlie locally.

```
limacharlie artifacts get_original --help
```

#### Replay

Shortcut utility to perform [Replay](/v2/docs/replay) jobs from the CLI.

```
limacharlie replay --help
```

#### Detection & Response

Shortcut utility to manage Detection and Response rules over the CLI.

```
limacharlie dr --help
```

#### Events & Detections

Print out to STDOUT events or detections matching the parameter.

```
limacharlie events --help
limacharlie detections --help
```

#### List Sensors

Print out all basic sensor information for all sensors matching the [selector](/v2/docs/reference-sensor-selector-expressions).

```
limacharlie sensors --selector 'plat == windows'
```

#### Invite Users

Invite single or multiple users to LimaCharlie. Invited users will be sent an email to confirm their address, enable the account and create a new password.

Keep in mind that this actions operates in the user context which means you need to use user scoped API key. For more information on how to obtain one, see <https://docs.limacharlie.io/apidocs/introduction#getting-a-jwt>

Invite a single user:

```
limacharlie users invite --email=user1@example.com
```

Invite multiple users:

```
limacharlie users invite --email=user1@example.com,user2@example.com,user3@example.com
```

Invite multiple users from new line delimited entries in a text file:

```
cat users_to_invite.txt
user1@example.com
user2@example.com
user3@example.com
```

```
limacharlie users invite --file=users_to_invite.txt
```

---

# Mac Unified Logging

## Overview

This Adapter allows you to collect events from MacOS Unified Logging.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

**Optional Arguments:**

* `predicate`: example, `predicate='subsystem=="com.apple.TimeMachine"'`

## CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter mac_unified_logging client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME
```

### Infrastructure as Code Deployment

```
# macOS Unified Logging Specific Docs: https://docs.limacharlie.io/docs/adapter-types-macos-unified-logging

sensor_type: "mac_unified_logging"
  mac_unified_logging:
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_MACOSUL"
      hostname: "user-macbook-pro"
      platform: "mac_unified_logging"
      sensor_seed_key: "macos-unified-logging-sensor"
    # Optional configuration
    write_timeout_sec: 600                           # Default: 600 seconds
    predicate: 'processImagePath endswith "/usr/sbin/sshd" OR subsystem == "com.apple.security"'
```

## Service Creation

If you want this adapter to run as a service, you can run the following script to add a plist file to the endpoint **with your variables replaced**. Please note that this example also has an example predicate, so if you do not wish to use a predicate, remove that line.

```
sudo -i

curl https://downloads.limacharlie.io/adapter/mac/64 -o /usr/local/bin/lc_adapter
chmod +x /usr/local/bin/lc_adapter

tee -a /Library/LaunchDaemons/io.limacharlie.adapter.macunifiedlogging.plist <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
  <dict>
    <key>Label</key>
    <string>io.limacharlie.adapter.macunifiedlogging</string>
    <key>UserName</key>
	<string>root</string>
    <key>RunAtLoad</key>
    <true/>
    <key>WorkingDirectory</key>
    <string>/usr/local/bin</string>
    <key>KeepAlive</key>
    <true/>
    <key>EnvironmentVariables</key>
    <dict>
      <key>PATH</key>
      <string>/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</string>
    </dict>
    <key>Program</key>
    <string>/usr/local/bin/lc_adapter</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/lc_adapter</string>
        <string>mac_unified_logging</string>
        <string>client_options.identity.installation_key=$INSTALLATION_KEY</string>
        <string>client_options.identity.oid=$OID</string>
        <string>client_options.hostname=$SENSOR_NAME</string>
        <string>client_options.platform=json</string>
        <string>client_options.sensor_seed_key=$SENSOR_NAME</string>
        <string>predicate=eventMessage CONTAINS[c] "corp.sap.privileges"</string>
    </array>
  </dict>
</plist>
EOF

launchctl load -w /Library/LaunchDaemons/io.limacharlie.adapter.macunifiedlogging.plist
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Mac Unified Logging

## Overview

This Adapter allows you to collect events from MacOS Unified Logging.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

**Optional Arguments:**

* `predicate`: example, `predicate='subsystem=="com.apple.TimeMachine"'`

## CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter mac_unified_logging client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME
```

### Infrastructure as Code Deployment

```
# macOS Unified Logging Specific Docs: https://docs.limacharlie.io/docs/adapter-types-macos-unified-logging

sensor_type: "mac_unified_logging"
  mac_unified_logging:
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_MACOSUL"
      hostname: "user-macbook-pro"
      platform: "mac_unified_logging"
      sensor_seed_key: "macos-unified-logging-sensor"
    # Optional configuration
    write_timeout_sec: 600                           # Default: 600 seconds
    predicate: 'processImagePath endswith "/usr/sbin/sshd" OR subsystem == "com.apple.security"'
```

## Service Creation

If you want this adapter to run as a service, you can run the following script to add a plist file to the endpoint **with your variables replaced**. Please note that this example also has an example predicate, so if you do not wish to use a predicate, remove that line.

```
sudo -i

curl https://downloads.limacharlie.io/adapter/mac/64 -o /usr/local/bin/lc_adapter
chmod +x /usr/local/bin/lc_adapter

tee -a /Library/LaunchDaemons/io.limacharlie.adapter.macunifiedlogging.plist <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
  <dict>
    <key>Label</key>
    <string>io.limacharlie.adapter.macunifiedlogging</string>
    <key>UserName</key>
	<string>root</string>
    <key>RunAtLoad</key>
    <true/>
    <key>WorkingDirectory</key>
    <string>/usr/local/bin</string>
    <key>KeepAlive</key>
    <true/>
    <key>EnvironmentVariables</key>
    <dict>
      <key>PATH</key>
      <string>/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</string>
    </dict>
    <key>Program</key>
    <string>/usr/local/bin/lc_adapter</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/lc_adapter</string>
        <string>mac_unified_logging</string>
        <string>client_options.identity.installation_key=$INSTALLATION_KEY</string>
        <string>client_options.identity.oid=$OID</string>
        <string>client_options.hostname=$SENSOR_NAME</string>
        <string>client_options.platform=json</string>
        <string>client_options.sensor_seed_key=$SENSOR_NAME</string>
        <string>predicate=eventMessage CONTAINS[c] "corp.sap.privileges"</string>
    </array>
  </dict>
</plist>
EOF

launchctl load -w /Library/LaunchDaemons/io.limacharlie.adapter.macunifiedlogging.plist
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Microsoft 365

Microsoft 365, formerly Office 365, is a product family of productivity software, collaboration and cloud-based services owned by Microsoft. This Adapter allows you to ingest audit events from the [Office 365 Management Activity API](https://learn.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference).

Microsoft 365 events can be ingested in LimaCharlie and observed as the `office365` platform.

## Adapter Deployment

Microsoft 365 events are ingested via a cloud-to-cloud Adapter configured specifically to review M365 events. When creating an Adapter, the following data points are required:

* `domain`: Office 365 domain
* `tenant_id`: Office 365 tenant ID
* `publisher_id`: Office 365 publisher ID (for single-tenant Apps, the PublisherID is the same as the Tenant ID)
* `client_id`: Office 365 client ID
* `client_secret`: Office 365 client secret
* `endpoint`: Office 365 API endpoint
* `content_types`: content types of events to ingest.
  + Options include:
    - `Audit.AzureActiveDirectory`
    - `Audit.Exchange`
    - `Audit.SharePoint`
    - `Audit.General`
    - `DLP.All`
  + *Default is all of the above*

If creating a Microsoft 365 Adapter via the Web UI, the helper form will navigate you through providing these values.

Establishing a cloud-to-cloud connector between LimaCharlie and Office 365 requires a few steps to provide the correct permissions for the [Office 365 Management Activity API](https://learn.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference).

### Infrastructure as Code Deployment

```
# Office 365 Management Activity API Specific Docs: https://docs.limacharlie.io/docs/adapter-types-office-365-management-activity-api
# For cloud sensor deployment, store credentials as hive secrets:

#   tenant_id: "hive://secret/o365-tenant-id"
#   client_id: "hive://secret/o365-client-id"
#   client_secret: "hive://secret/o365-client-secret"

sensor_type: "office365"
office365:
  tenant_id: "hive://secret/azure-o365-tenant-id"
  client_id: "hive://secret/azure-o365-client-id"
  client_secret: "hive://secret/azure-o365-client-secret"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_O365"
    hostname: "ms-o365-adapter"
    platform: "json"
    sensor_seed_key: "office365-audit-sensor"
    mapping:
      sensor_hostname_path: "ClientIP"
      event_type_path: "Operation"
      event_time_path: "CreationTime"
    indexing: []
  # Office 365 specific configuration
  content_types:
    - "Audit.AzureActiveDirectory"
    - "Audit.Exchange"
    - "Audit.SharePoint"
    - "Audit.General"
    - "DLP.All"
  # Optional configuration
  endpoint: "enterprise"                           # Default: "enterprise"
  start_time: "2024-01-01T00:00:00Z"              # Optional: historical start time
  domain: "yourcompany.onmicrosoft.com"           # Optional: for GCC environments
  publisher_id: "hive://secret/o365-publisher-id" # Optional: usually same as tenant_id
```

## Configuring a Microsoft 365 Adapter in the Web UI

### Preparing Office 365 details

To establish an Office 365 adapter, we will need to complete a few steps within the Azure portal. Ensure that you have the correct permissions to set up a new App registration.

* Within the Microsoft Azure portal, create a new App registration. You can follow Microsoft's Quickstart guide [here](https://learn.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app).
* The LimaCharlie connector requires a secret for Office 365 data. You can create one under `Certificates & secrets`. Be sure to copy this value and save it somewhere - you can only view it once.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2873%29.png)

* Additionally, you'll need to ensure that the app has the correct permissions to view Office 365 data via the Management API. Within `API Permissions`, configure the following permissions:
  + `ActivityFeed.Read` (Delegated & Application)
  + `ActivityFeed.ReadDlp` (Delegated & Application) *[if you want DLP permissions]*

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2874%29.png)

Additionally, you may need to grant admin consent to the above permissions.

At this point, you should have all the details you need to configure the Adapter.

### Setting Up the Adapter

Within the LimaCharlie web application, select `+ Add` Sensor, and then select `Office 365`:

You can select a pre-existing Installation Key or create a new one, unique for this adapter. Once an Installation Key is selected, you will be prompted with a form to finish setting up the adapter. Choose your desired adapter name, and provide the following values:

| Item | Azure Portal Location |
| --- | --- |
| Domain | Home |
| Tenant ID | App Registration Overview |
| Publisher ID | App Registration Overview |
| Client ID | App Registration Overview |
| Client Secret | Created during creation in Certificates & secrets |
| API Endpoint | `enterprise`, `gcc-gov`, `gcc-high-gov`, or `dod-gov` |

Finally, you will also need to select a "Content Type" to import. This is the type of events you want to bring in to LimaCharlie. The options are as follows:

* `Audit.AzureActiveDirectory`
* `Audit.Exchange`
* `Audit.SharePoint`
* `Audit.General`
* `DLP.All`

Without a value, the default is *all of the above*.

Click `Complete Cloud Installation`, and LimaCharlie will attempt to connect to the Microsoft Office 365 Management API and pull events.

## Sample Rule

When ingested into LimaCharlie, Office 365 data can be referenced directly in your D&R rules. You could do this via a platform operator:

```
op: is platform
name: office365
```

We can also reference Office 365 events directly. The following sample rule looks at `FileAccessed` events from anonymous user names, and reports accordingly.

```
# Detection
event: FileAccessed
path: event/UserId
op: contains
value: anon

# Response
- action: report
  name: OneDrive File Accessed by Anonymous User
```

Note that in the detection above, we pivot on the `FileAccessed` event, which is associated with SharePoint activity. Available event types will depend on source activity and events ingested. More information on audit log activities can be found [here](https://learn.microsoft.com/en-us/purview/audit-log-activities).

---

# Microsoft Defender

## Overview

LimaCharlie can ingest [Microsoft 365 Defender logs](https://learn.microsoft.com/en-us/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide) via three methods [Azure Event Hub](/v2/docs/adapter-types-azure-event-hub) Adapter, the [Microsoft Defender API](https://learn.microsoft.com/en-us/defender-endpoint/api/exposed-apis-create-app-nativeapp), or a Custom Webhook

Documentation for creating an event hub can be found here [here](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

Telemetry Platform: `msdefender`

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

* `connection_string` - The connection string provided in Azure for connecting to the Azure Event Hub, including the `EntityPath=...` at the end which identifies the Hub Name (this component is sometimes now shown in the connection string provided by Azure).

## Guided Deployment

In the LimaCharlie web app, you can find a Microsoft Defender helper for connecting to an existing Azure Event Hub and ingesting Microsoft Defender logs.

### CLI Deployment

The following example configuration ingests Microsoft Defender logs from an Azure Event Hub to LimaCharlie.

```
./lc_adapter azure_event_hub client_options.identity.installation_key=<INSTALLATION_KEY> \
client_options.identity.oid=<OID> \
client_options.platform=msdefender \
client_options.sensor_seed_key=<SENSOR_SEED_KEY> \
client_options.hostname=msdefender \
"connection_string=Endpoint=sb://mynamespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fnaaaaaaaaaaaaaaak0g54alYbbbbbbbbbbbbbbbALQ=;EntityPath=lc-stream"
```

### Infrastructure as Code Deployment

```
# Adapter Documentation: https://docs.limacharlie.io/docs/adapter-types
# For cloud sensor deployment, store credentials as hive secrets:

#   tenant_id: "hive://secret/azure-tenant-id"
#   client_id: "hive://secret/defender-client-id"
#   client_secret: "hive://secret/defender-client-secret"

sensor_type: "defender"
defender:
  tenant_id: "hive://secret/azure-tenant-id"
  client_id: "hive://secret/azure-defender-client-id"
  client_secret: "hive://secret/azure-defender-client-secret"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_DEFENDER"
    hostname: "ms-defender-adapter"
    platform: "json"
    sensor_seed_key: "defender-sensor"
    mapping:
      sensor_hostname_path: "machineDnsName"
      event_type_path: "alertType"
      event_time_path: "lastUpdateTime"
    indexing: []
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Command-line Interface

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Microsoft Defender

## Overview

LimaCharlie can ingest [Microsoft 365 Defender logs](https://learn.microsoft.com/en-us/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide) via three methods [Azure Event Hub](/v2/docs/adapter-types-azure-event-hub) Adapter, the [Microsoft Defender API](https://learn.microsoft.com/en-us/defender-endpoint/api/exposed-apis-create-app-nativeapp), or a Custom Webhook

Documentation for creating an event hub can be found here [here](https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-create).

Telemetry Platform: `msdefender`

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

* `connection_string` - The connection string provided in Azure for connecting to the Azure Event Hub, including the `EntityPath=...` at the end which identifies the Hub Name (this component is sometimes now shown in the connection string provided by Azure).

## Guided Deployment

In the LimaCharlie web app, you can find a Microsoft Defender helper for connecting to an existing Azure Event Hub and ingesting Microsoft Defender logs.

### CLI Deployment

The following example configuration ingests Microsoft Defender logs from an Azure Event Hub to LimaCharlie.

```
./lc_adapter azure_event_hub client_options.identity.installation_key=<INSTALLATION_KEY> \
client_options.identity.oid=<OID> \
client_options.platform=msdefender \
client_options.sensor_seed_key=<SENSOR_SEED_KEY> \
client_options.hostname=msdefender \
"connection_string=Endpoint=sb://mynamespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=fnaaaaaaaaaaaaaaak0g54alYbbbbbbbbbbbbbbbALQ=;EntityPath=lc-stream"
```

### Infrastructure as Code Deployment

```
# Adapter Documentation: https://docs.limacharlie.io/docs/adapter-types
# For cloud sensor deployment, store credentials as hive secrets:

#   tenant_id: "hive://secret/azure-tenant-id"
#   client_id: "hive://secret/defender-client-id"
#   client_secret: "hive://secret/defender-client-secret"

sensor_type: "defender"
defender:
  tenant_id: "hive://secret/azure-tenant-id"
  client_id: "hive://secret/azure-defender-client-id"
  client_secret: "hive://secret/azure-defender-client-secret"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_DEFENDER"
    hostname: "ms-defender-adapter"
    platform: "json"
    sensor_seed_key: "defender-sensor"
    mapping:
      sensor_hostname_path: "machineDnsName"
      event_type_path: "alertType"
      event_time_path: "lastUpdateTime"
    indexing: []
```

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Command-line Interface

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Mimecast

## Overview

This Adapter allows you to connect to the Mimecast API to stream audit events as they happen.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `mimecast`

* `client_id`: your Mimecast client ID
* `client_secret`: your Mimecast client secret

### CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment#adapter-binaries).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter mimecast client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
client_options.mappings.event_type_path=category \
client_id=$CLIENT_ID client_secret=$CLIENT_SECRET
```

### Infrastructure as Code Deployment

```
# Mimecast Specific Docs: https://docs.limacharlie.io/docs/adapter-types-mimecast
# For cloud sensor deployment, store credentials as hive secrets:

#   client_id: "hive://secret/mimecast-client-id"
#   client_secret: "hive://secret/mimecast-client-secret"

sensor_type: "mimecast"
mimecast:
  client_id: "hive://secret/mimecast-client-id"
  client_secret: "hive://secret/mimecast-client-secret"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_MIMECAST"
    hostname: "mimecast-logs-adapter"
    platform: "json"
    sensor_seed_key: "mimecast-audit-sensor"
    mapping:
      sensor_hostname_path: "sender"
      event_type_path: "eventType"
      event_time_path: "eventTime"
    indexing: []
```

## API Doc

See the official [documentation](https://developer.services.mimecast.com/docs/auditevents/1/routes/api/audit/get-audit-events/post).

---

# Okta

## Overview

This Adapter allows you to connect to Okta to fetch system logs.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `okta`

* `apikey`: your Okta API key/token
* `url`: your Okta URL (ex: `https://dev-003462479.okta.com`)

### CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment#adapter-binaries).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter okta client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
apikey=$API_KEY url=$URL
```

### Infrastructure as Code Deployment

```
# Okta Specific Docs: https://docs.limacharlie.io/docs/adapter-types-okta
# For cloud sensor deployment, store credentials as hive secrets:

#   apikey: "hive://secret/okta-api-token"

sensor_type: "okta"
okta:
  apikey: "hive://secret/okta-api-key"
  url: "https://your-company.okta.com"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_OKTA"
    hostname: "okta-systemlog-adapter"
    platform: "json"
    sensor_seed_key: "okta-system-logs-sensor"
    mapping:
      sensor_hostname_path: "client.device"
      event_type_path: "eventType"
      event_time_path: "published"
    indexing: []
```

## API Doc

See the official [documentation](https://developer.okta.com/docs/reference/api/system-log/).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Command-line Interface

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Platform Management

## Overview

The Platform Management section covers essential tools and settings that help you control and configure your LimaCharlie environment. Whether you're managing user permissions, handling billing, or customizing your Cloud Sensors and overall platform configuration, these guides will ensure you can efficiently oversee and fine-tune every aspect of your deployment. Dive into the following topics to streamline management and maximize control of your infrastructure.

---

# SQS

## Overview

This Adapter allows you to ingest events received from an AWS SQS instance.

## Configurations

Adapter Type: `sqs`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `access_key`: an Access Key from AWS used to access the queue.
* `secret_key`: the secret key associated with the `access_key` used to access the queue.
* `queue_url`: the queue URL for the SQS instance.

### Infrastructure as Code Deployment

```
# AWS SQS Specific Docs: https://docs.limacharlie.io/docs/adapter-types-sqs

sensor_type: "sqs"
sqs:
  queue_url: "https://sqs.us-east-1.amazonaws.com/123456789012/your-security-logs-queue"
  aws_access_key_id: "hive://secret/aws-access-key-id"
  aws_secret_access_key: "hive://secret/aws-secret-access-key"
  aws_region: "us-east-1"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_SQS"
    platform: "json"
    sensor_seed_key: "aws-sqs-sensor"
    mapping:
      sensor_hostname_path: "source.instance_id"
      event_type_path: "detail.eventName"
      event_time_path: "time"
    indexing: []
  # Optional SQS-specific configuration
  max_messages: 10                       # Default: 10 (max messages per poll)
  wait_time_seconds: 20                  # Default: 20 (long polling)
  visibility_timeout: 300                # Default: 300 seconds
  delete_after_processing: true          # Default: true
```

## API Doc

See the [official documentation](https://aws.amazon.com/sqs/).

---

# SQS

## Overview

This Adapter allows you to ingest events received from an AWS SQS instance.

## Configurations

Adapter Type: `sqs`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).
* `access_key`: an Access Key from AWS used to access the queue.
* `secret_key`: the secret key associated with the `access_key` used to access the queue.
* `queue_url`: the queue URL for the SQS instance.

### Infrastructure as Code Deployment

```
# AWS SQS Specific Docs: https://docs.limacharlie.io/docs/adapter-types-sqs

sensor_type: "sqs"
sqs:
  queue_url: "https://sqs.us-east-1.amazonaws.com/123456789012/your-security-logs-queue"
  aws_access_key_id: "hive://secret/aws-access-key-id"
  aws_secret_access_key: "hive://secret/aws-secret-access-key"
  aws_region: "us-east-1"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_SQS"
    platform: "json"
    sensor_seed_key: "aws-sqs-sensor"
    mapping:
      sensor_hostname_path: "source.instance_id"
      event_type_path: "detail.eventName"
      event_time_path: "time"
    indexing: []
  # Optional SQS-specific configuration
  max_messages: 10                       # Default: 10 (max messages per poll)
  wait_time_seconds: 20                  # Default: 20 (long polling)
  visibility_timeout: 300                # Default: 300 seconds
  delete_after_processing: true          # Default: true
```

## API Doc

See the [official documentation](https://aws.amazon.com/sqs/).

---

# Sophos

## Overview

This Adapter allows you to connect to Sophos Central to fetch event logs.

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `sophos`

* `tenantid`: your Sophos Central tenant ID
* `clientid`: your Sophos Central client ID
* `clientsecret`: your Sophos Central client secret
* `url`: your Sophos Central URL (ex: `https://api-us01.central.sophos.com`)

### Creating Your Credentials and Getting Your Tenant ID

Sophos documentation - <https://developer.sophos.com/getting-started-tenant>

1. Add a new credential [here](https://cloud.sophos.com/manage/config/settings/credentials)
2. Get your client ID and client secret from the credentials you just created
3. Get your JWT -- be sure to replace the values with the client ID and secret from the last step

   ```
   curl -XPOST -H "Content-Type:application/x-www-form-urlencoded" -d "grant_type=client_credentials&client_id=YOUR_CLIENT_ID&client_secret=YOUR_CLIENT_SECRET&scope=token" https://id.sophos.com/api/v2/oauth2/token
   ```

   Response content -- grab the `access_token` from the output:

   ```
   {
      "access_token": "SAVE_THIS_VALUE",
      "errorCode": "success",
      "expires_in": 3600,
      "message": "OK",
      "refresh_token": "<token>",
      "token_type": "bearer",
      "trackingId": "<uuid>"
   }
   ```
4. Get your tenant ID -- you will need the `access_token` (JWT) from the last step.

   ```
   curl -XGET -H "Authorization: Bearer YOUR_JWT_HERE" https://api.central.sophos.com/whoami/v1
   ```

   Response content -- grab the `id` (`tenant_id`) and `dataRegion` (`url`) from the output. You will need these for your LimaCharlie Sophos adapter configuration.

   ```
   {
       "id": "57ca9a6b-885f-4e36-95ec-290548c26059",
       "idType": "tenant",
       "apiHosts": {
           "global": "https://api.central.sophos.com",
           "dataRegion": "https://api-us03.central.sophos.com"
       }
   }
   ```
5. Now you have all the pieces for your adapter:

   1. `client_id`
   2. `client_secret`
   3. `tenant_id`
   4. `url`

### Infrastructure as Code Deployment

```
# Sophos Central Specific Docs: https://docs.limacharlie.io/docs/adapter-types-sophos-central
# For cloud sensor deployment, store credentials as hive secrets:

#   clientid: "hive://secret/sophos-client-id"
#   clientsecret: "hive://secret/sophos-client-secret"
#   tenantid: "hive://secret/sophos-tenant-id"

sensor_type: "sophos"
sophos:
  clientid: "hive://secret/sophos-client-id"
  clientsecret: "hive://secret/sophos-client-secret"
  tenantid: "hive://secret/sophos-tenant-id"
  url: "https://api-us01.central.sophos.com"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_SOPHOS"
    hostname: "sophos-central-adapter"
    platform: "json"
    sensor_seed_key: "sophos-siem-sensor"
    mapping:
      sensor_hostname_path: "endpoint.hostname"
      event_type_path: "type"
      event_time_path: "raisedAt"
    indexing: []
```

## API Doc

See the official [documentation](https://developer.sophos.com/docs/siem-v1/1/overview).

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Infrastructure as Code (IaC) automates the management and provisioning of IT infrastructure using code, making it easier to scale, maintain, and deploy resources consistently. In LimaCharlie, IaC allows security teams to deploy and manage sensors, rules, and other security infrastructure programmatically, ensuring streamlined, repeatable configurations and faster response times, while maintaining infrastructure-as-code best practices in cybersecurity operations.

---

# Stdin

This example is similar to the Syslog example above, except it uses the CLI Adapter and receives the data from the CLI's STDIN interface. This method is perfect for ingesting arbitrary logs on disk or from other applications locally.

```
./lc_adapter stdin client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d \
      client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
      client_options.platform=text \
      "client_options.mapping.parsing_grok.message=%{DATESTAMP:date} %{HOSTNAME:host} %{WORD:exe}\[%{INT:pid}\]: %{GREEDYDATA:msg}" \
      client_options.sensor_seed_key=testclient3 \
      client_options.mapping.event_type_path=exe
```

Here's a breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `stdin`: the method the Adapter should use to collect data locally. The `stdin` value will simply ingest from the Adapter's STDIN.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=text`: this indicates the type of data that will be received from this adapter. In this case it's `text` lines.
* `client_options.mapping.parsing_grok.message=....`: this is the grok expression describing how to interpret the text lines and how to convert them to JSON.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `client_options.mapping.event_type_path=....`: specifies the field that should be interpreted as the "event_type" in LimaCharlie.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Stdin

## Overview

This Adapter allows you to ingest logs from the adapter stdin.

## Configurations

Adapter Type: `stdin`

* `client_options`: common configuration for adapter as defined [here](/v2/docs/adapters#usage).

## API Doc

None

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Stdin

This example is similar to the Syslog example above, except it uses the CLI Adapter and receives the data from the CLI's STDIN interface. This method is perfect for ingesting arbitrary logs on disk or from other applications locally.

```
./lc_adapter stdin client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d \
      client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
      client_options.platform=text \
      "client_options.mapping.parsing_grok.message=%{DATESTAMP:date} %{HOSTNAME:host} %{WORD:exe}\[%{INT:pid}\]: %{GREEDYDATA:msg}" \
      client_options.sensor_seed_key=testclient3 \
      client_options.mapping.event_type_path=exe
```

Here's a breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `stdin`: the method the Adapter should use to collect data locally. The `stdin` value will simply ingest from the Adapter's STDIN.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=text`: this indicates the type of data that will be received from this adapter. In this case it's `text` lines.
* `client_options.mapping.parsing_grok.message=....`: this is the grok expression describing how to interpret the text lines and how to convert them to JSON.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `client_options.mapping.event_type_path=....`: specifies the field that should be interpreted as the "event_type" in LimaCharlie.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Stdin JSON

This example is similar to the Stdin example above, except it assumes the data being read is JSON, not just text. If your data source is already JSON, it's much simpler to let LimaCharlie do the JSON parsing directly.

```
./lc_adapter stdin client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d \
    client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
    client_options.platform=json \
    client_options.sensor_seed_key=testclient3 \
    client_options.mapping.event_type_path=type \
    client_options.hostname=testclient3
```

Here's a breakdown of the above example:

* `lc_adapter`: simply the CLI Adapter.
* `stdin`: the method the Adapter should use to collect data locally. The `stdin` value will simply ingest from the Adapter's STDIN.
* `client_options.identity.installation_key=....`: the Installation Key value from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=json`: this indicates that the data read is already JSON, so just parse it as so.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor ID generated for this Adapter later if you have to re-install the Adapter.
* `client_options.mapping.event_type_path=....`: specifies the field that should be interpreted as the "event_type" in LimaCharlie.
* `client_options.hostname=....`: specifies the sensor hostname for the adapter

Note that we did not need to specify a `parsing_re` or `parsing_grok` because the data ingested is not text, but already JSON, so the Parsing step is already done for us by setting a `platform=json`.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Sublime Security

[Sublime Security](https://sublime.security/) is a comprehensive email security platform that allows users to create custom detections, gain visibility and control, and focus on prevention of malicious emails.

## Ingesting Audit Logs

Audit logs from Sublime can be ingested cloud-to-cloud via the API.

### Adapter-specific Options

Adapter Type: `sublime`

* `api_key`: your Okta API key/token

### CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment#adapter-binaries).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter sublime client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=sublime \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
api_key=$API_KEY
```

### Infrastructure as Code Deployment

```
# Sublime Security Specific Docs: https://docs.limacharlie.io/docs/adapter-types-sublime-security
# For cloud sensor deployment, store credentials as hive secrets:

#   api_key: "hive://secret/sublime-api-key"

sensor_type: "sublime"
sublime:
  api_key: "hive://secret/sublime-api-key"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_SUBLIME"
    hostname: "sublime-security-adapter"
    platform: "json"
    sensor_seed_key: "sublime-audit-sensor"
    mapping:
      sensor_hostname_path: "user.email"
      event_type_path: "type"
      event_time_path: "created_at"
    indexing: []
```

## API Doc

See the official [documentation](https://docs.sublime.security/reference/authentication).

## Ingesting Alerts

Sublime events can be ingested in LimaCharlie via a `json` Webhook Adapter configuration.

### Adapter Deployment

Sublime Security logs are ingested via a cloud-to-cloud webhook Adapter configured to receive JSON events. The steps of creating this Adapter and enabling the input include:

1. Creating the Webhook Adapter via the LimaCharlie CLI
2. Discovering the URL created for the Webhook Adapter.
3. Providing the completed URL to Sublime Security for webhook events.

#### 1. Creating the LimaCharlie Webhook Adapter

The following steps are modified from the generic Webhook Adapter creation documentation, found [here](/v2/docs/tutorial-creating-a-webhook-adapter).

Creating a Webhook Adapter requires a set of parameters, including organization ID, Installation Key, platform, and mapping details, among other parameters. The following configuration can be modified to easily configure a Webhook Adapter for ingesting Sublime Security events:

```
{
    "sensor_type": "webhook",
    "webhook": {
       "secret": "sublime-security",
        "client_options": {
            "hostname": "sublime-security",
            "identity": {
                "oid": "<your_oid>",
                "installation_key": "<your_installation_key>"
            },
            "platform": "json",
            "sensor_seed_key": "sublime-super-secret-key",
            "mapping" : {
                "event_type_path" : "data/flagged_rules/name",
                "event_time_path" : "created_at"
            }
        }
    }
}
```

Note that in the mapping above, we make the following changes:

* `event_type_path` is mapped to the rule name from the Sublime alert
* `event_time_path` is mapped to the `created_at` field from the Sublime alert

#### 2. Building the Adapter URL

After creating the webhook, you'll need to retrieve the webhook URL from the [Get Org URLs](https://docs.limacharlie.io/apidocs/get-org-urls) API call. You'll need the following information to complete the Webhook URL:

* Organization ID
* Webhook name (from the config)
* Secret (from the config)

Let's assume the returned domain looks like `9157798c50af372c.hook.limacharlie.io`, the format of the URL would be:

`https://9157798c50af372c.hook.limacharlie.io/OID/HOOKNAME/SECRET`

Note that the `secret` value can be provided in the webhook URL or as an HTTP header named `lc-secret`.

#### 3. Configuring the Sublime webhook Action

Within the Sublime Security console, navigate to **Manage** > **Actions**. From here, you can select **New Action** > **Webhook**.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28174%29.png)

Within the **Configure webhook** menu, provide a name and the Adapter URL constructed in Step 2 above.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28175%29.png)

As mentioned in Step 2, you can configure the HTTP header `lc-secret`, if so desired.

Upon configuration of the webhook within Sublime Security, alerts can be configured to be sent to the LimaCharlie platform. To test the Webhook, select **Trigger Custom Action** from any Flagged message, and send to the LimaCharlie webhook.

---

# Syslog

Syslog is both a protocol and common logging format that consolidate events to a central location for storage. On \*nix systems, Syslog often outputs to predefined locations, such as `/var/log`. The LimaCharlie Adapter can be configured as a Syslog endpoint to collect events either via TCP or UDP.

Syslog data can also be ingested via other data platforms, such as an S3 bucket.

Syslog events are observed in LimaCharlie as the `text` platform.

A more detailed guide to syslog collection can be found in the [Log Collection Guide](/v2/docs/logcollectionguide).

## Adapter Deployment

Given its ubiquity, Syslog can be ingested via a myriad of methods in both text/log and streaming formats. For non-streaming methods, please refer to the corresponding Adapter type (such as [S3](/v2/docs/adapter-types-s3), [GCP](/v2/docs/adapter-types-google-cloud-pubsub), etc.)

### Syslog-specific Configurations

All Adapters have the same common client configuration options, found [here](/v2/docs/adapter-usage). A syslog Adapter has a few unique configuration options not found with other Adapter types. These include:

* `port`: port to listen for syslog from.
* `iface`: the interface name to listen for new connections/packets from, defaults to all.
* `is_udp`: if `true`, listen over UDP instead of TCP.
* `ssl_cert`: path to a file with the SSL cert to use to receive logs over TCP.
* `ssl_key`: path to a file with the SSL key to use to receive logs over TCP.

### Collecting Syslog via Docker

The following example walks through configuring a Docker container as a syslog Adapter.

```
docker run --rm -it -p 1514:1514 refractionpoint/lc-adapter:latest syslog port=1514 \
  client_options.identity.installation_key=e9a3bcdf-efa2-47ae-b6df-579a02f3a54d \
  client_options.identity.oid=8cbe27f4-bfa1-4afb-ba19-138cd51389cd \
  client_options.platform=text "client_options.mapping.parsing_grok=%{DATESTAMP:date} %{HOSTNAME:host} %{WORD:exe}\[%{INT:pid}\]: %{GREEDYDATA:msg}" \
  client_options.sensor_seed_key=testclient1 \
  client_options.mapping.rename_only=true \
  "client_options.mapping.mapping[0].src_field=host" \
  "client_options.mapping.mapping[0].dst_field=syslog_hostname"
```

Here's a breakdown of the above example:

* `docker run --rm`: run a container and don't keep the contents around when it's stopped.
* `-it`: make the container interactive so you can ctrl-c to stop it.
* `-p 1514:1514`: allow the container to listen on port `1514` on the local host and use the same port within the container.
* `refractionpoint/lc-adapter:latest`: this is the name of the public container provided by LimaCharlie.
* `syslog`: the method the Adapter should use to collect data locally. The `syslog` value will operate as a syslog endpoint on the TCP port specified.
* `port=1514`: the TCP port the Adapter should listen on. By default this is a normal TCP connection (not SSL), although SSL options exist.
* `client_options.identity.installation_key=....`: the Installation Key from LimaCharlie.
* `client_options.identity.oid=....`: the Organization ID from LimaCharlie the installation key above belongs to.
* `client_options.platform=text`: this indicates the type of data that will be received from this adapter. In this case it's syslog, so `text` lines.
* `client_options.mapping.parsing_grok=....`: this is the grok expression describing how to interpret the text lines and how to convert them to JSON.
* `client_options.sensor_seed_key=....`: this is the value that identifies this instance of the Adapter. Record it to re-use the Sensor generated for this Adapter later if you have to re-install the Adapter.
* `client_options.mapping.rename_only=true`: only rename the field in mapping below, so keep the other original fields.
* `client_options.mapping.mapping[0].src_field=....`: the source field of the first mapping record.
* `client_options.mapping.mapping[0].dst_field=....`: the destination field of the first mapping record.

To test it, assuming we're on the same Debian box as the container, pipe the syslog to the container:

```
journalctl -f -q | netcat 127.0.0.1 1514
```

### Collecting Syslog via Binary Adapter

The LimaCharlie binary Adapter can be deployed as a syslog listener. This option allows you to configure multiple syslog outputs to a single listener, and ingest multiple types of events with a single Adapter.

#### Step 1: Create an installation key

We recommend utilizing a unique installation key for this deployment, specifically with a `syslog` Tag. This allows for a level of delineation within rules and outputs via Tags.

#### Step 2: Create an Adapter config file

Syslog events are typically ingested as `text`, however often have specific structures to them. Utilizing a config file allows for easy management of a regex string to extract relevant fields from syslog output.

The following example config file can be a starting point. However, you might need to modify the regex to match your specific message.

```
# Syslog Specific Docs: https://docs.limacharlie.io/docs/adapter-types-syslog

sensor_type: "syslog"
  syslog:
    port: 1514
    iface: "0.0.0.0"
    client_options:
      identity:
        oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
        installation_key: "YOUR_LC_INSTALLATION_KEY_SYSLOG"
      hostname: "syslog-adapter"
      platform: "linux"
      sensor_seed_key: "syslog-collector"
      mapping:
        parsing_grok:
          message: "^<%{INT:pri}>%{SYSLOGTIMESTAMP:timestamp}\\s+%{HOSTNAME:hostname}\\s+%{WORD:tag}(?:\\[%{INT:pid}\\])?:\\s+%{GREEDYDATA:message}"
        sensor_hostname_path: "hostname"
        event_type_path: "tag"
        event_time_path: "timestamp"
    # Optional syslog-specific configuration
    is_udp: false                               # TCP (default) vs UDP
    write_timeout_sec: 30                       # Write timeout
    ssl_cert: "/certs/syslog_server.pem"       # Optional SSL cert
    ssl_key: "/certs/syslog_server.key"        # Optional SSL key
    mutual_tls_cert: "/certs/client_ca.pem"    # Optional mTLS
```

#### Step 3: Configure syslog output to send messages to a local listener

This step will depend on the type of syslog daemon you are using (syslog, rsyslog, syslog-ng, etc.) Within the daemon configuration file, configure the desired facility(-ies) to direct to the local listener. In the following example, we configured `auth` and `authpriv` events to write to both `/var/log/audit.log` and `127.0.0.1:1514`.

```
auth,authpriv.*			/var/log/auth.log
auth,authpriv.*			@@127.0.0.1:1514
```

After applying the appropriate configuration, restart the syslog daemon.

#### Step 4: Confirm that syslog messages are sent to the correct location

Utilizing a tool like `netcat`, you can listen on the appropriate port to confirm that messages are being sent. The following command will spawn a `netcat` listener on port 1514:

```
nc -l -p 1514
```

#### Step 5: Run the LimaCharlie Adapter

Execute the binary Adapter with the syslog configuration file in order to start the LimaCharlie listener. If started correctly, you should see the following messages in `stdout`:

```
DBG <date>: usp-client connecting
DBG <date>: usp-client connected
DBG <date>: listening for connections on :1514
```

Double-check the LimaCharlie Sensors list, and you should see the text adapter with the respective hostname sending `Syslog` events.

---

# Tutorial: Creating a Webhook Adapter

LimaCharlie supports webhooks as a telemetry ingestion method. Webhooks are technically cloud Adapters, as they cannot be deployed on-prem or through the downloadable Adapter binary.

Webhook adapters are created by enabling a webhook through the `cloud_sensor` Hive feature. Webhook creation will enable a specific URL that can receive webhooks from any platform. Received data will be ingested in LimaCharlie as a Sensor, similar to an Office365 or Syslog Adapter.

## Creating a Webhook Adapter

Webhook adapters can be created either through the webapp, API, or CLI. Before creation, let's look at the basic webhook configuration and values necessary to build the adapter.

```
{
    "sensor_type": "webhook",
    "webhook": {
        // This secret value will be part of the URL to accept your webhooks.
        // It enables you to prevent or revoke unauthorized access to a hook.
        "secret": "some-secret-value-hard-to-predict",

        // Placeholder for generic webhook signature validation.
        // If you require a specific format, please get in touch with us.
        "signature_secret": "",
        "signature_header": "",
        "signature_scheme": "",

        // Format with which the data is ingested in LC.
        "client_options": {
            // Provide your own name for the webhook adapter
            "hostname": "<any_name>",
            "identity": {
                // Provide the OID of the organization you wish to send to
                "oid": "<oid>",
                // Provide the installation key to be used for the adapter
                "installation_key": "<installation_key>"
            },
            "platform": "json",
            "sensor_seed_key": "<any-super-secret-seed-key>"
        }
    }
}
```

When the above configuration is provided to LimaCharlie, a webhook adapter will appear and be available for webhook event ingestion. Here's an example of creating the above record through the LimaCharlie CLI:

```
echo '{"sensor_type": "webhook", "webhook": {"secret": "some-secret-value-hard-to-predict", "signature_secret": "", "signature_header": "", "signature_scheme": "", "client_options": {"hostname": "<any_name>", "identity": {"oid": "<oid>", "installation_key": "<installation_key>"}, "platform": "json", "sensor_seed_key": "test-webhook"}}}' | limacharlie hive set cloud_sensor --key my-webhook --data -
```

After creating the webhook, you will be provided with a geo-dependent URL, respective to your LimaCharlie Organization location. You can also retrieve your webhook URLs with either of the following commands:

* REST API: getOrgURLs
* Python SDK:

```
python3 -c "import limacharlie; print(limacharlie.Manager().getOrgURLs()['hooks'])"
```

## Using the webhook adapter

After capturing the webhook URL in the previous step, only a few more pieces of data are necessary to construct the webhook ingestion.

Let's assume the returned domain looks like `9157798c50af372c.hook.limacharlie.io`, the format of the URL would be:

`https://9157798c50af372c.hook.limacharlie.io/OID/HOOKNAME/SECRET`, where:

* OID is the Organization OID provided in the configuration above.
* HOOKNAME is the name of the hook provided in the configuration above.
* SECRET is the secret value provided in the configuration. You can provide the secret value in the URL or as an HTTP header named `lc-secret`.

## Supported Webhook Format

When sending data via POST requests to the URL, the body of your request is expected to be one or many JSON events. Supported formats include:

* Simple JSON object:
  + `{"some":"data"}`
* List of JSON objects:
  + `[{"some":"data"},{"some":"data"}]`
* Newline separated JSON objects like:

```
{"some":"data"}
{"some":"data"}
{"some":"data"}
```

Or, one of the above, but compressed using gzip.

With the completed webhook URL, you can begin sending events and will see them in the Timeline for your webhook Adapater.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Tutorial: Creating a Webhook Adapter

LimaCharlie supports webhooks as a telemetry ingestion method. Webhooks are technically cloud [Adapters](/v2/docs/adapters), as they cannot be deployed on-prem or through the downloadable Adapter binary.

Webhook adapters are created by enabling a webhook through the `cloud_sensor` [Hive](/v2/docs/config-hive) feature. Webhook creation will enable a specific URL that can receive webhooks from any platform. Received data will be ingested in LimaCharlie as a Sensor, similar to an Office365 or Syslog Adapter.

## Creating a Webhook Adapter

Webhook adapters can be created either through the webapp, API, or CLI. Before creation, let's look at the basic webhook configuration and values necessary to build the adapter.

```
{
    "sensor_type": "webhook",
    "webhook": {
        // This secret value will be part of the URL to accept your webhooks.
        // It enables you to prevent or revoke unauthorized access to a hook.
        "secret": "some-secret-value-hard-to-predict",

        // Placeholder for generic webhook signature validation.
        // If you require a specific format, please get in touch with us.
        "signature_secret": "",
        "signature_header": "",
        "signature_scheme": "",

        // Format with which the data is ingested in LC.
        "client_options": {
            // Provide your own name for the webhook adapter
            "hostname": "<any_name>",
            "identity": {
                // Provide the OID of the organization you wish to send to
                "oid": "<oid>",
                // Provide the installation key to be used for the adapter
                "installation_key": "<installation_key>"
            },
            "platform": "json",
            "sensor_seed_key": "<any-super-secret-seed-key>"
        }
    }
}
```

When the above configuration is provided to LimaCharlie, a webhook adapter will appear and be available for webhook event ingestion. Here's an example of creating the above record through the LimaCharlie CLI:

```
echo '{"sensor_type": "webhook", "webhook": {"secret": "some-secret-value-hard-to-predict", "signature_secret": "", "signature_header": "", "signature_scheme": "", "client_options": {"hostname": "<any_name>", "identity": {"oid": "<oid>", "installation_key": "<installation_key>"}, "platform": "json", "sensor_seed_key": "test-webhook"}}}' | limacharlie hive set cloud_sensor --key my-webhook --data -
```

After creating the webhook, you will be provided with a geo-dependent URL, respective to your LimaCharlie Organization location. You can also retrieve your webhook URLs with either of the following commands:

* REST API: [getOrgURLs](https://docs.limacharlie.io/apidocs/get-org-urls)
* Python SDK:

```
python3 -c "import limacharlie; print(limacharlie.Manager().getOrgURLs()['hooks'])"
```

## Using the webhook adapter

After capturing the webhook URL in the previous step, only a few more pieces of data are necessary to construct the webhook ingestion.

Let's assume the returned domain looks like `9157798c50af372c.hook.limacharlie.io`, the format of the URL would be:

`https://9157798c50af372c.hook.limacharlie.io/OID/HOOKNAME/SECRET`, where:

* OID is the Organization OID provided in the configuration above.
* HOOKNAME is the name of the hook provided in the configuration above.
* SECRET is the secret value provided in the configuration. You can provide the secret value in the URL or as an HTTP header named `lc-secret`.

## Supported Webhook Format

When sending data via POST requests to the URL, the body of your request is expected to be one or many JSON events. Supported formats include:

* Simple JSON object:

  + `{"some":"data"}`
* List of JSON objects:

  + `[{"some":"data"},{"some":"data"}]`
* Newline separated JSON objects like:

```
{"some":"data"}
{"some":"data"}
{"some":"data"}
```

Or, one of the above, but compressed using gzip.

With the completed webhook URL, you can begin sending events and will see them in the Timeline for your webhook Adapater.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# VirusTotal

## API Keys

The VirusTotal API key is added via the [integrations](https://docs.limacharlie.io/v2/docs/add-ons-api-integrations#configuration) menu within LimaCharlie.

## Usage

With the `vt` [add-on](https://app.limacharlie.io/add-ons/detail/vt) subscribed and a VirusTotal API Key configured in the Integrations page, VirusTotal can be used as an API-based lookup.

```
event: CODE_IDENTITY
op: lookup
path: event/HASH
resource: hive://lookup/vt
metadata_rules:
  op: is greater than
  value: 1
  path: /
  length of: true
```

Step-by-step, this rule will do the following:

* Upon seeing a `CODE_IDENTITY` event, retrieve the `event/HASH` value and send it to VirusTotal via the `api/vt` resource.
* Upon receiving a response from `api/vt`, evaluate it using `metadata_rules` to see if the length of the response is greater than 1 (in this case meaning that more than 1 vendor reporting a hash is bad).

---

# Zendesk

## Overview

This Adapter allows you to connect to Zendesk to fetch [account activity logs](https://developer.zendesk.com/api-reference/ticketing/account-configuration/audit_logs/#list-audit-logs).

## Deployment Configurations

All adapters support the same `client_options`, which you should always specify if using the binary adapter or creating a webhook adapter. If you use any of the Adapter helpers in the web app, you will not need to specify these values.

* `client_options.identity.oid`: the LimaCharlie Organization ID (OID) this adapter is used with.
* `client_options.identity.installation_key`: the LimaCharlie Installation Key this adapter should use to identify with LimaCharlie.
* `client_options.platform`: the type of data ingested through this adapter, like `text`, `json`, `gcp`, `carbon_black`, etc.
* `client_options.sensor_seed_key`: an arbitrary name for this adapter which Sensor IDs (SID) are generated from, see below.

### Adapter-specific Options

Adapter Type: `zendesk`

* `api_token`: your Zendesk API token
* `zendesk_domain`: your Zendesk domain, like `initech.zendesk.com`
* `zendesk_email`: your Zendesk email address that created the API token

### CLI Deployment

Adapter downloads can be found [here](/v2/docs/adapter-deployment#adapter-binaries).

```
chmod +x /path/to/lc_adapter

/path/to/lc_adapter zendesk client_options.identity.installation_key=$INSTALLATION_KEY \
client_options.identity.oid=$OID \
client_options.platform=json \
client_options.sensor_seed_key=$SENSOR_NAME \
client_options.hostname=$SENSOR_NAME \
client_options.mappings.event_type_path=action \
api_token=$API_TOKEN \
zendesk_domain='$YOUR_COMPANY.zendesk.com' \
zendesk_email=you@yourcompany.com
```

### Infrastructure as Code Deployment

```
# Zendesk Specific Docs: https://docs.limacharlie.io/docs/adapter-types-zendesk
# For cloud sensor deployment, store credentials as hive secrets:
#   api_token: "hive://secret/zendesk-api-token"
#   zendesk_email: "hive://secret/zendesk-email"

sensor_type: "zendesk"
zendesk:
  api_token: "hive://secret/zendesk-api-token"
  zendesk_domain: "yourcompany.zendesk.com"
  zendesk_email: "hive://secret/zendesk-api-email"
  client_options:
    identity:
      oid: "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
      installation_key: "YOUR_LC_INSTALLATION_KEY_ZENDESK"
    hostname: "zendesk-support-adapter"
    platform: "json"
    sensor_seed_key: "zendesk-audit-sensor"
    mapping:
      sensor_hostname_path: "actor_name"
      event_type_path: "action"
      event_time_path: "created_at"
    indexing: []
```

## API Doc

See the official [documentation](https://developer.zendesk.com/api-reference/ticketing/account-configuration/audit_logs/#list-audit-logs).

---

# Query & Search

# Building Reports with BigQuery + Looker Studio

LimaCharlie does not include reporting by default, however our granular and customizable [Output](/v2/docs/outputs) options allow you to push data to any source and use third-party tools for reporting. In this tutorial, we'll push a subset of LimaCharlie EDR telemetry to [BigQuery](https://cloud.google.com/bigquery) and analyze our data using Google's [Looker Studio](https://lookerstudio.google.com/). We'll be doing the work in the web UI, however this could also be done via the API.

For this example, we will aggregate and analyze Windows processes making network connections.

## Preparing BigQuery

Within your project of choice, begin by creating a new dataset. For the purposes of this tutorial, I'm going to create a dataset named `windows_process_details`. Within this dataset, I'll create a table named `network_connections`.

Let's examine this hierarchy for a moment:

```
 limacharlie-bq-testing    # project
    windows_process_details    # dataset
       network_connections    # table
```

The nice part about this type of hierarchy is that I can build out multiple tables of process details within the same dataset, and then link/analyze them as needed. We'll focus on the `network_connections` data for now, but we could also look at exporting other process details into the same dataset.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2897%29.png)

Within the Google Cloud Console, we also want to create a Service Account and gather an API key. More details on that can be found [here](https://cloud.google.com/iam/docs/service-accounts-create).

Copy the API key and keep it somewhere safe, we'll need to configure it in the output.

## Creating the BigQuery Output

Creating an Output within LimaCharlie is straightforward. Navigate to `Outputs` in the web UI, select `Add Output`, and select `Events`.

> **Note:**
> We want to export raw events in this case - however, we'll use filters to export only the events of interest to BigQuery.

Within the Output Destination menu, select `Google Cloud BigQuery`. You'll be prompted with a configuration menu; expand the `Advanced Options`, as we'll need those too.

The following values must be provided in order for the Output to work:

* Name (choose your own name)
* Dataset (from the previous section)
* Table (from the previous section)
* Project (from the previous section)
* Secret Key (the API key from the GCP service account)

> **Where to Store the Secret?**
> The secret key for this output can be inserted directly in the web app helper, however we recommend keeping secrets in the [Secret hive](/v2/docs/config-hive-secrets) for centralized management.

Within the `Advanced Options`, we'll need to provide the following details:

* Custom Transform - we don't want to include *all* the details from the `NETWORK_CONNECTIONS` event. For this output, we are interested in processes making network connections and the users associated with them. Thus, we'll apply the following transform to pare this down:

```
{
  "hostname": "routing.hostname",
  "command_line": "event.COMMAND_LINE",
  "user": "event.USER_NAME"
}
```

Within the `Specific Event Types` field, we'll specify only `NETWORK_CONNECTIONS`. This is another way to pare down the number of events processed and exported.

Finally, we'll also specify a tag of `windows`, ensuring we only capture Windows systems (per our tagging - your tags may differ). Based on the values provided and discussed, here's a screenshot of the Output configuration (minus the API key):

![image](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/output-config.png)

Save the output details, and then check `View Samples` in the Outputs menu to see if you're successfully seeing events.

![image](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/output-sample.png)

## Analyzing Events in BigQuery + Looker Studio

Navigating back to BigQuery, we can see some initial events flowing in:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28102%29.png)

Let's hop over to Looker Studio. Create a Blank Report, and select `BigQuery` in the `Connect to Data` menu.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28103%29.png)

Select the Project, Dataset, and Table of interest, and click `Add`.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28104%29.png)

Looker Studio may prompt you about permissions of connected data. However, once connected, we'll be able to see a starter table with aggregate details from our `network_connections` table.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28105%29.png)

And that's it! From here, you can manipulate and move around the data as needed. You can also blend with another table, allowing you to combine multiple data points.

Reports can also be styled, additional statistics generated, etc. The following example continues to pull on the basic data we exported to provide some unique insights:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28106%29.png)

---

# Google Cloud BigQuery

Output events and detections to a Google Cloud BigQuery Table.

For a practical use case of this output, see this [tutorial on pushing Velociraptor data to BigQuery](/v2/docs/velociraptor-to-bigquery).

## Configuration Parameters

* `schema`: describes the column names, data types, and other information; should match the text-formatted schema from bigquery
* `table`: the table name where to send data.
* `dataset`: the dataset name where to send data.
* `project`: the project name where to send the data.
* `secret_key`: the secret json key identifying a service account.
* `sec_per_file`: the number of seconds after which a batch of data is loaded.
* `custom_transform`: should align with the schema fields/formats

## Example

```
schema: event_type:STRING, oid:STRING, sid:STRING
table: alerts
dataset: limacharlie_data
project: lc-example-analytics
secret_key: {
  "type": "service_account",
  "project_id": "my-lc-data",
  "private_key_id": "11b6f4173dedabcdefb779e4afae6d88ddce3cc1",
  "private_key": "-----BEGIN PRIVATE KEY-----\n.....\n-----END PRIVATE KEY-----\n",
  "client_email": "my-service-writer@my-lc-data.iam.gserviceaccount.com",
  "client_id": "102526666608388828174",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/my-service-writer%40my-lc-data.iam.gserviceaccount.com"
}
custom_transform: |-
  {
    "oid":"routing.oid",
    "sid":"routing.sid",
    "event_type":"routing.event_type"
  }
```

---

# Google Cloud BigQuery

Output events and detections to a Google Cloud BigQuery Table.

For a practical use case of this output, see this [tutorial on pushing Velociraptor data to BigQuery](/v2/docs/velociraptor-to-bigquery).

## Configuration Parameters

* `schema`: describes the column names, data types, and other information; should match the text-formatted schema from bigquery
* `table`: the table name where to send data.
* `dataset`: the dataset name where to send data.
* `project`: the project name where to send the data.
* `secret_key`: the secret json key identifying a service account.
* `sec_per_file`: the number of seconds after which a batch of data is loaded.
* `custom_transform`: should align with the schema fields/formats

## Example

```
schema: event_type:STRING, oid:STRING, sid:STRING
table: alerts
dataset: limacharlie_data
project: lc-example-analytics
secret_key: {
  "type": "service_account",
  "project_id": "my-lc-data",
  "private_key_id": "11b6f4173dedabcdefb779e4afae6d88ddce3cc1",
  "private_key": "-----BEGIN PRIVATE KEY-----\n.....\n-----END PRIVATE KEY-----\n",
  "client_email": "my-service-writer@my-lc-data.iam.gserviceaccount.com",
  "client_id": "102526666608388828174",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/my-service-writer%40my-lc-data.iam.gserviceaccount.com"
}
custom_transform: |-
  {
    "oid":"routing.oid",
    "sid":"routing.sid",
    "event_type":"routing.event_type"
  }
```

---

# Hayabusa to BigQuery

## Overview

Our BigQuery output allows you to send Hayabusa analysis results to a BigQuery table allowing SQL-like queries against the data. This allows you to perform analysis at scale against massive datasets. For guidance on using Hayabusa within LimaCharlie, see [Hayabusa Extension](/v2/docs/ext-hayabusa).

Imagine you wanted to analyze event logs from 10s, 100s, or 1000s of systems using Hayabusa. You have a couple options:

1. Send the resulting CSV artifact to another platform, like [Timesketch](https://timesketch.org/), for further analysis, as the CSV generated by Hayabusa in LimaCharlie is compatible with Timesketch
2. Run queries against all of the data returned by Hayabusa in BigQuery

BigQuery dataset containing Hayabusa results:
![Screenshot 2024-02-27 10.50.46 AM.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Screenshot%202024-02-27%2010.50.46%20AM.png)

### Steps to Accomplish

1. You will need a Google Cloud project
2. You will need to create a service account within your Google Cloud project

   1. Navigate to your project
   2. Navigate to IAM
   3. Navigate to Service Accounts > Create Service Account
   4. Click on newly created Service Account and create a new key

      1. ![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28188%29.png)
      2. This will provide you with the JSON format secret key you will later setup in your LimaCharlie output.
   5. In BigQuery, create a Dataset, Table, & Schema similar to the screenshot below. Keep in mind, the name of your dataset and table are arbitrary but they need to match what you configure in your output in LimaCharlie.

      1. Project - `your_project_name`
      2. Dataset - `hayabusa`
      3. Table - `hayabusa`
      4. Schema - `computer:STRING, message:STRING, timestamp:STRING, details:STRING, channel:STRING, event_id:STRING, level:STRING, mitre_tactics:STRING, mitre_tags:STRING, extra:STRING`

         1. Note that this can be any of the fields from the Hayabusa event that you wish to use. **This schema and transform are based on the CSV output using the** `timesketch-verbose` **profile.**
3. Now we're ready to create our LimaCharlie Events Output

   1. In the side navigation menu, click "Outputs" then add a new ouput

      1. **Output stream**: Events
      2. **Destination**: Google Cloud BigQuery

         1. **Name**: `hayabusa-bigquery`

            1. You can change this, but it affects a subsequent step so take note of the output name
         2. **schema**: `computer:STRING, message:STRING, timestamp:STRING, details:STRING, channel:STRING, event_id:STRING, level:STRING, mitre_tactics:STRING, mitre_tags:STRING, extra:STRING`

            1. Note that this can be any of the fields from the Hayabusa event that you wish to use. **This schema and transform are based on the CSV output using the** `timesketch-verbose` **profile.**
         3. **Dataset**: *whatever you named BQ your dataset above*
         4. **Table**: *whatever you named your BQ table above*
         5. **Project**: *your* GCP *project name*
         6. **Secret Key**: *provide the JSON secret key for your GCP service account*
         7. **Advanced Options**

            1. **Custom Transform**: paste in this JSON

               1. Note that this can be any of the fields from the Hayabusa event that you wish to use. **This schema and transform are based on the CSV output using the** `timesketch-verbose` **profile.**

               ```
               {
               "channel": "event.results.Channel",
               "computer": "event.results.Computer",
               "message": "event.results.message",
               "timestamp": "event.results.datetime",
               "details": "event.results.Details",
               "event_id": "event.results.EventID",
               "level": "event.results.Level",
               "mitre_tactics": "event.results.MitreTactics",
               "mitre_tags": "event.results.MitreTags",
               "extra": "event.results.ExtraFieldInfo",
               }
               ```
            2. **Specific Event Types**: `hayabusa_event`
            3. **Sensor**: `ext-hayabusa`
4. You are now ready to send Hayabusa events to BigQuery!

---

# LCQL Examples

LimaCharlie Query Language (LCQL) lets you write well-structured queries to search across telemetry within LimaCharlie. The following examples can help you perform targeted searches or hunts across your telemetry, as well as modify them to build your own. Example queries are sorted by *source*, however can be adjusted for your environment.

## General Queries

Search *all* event types across *all* Windows systems for a particular string showing up in *any* field.
```
-24h | plat == windows | * | event/* contains 'psexec'
```

## GitHub Telemetry

GitHub logs can be an excellent source of telemetry to identify potential repository or account abuse or misuse. When ingested properly, GitHub log data can be observed via `plat == github`.

### GitHub Protected Branch Override

Show me all the GitHub branch protection override (force pushing to repo without all approvals) in the past 12h that came from a user outside the United States, with the repo, user and number of infractions.

```
-12h | plat == github | protected_branch.policy_override | event/public_repo is false and event/actor_location/country_code is not "us" | event/repo as repo event/actor as actor COUNT(event) as count GROUP BY(repo actor)
```

which could result in:

```
| actor    |   count | repo                               |
|----------|---------|------------------------------------|
| mXXXXXXa |      11 | acmeCorpCodeRep/customers          |
| aXXXXXXb |      11 | acmeCorpCodeRep/analysis           |
| cXXXXXXd |       3 | acmeCorpCodeRep/devops             |
```

## Network Telemetry

Network details recorded on endpoints, such as new connections or DNS requests, allow for combined insight. We can also query this data for aggregate details, and display data in an easily-consumed manner.

### Domain Count

Show me all domains resolved by Windows hosts that contain "google" in the last 10 minutes and the number of times each was resolved.

```
-10m | plat == windows | DNS_REQUEST | event/DOMAIN_NAME contains 'google' | event/DOMAIN_NAME as domain COUNT(event) as count GROUP BY(domain)
```

which could result in:

```
|   count | domain                     |
|---------|----------------------------|
|      14 | logging.googleapis.com     |
|      36 | logging-alv.googleapis.com |
```

### Domain Prevalence

Show me all domains resolved by Windows hosts that contain "google" in the last 10 minutes and the number of unique Sensors that have resolved them.

```
-10m | plat == windows | DNS_REQUEST | event/DOMAIN_NAME contains 'google' | event/DOMAIN_NAME as domain COUNT_UNIQUE(routing/sid) as count GROUP BY(domain)
```

which could result in:

```
|   count | domain                     |
|---------|----------------------------|
|       4 | logging.googleapis.com     |
|       3 | logging-alv.googleapis.com |
```

## Process Activity

### Unsigned Binaries

Grouped and counted.
```
-24h | plat == windows | CODE_IDENTITY | event/SIGNATURE/FILE_IS_SIGNED != 1 | event/FILE_PATH as Path event/HASH as Hash event/ORIGINAL_FILE_NAME as OriginalFileName COUNT_UNIQUE(Hash) as Count GROUP BY(Path Hash OriginalFileName)
```

### Process Command Line Args

```
-1h | plat == windows | NEW_PROCESS EXISTING_PROCESS | event/COMMAND_LINE contains "psexec" | event/FILE_PATH as path event/COMMAND_LINE as cli routing/hostname as host
```

### Stack Children by Parent

```
-12h | plat == windows | NEW_PROCESS | event/PARENT/FILE_PATH contains "cmd.exe" | event/PARENT/FILE_PATH as parent event/FILE_PATH as child COUNT_UNIQUE(event) as count GROUP BY(parent child)
```

## Windows Event Log (WEL)

When ingested with EDR telemetry, or as a separate Adapter, `WEL` type events are easily searchable via LimaCharlie. Sample queries are organized alphabetically, with threat/technique details provided where applicable.

### %COMSPEC% in Service Path

```
-12h | plat == windows | WEL | event/EVENT/System/EventID == "7045" and event/EVENT/EventData/ImagePath contains "COMSPEC"
```

### Overpass-the-Hash

```
-12h | plat == windows | WEL | event/EVENT/System/EventID == "4624" and event/EVENT/EventData/LogonType == "9" and event/EVENT/EventData/AuthenticationPackageName == "Negotiate" and event/EVENT/EventData/LogonProcess == "seclogo"
```

### Taskkill from a Non-System Account

*Requires process auditing to be enabled*

```
-12h | plat == windows | WEL | event/EVENT/System/EventID == "4688" and event/EVENT/EventData/NewProcessName contains "taskkill" and event/EVENT/EventData/SubjectUserName not ends with "!"
```

### Logons by Specific LogonType

```
-24h | plat == windows | WEL | event/EVENT/System/EventID == "4624" AND event/EVENT/EventData/LogonType == "10"
```

### Stack/Count All LogonTypes by User

```
-24h | plat == windows | WEL | event/EVENT/System/EventID == "4624" | event/EVENT/EventData/LogonType AS LogonType event/EVENT/EventData/TargetUserName as UserName COUNT_UNIQUE(event) as Count GROUP BY(UserName LogonType)
```

### Failed Logons

```
-1h | plat==windows | WEL | event/EVENT/System/EventID == "4625" | event/EVENT/EventData/IpAddress as SrcIP event/EVENT/EventData/LogonType as LogonType event/EVENT/EventData/TargetUserName as Username event/EVENT/EventData/WorkstationName as SrcHostname
```

---

# LimaCharlie Query Language

LimaCharlie Query Language (LCQL) provides a flexible, intuitive and interactive way to explore your data in LimaCharlie. Telemetry ingested via EDR sensors or adapters are searchable via LCQL, and can be searched en masse. Sample use cases for LCQL include:

* Analyze your entire, multi-platform fleet for network connections of interest.
* Search across all Windows Event Logs for unique user activity.
* Look at all Linux systems for specific package installation events.
* Analyze all volume mounts and unmounts on macOS devices
* And many more!!!

The steps below walk you through creating your own LCQL queries. If you're looking for samples or LCQL inspiration, check out our [LCQL Examples](/v2/docs/lcql-examples) page.

> **Beta Feature**: LCQL is currently in Beta, and features may change in the future.

## Building LimaCharlie Queries

LCQL queries contain 4 components with a 5th optional one, each component is separated by a pipe (`|`):

1. **Timeframe**: the time range the query applies to. This can be either a single offset in the past like `-1h` or `-30m`. Or it can be a date time range like `2022-01-22 10:00:00 to 2022-01-25 14:00:00`.

   Note: the time frame is still used in the CLI and API, but no longer exposed in the UI; use the time selector control instead.

2. **Sensor selector**: the set of sensors to query. This can be either `*` for all sensors, or a [Sensor Selector expression](/v2/docs/reference-sensor-selector-expressions), like `plat == windows` or `hostname == foo.com or hostname == bar.com` (Note: a full list of platform types can be found in the [ID Schema Reference](/v2/docs/reference-id-schema))

3. **Event type**: the event types to include in the query. Use `or` to search for multiple events at once, for example `NEW_PROCESS or DNS_REQUEST`, or a `*` to go over all event types.

4. **Filters**: the actual query filters. The filters are a series of statements combined with "and" and "or" that can be associated with parenthesis (`()`). String literals, when used, can be double-quoted to be case insensitive or single-quoted to be case sensitive. Selectors behave like rules, for example: `event/FILE_PATH`.

   The [Query Console UI](/v2/docs/query-console-ui) provides a type-ahead assistance to bring up the available operators and help design the query.

5. **Projection (optional)**: a list of fields you would like to extract from the results with a possible alias, like: `event/FILE_PATH as path event/USER_NAME AS user_name event/COMMAND_LINE`. The Projection can also support a grouping functionality by adding `GROUP BY(field1 field2 ...)` at the end of the projection statement.

   When grouping, all fields being projected must either be in the `GROUP BY` statement, or have an aggregator modifier. An aggregator modifier is, for example, `COUNT( host )` or `COUNT_UNIQUE( host )` instead of just `host`.

   A full example with grouping is:

   `-1h | * | DNS_REQUEST | event/DOMAIN_NAME contains "apple" | event/DOMAIN_NAME as dns COUNT_UNIQUE(routing/hostname) as hostcount GROUP BY(dns host)`

   which would give you the number of hosts having resolved a domain containing `apple`, grouped by domain.

> **Projection Syntax**: Note: There is no space between `BY` and the `(` opening of the parentheses in a projection.
>
> Example: `GROUP BY(dns host)` or `COUNT_UNIQUE(routing/hostname)`

All of this can result in a query like:

`-30m | plat == windows | NEW_PROCESS | event/COMMAND_LINE contains "powershell" and event/FILE_PATH not contains "powershell" | event/COMMAND_LINE as cli event/FILE_PATH as path routing/hostname as host`

OR

`-30m | plat == windows | * | event/COMMAND_LINE contains "powershell" and event/FILE_PATH not contains "powershell"`

---

# Query Console UI

Many critical security operations require a query console with strong search functionality. It enables analysts to query large volumes of telemetry, logs, and events for investigations, hunting, and incident response.

LimaCharlie's Query Console (with integrated Search) brings this functionality to the SecOps Cloud Platform. We've combined familiar query workflows with features like type-ahead syntax, time range selection, and detection-as-code conversion. This lets your team quickly investigate alerts, analyze data across tenants, and extend LimaCharlie into other modern SIEM use cases.

All events ingested into LimaCharlie are retained and available for analysis. Data is parsed at ingest and saved in LimaCharlie hot storage. Search queries this large volume of telemetry for matches within the time frame you provide. Searches are billed based on the number of events scanned (measured in millions). See Pricing for specifics.

## Permissions

To view and operate the Query Console, the following permissions are required:

* `insight.evt.get` for search
* `org.get` for schema service access
* `query.set` for saving queries
* `query.get` for reading a list of queries (if you don't have this set you will see an error saying you need `query.get.mtd`, but this is the permission you need)
* `query.del` for editing or deleting queries (editing is creating a new one and removing the old one)

## UI Element Overview

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(338).png)

1. **Source:** Select Events (everything that had been injected from endpoints and XDR sources, default), Detections, or Platform Audit events as the data source for the search.

2. **Query editor:** Enter a LimaCharlie Query Language (LCQL) query to include:

   1. *Sensor Selector -* precisely define the sensors that produced the desired events.
   2. *Event Type* - filter results to only return specific types of events.
   3. Filter - the actual query filter using individual fields and operations on top of them.
   4. Projections (optional) - control output columns, sort results via `ORDER BY` and/or aggregate the data with `GROUP BY`, `COUNT`, `COUNT_UNIQUE` and more. See LCQL reference and Examples for details.

3. **Time period:** Set the searchable time period using three options: last [time period], around [time frame], and absolute "from startto finish".

   ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(340).png)

   * Enter a time `16:00`, or day and time `2025-01-16 08:52:54`, using most common time formats. For example:

     + From `33m` to `now` - last 33 minutes
     + Around `2025-01-16 08:52:54` +- `15 minutes` - 15 minutes before and after the specified time stamp
     + From `10am` to `1:30pm`

   **Note:** All times are shown according to the timezone selected by the user in User Settings.

4. **Available Fields:** Managed data exploration

   1. Schema fields - a list of all the fields associated with ingested events.
   2. Event types - event types present in the returned portion of the query. As more data is churned to complete the specified time frame more event types may appear.
   3. Query fields - event fields present in the *portion of the result already fetched by the query*, with a count of total occurrences. Clicking on the event field opens a details panel. From here you can add a term to the query.

      ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(341).png)
   4. Table columns: control the columns displayed in Table View.

   Note: While the schema fields are always available, the event types and query fields are only shown for portion of the time frame *searched so far*. As more data is churned in the background (to complete your selected time frame), more event types and fields may appear.

5. **Query status:** Shows the state of your query in real time, highlighting any existing syntax errors or providing a cost estimate if the query is properly formed.

   As the query runs the status displays progress, query status, and a running total of the cost accrued.

   *Query cost estimation:* Queries are charged by the amount of data churned, measured and billed per one million events evaluated. This estimation shows the "at most" cost of a query for the selected time range. Only retrieved data is chargeable.

   *Performance tuning:* The better tuned the query, the faster the search and lower the cost. Using Sensor Selector and Event Type to precisely target the desired telemetry will increase search speeds and lower costs.

6. **Histogram:**

   When a search is run, a histogram appears below the query field showing the distribution of events over time. The portion with a vertical bar chart represents results that have been retrieved so far. The non-bar chart portion shows the total number of events in the selected time frame. The histogram shows the progress of the search through the time frame. As you paginate through the search, more events are evaluated, and more bars appear to signify the progress through the time frame.

7. **Search results:** displays results in two views, **timeline** and **table**. Timeline view shows matching events with the most recent on top. Table view provides a way to sort results into desired columns. Find the desired field in Query Fields and use the `pin` icon to add it as a column.

   1. A **Tab Columns** section appears in the **Fields** sidebar when table view is selected. Columns can be viewed or removed here.
   2. **Event Details** allows you to click on an event and perform applicable event actions like **Build a D&R Rule**.
   3. **Download** all the events you've retrieved in a [.ndjson format](https://github.com/ndjson/ndjson-spec). The automatic download of the entire time range is coming soon.

   ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(342).png)

8. **Saving Queries and Query Library.** A query can be saved in your private user library or shared via an org library. Use the library to browse queries and load the desired one to the query editor.

---

# Query Console UI

Many critical security operations require a query console with strong search functionality. It enables analysts to query large volumes of telemetry, logs, and events for investigations, hunting, and incident response.

LimaCharlie's Query Console (with integrated Search) brings this functionality to the SecOps Cloud Platform. We've combined familiar query workflows with features like type-ahead syntax, time range selection, and detection-as-code conversion. This lets your team quickly investigate alerts, analyze data across tenants, and extend LimaCharlie into other modern SIEM use cases.

All events ingested into LimaCharlie are retained and available for analysis. Data is parsed at ingest and saved in LimaCharlie hot storage. Search queries this large volume of telemetry for matches within the time frame you provide. Searches are billed based on the number of events scanned (measured in millions). See Pricing for specifics.

## Permissions

To view and operate the Query Console, the following permissions are required:

* `insight.evt.get` for search
* `org.get` for schema service access
* `query.set` for saving queries
* `query.get` for reading a list of queries (if you don't have this set you will see an error saying you need `query.get.mtd`, but this is the permission you need)
* `query.del` for editing or deleting queries (editing is creating a new one and removing the old one)

## UI Element Overview

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(338).png)

1. **Source:** Select Events (everything that had been injected from endpoints and XDR sources, default), Detections, or Platform Audit events as the data source for the search.

2. **Query editor:** Enter a LimaCharlie Query Language (LCQL) query to include:
   * *Sensor Selector* - precisely define the sensors that produced the desired events.
   * *Event Type* - filter results to only return specific types of events.
   * *Filter* - the actual query filter using individual fields and operations on top of them.
   * *Projections* (optional) - control output columns, sort results via `ORDER BY` and/or aggregate the data with `GROUP BY`, `COUNT`, `COUNT_UNIQUE` and more. See LCQL reference and Examples for details.

3. **Time period:** Set the searchable time period using three options: last [time period], around [time frame], and absolute "from startto finish".

   ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(340).png)

   * Enter a time `16:00`, or day and time `2025-01-16 08:52:54`, using most common time formats. For example:
     * From `33m` to `now` - last 33 minutes
     * Around `2025-01-16 08:52:54` +- `15 minutes` - 15 minutes before and after the specified time stamp
     * From `10am` to `1:30pm`

   **Note:** All times are shown according to the timezone selected by the user in User Settings.

4. **Available Fields:** Managed data exploration
   * *Schema fields* - a list of all the fields associated with ingested events.
   * *Event types* - event types present in the returned portion of the query. As more data is churned to complete the specified time frame more event types may appear.
   * *Query fields* - event fields present in the *portion of the result already fetched by the query*, with a count of total occurrences. Clicking on the event field opens a details panel. From here you can add a term to the query.

     ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(341).png)

   * *Table columns*: control the columns displayed in Table View.

   **Note:** While the schema fields are always available, the event types and query fields are only shown for portion of the time frame *searched so far*. As more data is churned in the background (to complete your selected time frame), more event types and fields may appear.

5. **Query status:** Shows the state of your query in real time, highlighting any existing syntax errors or providing a cost estimate if the query is properly formed.

   As the query runs the status displays progress, query status, and a running total of the cost accrued.

   *Query cost estimation:* Queries are charged by the amount of data churned, measured and billed per one million events evaluated. This estimation shows the "at most" cost of a query for the selected time range. Only retrieved data is chargeable.

   *Performance tuning:* The better tuned the query, the faster the search and lower the cost. Using Sensor Selector and Event Type to precisely target the desired telemetry will increase search speeds and lower costs.

6. **Histogram:**

   When a search is run, a histogram appears below the query field showing the distribution of events over time. The portion with a vertical bar chart represents results that have been retrieved so far. The non-bar chart portion shows the total number of events in the selected time frame. The histogram shows the progress of the search through the time frame. As you paginate through the search, more events are evaluated, and more bars appear to signify the progress through the time frame.

7. **Search results:** displays results in two views, **timeline** and **table**. Timeline view shows matching events with the most recent on top. Table view provides a way to sort results into desired columns. Find the desired field in Query Fields and use the `pin` icon to add it as a column.
   * A **Tab Columns** section appears in the **Fields** sidebar when table view is selected. Columns can be viewed or removed here.
   * **Event Details** allows you to click on an event and perform applicable event actions like **Build a D&R Rule**.
   * **Download** all the events you've retrieved in a [.ndjson format](https://github.com/ndjson/ndjson-spec). The automatic download of the entire time range is coming soon.

   ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(342).png)

8. **Saving Queries and Query Library.** A query can be saved in your private user library or shared via an org library. Use the library to browse queries and load the desired one to the query editor.

---

# Velociraptor to BigQuery

## Overview

Our BigQuery output allows you to send Velociraptor hunt results to a BigQuery table allowing SQL-like queries against the hunt data. This is very similar to using [Velociraptor notebooks](https://docs.velociraptor.app/docs/vql/notebooks/), allowing you to perform hunt analysis at scale against massive datasets. For guidance on using LimaCharlie to execute Velociraptor hunts, see [Velociraptor Extension](/v2/docs/ext-velociraptor).

Imagine you wanted to obtain running processes from 10s, 100s, or 1000s of systems using Velociraptor. You could easily issue a `Windows.System.Pslist` hunt across these systems, and let LimaCharlie push Velociraptor to the endpoints and collect the results. The issue is, if you want to run queries against all of the data returned by the hunts, you'll need a database-like tool to do that which is where BigQuery comes in.

BigQuery dataset containing Velociraptor hunt results:
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28186%29.png)

### Steps to Accomplish

1. You will need a Google Cloud project
2. You will need to create a service account within your Google Cloud project

   1. Navigate to your project
   2. Navigate to IAM
   3. Navigate to Service Accounts > Create Service Account
   4. Click on newly created Service Account and create a new key

      1. ![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28188%29.png)
      2. This will provide you with the JSON format secret key you will later setup in your LimaCharlie output
   5. In BigQuery, create a Dataset, Table, & Schema similar to the screenshot below

      1. ![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28189%29.png)
3. Now we're ready to create our LimaCharlie tailored output

   1. In the side navigation menu, click "Outputs" then add a new output

      1. **Output stream**: Tailored
      2. **Destination**: Google Cloud BigQuery

         1. **Name**: `bigquery-tailored`

            1. You can change this, but it affects a subsequent step so take note of the output name
         2. **schema**: `sid:STRING, job_id:STRING, artifact:JSON`
         3. **Dataset**: *whatever you named BQ your dataset above*
         4. **Table**: *whatever you named your BQ table above*
         5. **Project**: *your GCP project name*
         6. **Secret Key**: *provide the JSON secret key for your GCP service account*
         7. **Advanced Options**

            1. **Custom Transform**: paste in this JSON

               ```
               {
               "sid": "event.sid",
               "job_id": "event.job_id",
               "artifact": "{{ json .event.collection }}"
               }
               ```
            2. **Specific Event Types**: `velociraptor_collection`
      3. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor.png)
4. We now need a  rule that will watch for Velociraptor collections send send them to the new tailored output

   1. Create a new D&R rule

      1. Detection

         ```
         event: velociraptor_collection
         op: exists
         path: event/collection
         ```
      2. Response

         ```
         - action: output
           name: bigquery-tailored # must match the output name you created earlier
         - action: report
           name: Velociraptor hunt sent to BigQuery
         ```
5. You are now ready to send Velociraptor hunts to BigQuery!

## BigQuery Tips

### Query Examples

Once the data arrives in BigQuery, it will be in three simple columns: `sid`, `job_id`, and `artifact`. The `artifact` column contains the raw JSON of the hunt results from each sensor that returned results.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28191%29.png)

Let's say we wanted to split out all results of a `Windows.System.Pslist` hunt so that each process, from each system, is returned in it's own row. Here is an example notebook to accomplish this:

```
SELECT
  sid,
  json_extract_scalar(obj, '$.Name') as Name,
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Authenticode,
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256,
  json_extract_scalar(obj, '$.Pid') as Pid,
  json_extract_scalar(obj, '$.Ppid') as Ppid,
  json_extract_scalar(obj, '$.Username') as Username
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
LIMIT 1000
```

Be sure to swap out `lc-demo-infra.velociraptor.hunts` for your own `project.dataset.table` names.

This results in the following view of our data
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28200%29.png)

Suppose we wanted to perform some stacking analysis to identify the rarest combinations of `Exe` and `CommandLine`; the following query could help:

```
SELECT
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  COUNT(*) as Count
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
GROUP BY
  Exe,
  CommandLine
ORDER BY
  Count ASC
```

This results in the following view of our data
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28201%29.png)

Now let's say you wanted to look for only processes that are `Authenticode` = `untrusted`, you would use a query such as this:

```
SELECT
  sid,
  json_extract_scalar(obj, '$.Name') as Name,
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Authenticode,
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256,
  json_extract_scalar(obj, '$.Pid') as Pid,
  json_extract_scalar(obj, '$.Ppid') as Ppid,
  json_extract_scalar(obj, '$.Username') as Username
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
WHERE
  json_extract_scalar(obj, '$.Authenticode.Trusted') = 'untrusted'
LIMIT 1000
```

### WHERE Filters for Specific Conditions

Here are some brief examples of `WHERE` statements to perform specific filtering.

#### String presence

This example checks for the presence of a string `mimikatz` appearing anywhere within `CommandLine`

```
WHERE
  STRPOS(json_extract_scalar(obj, '$.CommandLine'), 'mimikatz') > 0 AND
```

#### Compare integers

This example checks for the presence of an integer `0` in a numeric field `GroupID`

```
WHERE
  CAST(json_extract_scalar(obj, '$.GroupID') AS INT64) = 0
```

### Parsing Nested JSON Objects

In the `Windows.System.Pslist` examples above, there are a few columns which contain nested JSON such as `Authenticode` and `Hash`. To expand these objects in their entirety in the corresponding column/row, we'd write a query like this:

```
SELECT
  json_extract(obj, '$.Authenticode') as Authenticode, # json_extract to unpack nested json
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Trusted,
  json_extract(obj, '$.Hash') as Hashes, # json_extract to unpack nested json
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256, # extract a specific field from the nested json
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
LIMIT 1000
```

See the output of this query below:
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28202%29.png)

---

# Velociraptor to BigQuery

## Overview

Our BigQuery output allows you to send Velociraptor hunt results to a BigQuery table allowing SQL-like queries against the hunt data. This is very similar to using [Velociraptor notebooks](https://docs.velociraptor.app/docs/vql/notebooks/), allowing you to perform hunt analysis at scale against massive datasets. For guidance on using LimaCharlie to execute Velociraptor hunts, see [Velociraptor Extension](/v2/docs/ext-velociraptor).

Imagine you wanted to obtain running processes from 10s, 100s, or 1000s of systems using Velociraptor. You could easily issue a `Windows.System.Pslist` hunt across these systems, and let LimaCharlie push Velociraptor to the endpoints and collect the results. The issue is, if you want to run queries against all of the data returned by the hunts, you'll need a database-like tool to do that which is where BigQuery comes in.

BigQuery dataset containing Velociraptor hunt results:
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28186%29.png)

### Steps to Accomplish

1. You will need a Google Cloud project
2. You will need to create a service account within your Google Cloud project

   1. Navigate to your project
   2. Navigate to IAM
   3. Navigate to Service Accounts > Create Service Account
   4. Click on newly created Service Account and create a new key

      1. ![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28188%29.png)
      2. This will provide you with the JSON format secret key you will later setup in your LimaCharlie output
   5. In BigQuery, create a Dataset, Table, & Schema similar to the screenshot below

      1. ![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28189%29.png)
3. Now we're ready to create our LimaCharlie tailored output

   1. In the side navigation menu, click "Outputs" then add a new output

      1. **Output stream**: Tailored
      2. **Destination**: Google Cloud BigQuery

         1. **Name**: `bigquery-tailored`

            1. You can change this, but it affects a subsequent step so take note of the output name
         2. **schema**: `sid:STRING, job_id:STRING, artifact:JSON`
         3. **Dataset**: *whatever you named BQ your dataset above*
         4. **Table**: *whatever you named your BQ table above*
         5. **Project**: *your GCP project name*
         6. **Secret Key**: *provide the JSON secret key for your GCP service account*
         7. **Advanced Options**

            1. **Custom Transform**: paste in this JSON

               ```
               {
               "sid": "event.sid",
               "job_id": "event.job_id",
               "artifact": "{{ json .event.collection }}"
               }
               ```
            2. **Specific Event Types**: `velociraptor_collection`
      3. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/velociraptor.png)
4. We now need a  rule that will watch for Velociraptor collections send send them to the new tailored output

   1. Create a new D&R rule

      1. Detection

         ```
         event: velociraptor_collection
         op: exists
         path: event/collection
         ```
      2. Response

         ```
         - action: output
           name: bigquery-tailored # must match the output name you created earlier
         - action: report
           name: Velociraptor hunt sent to BigQuery
         ```
5. You are now ready to send Velociraptor hunts to BigQuery!

## BigQuery Tips

### Query Examples

Once the data arrives in BigQuery, it will be in three simple columns: `sid`, `job_id`, and `artifact`. The `artifact` column contains the raw JSON of the hunt results from each sensor that returned results.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28191%29.png)

Let's say we wanted to split out all results of a `Windows.System.Pslist` hunt so that each process, from each system, is returned in it's own row. Here is an example notebook to accomplish this:

```
SELECT
  sid,
  json_extract_scalar(obj, '$.Name') as Name,
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Authenticode,
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256,
  json_extract_scalar(obj, '$.Pid') as Pid,
  json_extract_scalar(obj, '$.Ppid') as Ppid,
  json_extract_scalar(obj, '$.Username') as Username
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
LIMIT 1000
```

Be sure to swap out `lc-demo-infra.velociraptor.hunts` for your own `project.dataset.table` names.

This results in the following view of our data
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28200%29.png)

Suppose we wanted to perform some stacking analysis to identify the rarest combinations of `Exe` and `CommandLine`; the following query could help:

```
SELECT
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  COUNT(*) as Count
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
GROUP BY
  Exe,
  CommandLine
ORDER BY
  Count ASC
```

This results in the following view of our data
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28201%29.png)

Now let's say you wanted to look for only processes that are `Authenticode` = `untrusted`, you would use a query such as this:

```
SELECT
  sid,
  json_extract_scalar(obj, '$.Name') as Name,
  json_extract_scalar(obj, '$.Exe') as Exe,
  json_extract_scalar(obj, '$.CommandLine') as CommandLine,
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Authenticode,
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256,
  json_extract_scalar(obj, '$.Pid') as Pid,
  json_extract_scalar(obj, '$.Ppid') as Ppid,
  json_extract_scalar(obj, '$.Username') as Username
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
WHERE
  json_extract_scalar(obj, '$.Authenticode.Trusted') = 'untrusted'
LIMIT 1000
```

### WHERE Filters for Specific Conditions

Here are some brief examples of `WHERE` statements to perform specific filtering.

#### String presence

This example checks for the presence of a string `mimikatz` appearing anywhere within `CommandLine`

```
WHERE
  STRPOS(json_extract_scalar(obj, '$.CommandLine'), 'mimikatz') > 0 AND
```

#### Compare integers

This example checks for the presence of an integer `0` in a numeric field `GroupID`

```
WHERE
  CAST(json_extract_scalar(obj, '$.GroupID') AS INT64) = 0
```

### Parsing Nested JSON Objects

In the `Windows.System.Pslist` examples above, there are a few columns which contain nested JSON such as `Authenticode` and `Hash`. To expand these objects in their entirety in the corresponding column/row, we'd write a query like this:

```
SELECT
  json_extract(obj, '$.Authenticode') as Authenticode, # json_extract to unpack nested json
  json_extract_scalar(obj, '$.Authenticode.Trusted') as Trusted,
  json_extract(obj, '$.Hash') as Hashes, # json_extract to unpack nested json
  json_extract_scalar(obj, '$.Hash.SHA256') as SHA256, # extract a specific field from the nested json
FROM
  `lc-demo-infra.velociraptor.hunts`,
  UNNEST(json_extract_array(artifact.Windows_System_Pslist)) as obj
LIMIT 1000
```

See the output of this query below:
![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28202%29.png)

---

# Sensors & Agents

# AI Agent Engine [LABS]

The AI Agent Engine Extension allows you to easily codify and execute AI Agents within the context of your Organization with access to the LimaCharlie APIs for investigation, remediation and automation.

The AI Agent definition themselves are managed in the `ai_agent` [Hive](/v2/docs/config-hive) Configurations and can be managed across tenants using the Infrastructure as Code extension. This hive requires the `ai_agent.*` permissions.

The execution of an AI Agent can be triggered through the following means:

1. Interactively in the web app by going to the Extensions section for the AI Agent Engine extension.
2. By issuing an `extension request` action through a [D&R rule](/v2/docs/detection-and-response-examples).
3. By issuing an extension request on the API directly: <https://api.limacharlie.io/static/swagger/#/Extensions/createExtensionRequest>
4. By issuing an extension request through the Python CLI/SDK or Golang SDK, which means they're also available to [Playbooks](/v2/docs/playbook).

This means agents can be invoked in a fully automated fashion based on events, detections, audit messages or any other [target](/v2/docs/detection-on-alternate-targets) of rules. But it can also be used in an ad-hoc fashion triggered manually.

## Usage

When invoking an AI Agent, all you need is the playbook name as defined in Hive and an initial message. Optionally, an AI Agent can also receive a JSON dictionary object as parameters, this is useful when passing the AI Agent additional context like a detection or event from a D&R rule.

Interactions with the agent are associated with a given (Interactive) Session ID (ISID). A Session ID is like a ChatGPT session where all the context is available to the agent. Starting a new session returns an `isid` and the `get_session` action requires an `isid`.

Common tips:

* Specify only the subset of tools you want your AI to use, otherwise it may do things you didn't expect or take initiative in ways you don't intend.
* Make the AI as specialized as possible, tell it exactly what you want it to do, processes and how you want to get the response (markdown, JSON etc).
* Give the AI examples, adding more details and examples to the `instructions` help greatly.

The credentials provided to the engine are simply a LimaCharlie API key, we recommend storing it in a [secret](/v2/docs/config-hive-secrets) and referencing as `hive://secret/my-lc-creds`.

### Actions

#### start_session

Start a new AI Agent session, specifying all the detailed parameters (see AI Agent Structure below that are both the Agent Definition parameters and the `start_session` parameters).

#### list_tools

List all the tools available to be called by the agent along with their categories that can be used to customize agents.

### D&R rule example

Here is an example D&R rule starting a new invocation of a playbook.

```
- action: extension request
  extension name: ext-ai-agent-engine
  extension action: start_session
  extension request:
    agent_definition: '{{ "my-agent-name" }}'
    message: You're a cyber security expert, summarize this detection: {...}
```

### Python example

```
# Import LC SDK
import limacharlie
import json
# Instantiate the SDK with default creds.
lc = limacharlie.Manager()
# Instantiate the Extension manager object.
ext = limacharlie.Extension(lc)

# Issue a request to the "ext-ai-agent-engine" extension for the "my-agent-name" agent.
response = ext.request("ext-ai-agent-engine", "start_session", {
    "agent_definition": "my-agent-name",
    "message": "You're a cyber security expert, summarize this detection: {...}"
})

for msg in response['data']['responses']:
  print(f"AI says: {json.dumps(msg, indent=2)}")
```

## AI Agent structure

#### Example AI Agent Definition

The following is a sample AI Agent definition that simply aims at summarizing detections.

```
{
  "name": "my-agent",
  "description": "Some agent that does something...",
  "credentials": "hive://secret/ai-creds", // These credentials will be used when accessing LimaCharlie APIs.
  // Instructions are the core system behavior for the AI
  "instructions": "You are a cybersecurity expert system who's job it is to summarize detections/alerts for SOC analysts. Output as markdown. Include detailed technical context about the alert and if MITRE techniques are mentioned, summarize them. Also include what next steps of the investigation should be. The audience of the report is a cyber security team at a medium sized enterprise.",
  "max_iterations": 10, // If the AI makes tool calls to the LC API or LC Sensors, this limits the number of iterations the AI is called.
  "allowed_tools": [
    "get_sensor_info" // List of tool categories (see list_tools or the Available Tools section below).
  ]
}
```

### Available Tools

The tools available to the AI Agents are the same ones available from the official [LimaCharlie MCP Server](/v2/docs/mcp-server).

## Infrastructure as Code

Not currently available, coming up.

## Billing

The AI Agent Engine is billed per token processed, including initial messages, prompt and response.

## Privacy

Currently, the model in use is the commercial Gemini models.

Although the models may change (and eventually Bring-Your-Own-Model), these models will never use your data to train more models and LimaCharlie never uses the data to train models.

---

# Adapter Deployment

Adapters can be deployed in one of two ways:

* **On-prem**, Adapters utilize the LC Adapter binary to ingest a data source and forward it to LimaCharlie.
* **Cloud-to-cloud**, connects the LimaCharlie cloud directly with your cloud source and automatically ingests data.

## Which Adapter Do I Use for Cloud Data?

You can use on-prem adapters to forward cloud data, or you could acquire the same data with a cloud-to-cloud connection. So, which one to use?

The answer lies in *how* you want to send your data to LimaCharlie. Are you OK with configuring a connector from our platform, or would you rather use a bastion box in between? Either way works for us!

The data ingested from adapters is parsed/mapped into JSON by LimaCharlie, according to the parameters you provided, unless using a pre-defined format.

## Adapter Binaries

Software-based, or "on-prem" adapters are available in the following formats:

* Binaries:

  + \*nix

    - [AIX ppc64](https://downloads.limacharlie.io/adapter/aix/ppc64)
    - [Linux (Generic) 64-bit](https://downloads.limacharlie.io/adapter/linux/64)
    - [Linux (Generic) arm](https://downloads.limacharlie.io/adapter/linux/arm)
    - [Linux (Generic) arm64](https://downloads.limacharlie.io/adapter/linux/arm64)
    - [FreeBSD 64-bit](https://downloads.limacharlie.io/adapter/freebsd/64)
    - [OpenBSD 64-bit](https://downloads.limacharlie.io/adapter/openbsd/64)
    - [NetBSD 64-bit](https://downloads.limacharlie.io/adapter/netbsd/64)
    - [Solaris 64-bit](https://downloads.limacharlie.io/adapter/solaris/64)
  + macOS

    - [macOS x64](https://downloads.limacharlie.io/adapter/mac/64)
    - [macOS arm64](https://downloads.limacharlie.io/adapter/mac/arm64)
  + Windows

    - [Windows x64](https://downloads.limacharlie.io/adapter/windows/64)
* Docker:

  + <https://hub.docker.com/r/refractionpoint/lc-adapter>

> **Another platform?**
>
> If you need support for a specific platform, or require more information about supported platforms, please [let us know](https://www.limacharlie.io/contact).

## On-Prem + Cloud Management

LimaCharlie Adapters deployed manually (on-prem) also support cloud-based management. This makes the deployment of the adapter extremely easy while also making it easy to update the configs remotely after the fact. This is particularly critical for service providers that may be deploying adapters on customer networks where gaining access to the local adapter may be difficult.

To accomplish this, you need the `externaladapter.*` permissions.

### Preparing

The first step of deploying this way is to create a new External Adapter record. These are found in the `external_adapter` Hive or under the Sensors section of the web app.

The content of an external adapter is exactly the same as a traditional adapter configuration in YAML. It describes what you want your external adapter to do, like collect from file, operate as a syslog server etc. For example:

```yaml
sensor_type: syslog
syslog:
  client_options:
    buffer_options: {}
    hostname: test-syslog
    identity:
      installation_key: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa
      oid: bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb
    mapping: {}
    platform: text
    sensor_seed_key: test-syslog
  port: 4242
```

Once your external adapter record is created, take note of the `GUID` (Globally Unique ID) found under the `sys_mtd` section of the JSON record, or on the right-hand side of the record view in the web app.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(308).png)

This `GUID` is a shared secret value you will use in the deployed adapter to reference it to the record it should update and operate from.

### Deploying

Now that the configuration of the adapter is ready, you can deploy the adapter on-prem according to the normal process. The only difference is that instead of running it with the full configuration locally, you can run it with the `cloud` collection method like this:

```bash
./lc_adapter cloud conf_guid=XXXXXXXXXXXXXXXXXXXXx oid=YYYYYYYYYYYYYYYYYYY
```

This will start the adapter telling it to fetch the configuration it requires from the cloud based on the Organization ID (your tenant in LC) and the `GUID` of the record it should use.

From this point on, updating the record in LimaCharlie will automatically reconfigure the adapter on-prem, within about 1 minute of the change.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Agent Deployment via Microsoft Intune

[Microsoft Intune](https://learn.microsoft.com/en-us/mem/) is a cloud-based endpoint management solution that integrates with Microsoft Azure. It allows for simplified app and device management across a wide range of devices, including mobile devices, desktop computers, and virtual endpoints.

Intune can be used to simplify LimaCharlie Sensor deployment within enterprise environments. To add a custom App to Intune, select the `+ Add` button within the Intune admin center:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2861%29.png)

InTune supports Windows and macOS package deployment.

## Windows Deployment via Intune

Deploying Windows applications via Intune requires creating an Intune application package (`.intunewin` file extension). To do this, please utilize Microsoft's IntuneWinAppUtil.exe file. Usage and documentation on creating an `.intunewin` file can be found [here](https://learn.microsoft.com/en-us/mem/intune/apps/apps-win32-prepare).

> **Intune Package Contents**
>
> Intune packages may need to be created for each Organization, as the Installation Key must be provided at the time of installation.
>
> We recommend first creating a [custom MSI installer](/v2/docs/building-a-custom-msi-installer-for-windows), bundled with the appropriate installation key, and then including that in your `.intunewin` file.

After clicking `+ Add`, choose `Windows app (Win32)`:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2863%29.png)

---

# Agent Deployment via Microsoft Intune

[Microsoft Intune](https://learn.microsoft.com/en-us/mem/) is a cloud-based endpoint management solution that integrates with Microsoft Azure. It allows for simplified app and device management across a wide range of devices, including mobile devices, desktop computers, and virtual endpoints.

Intune can be used to simplify LimaCharlie Sensor deployment within enterprise environments. To add a custom App to Intune, select the `+ Add` button within the Intune admin center:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2861%29.png)

InTune supports Windows and macOS package deployment.

## Windows Deployment via Intune

Deploying Windows applications via Intune requires creating an Intune application package (`.intunewin` file extension). To do this, please utilize Microsoft's IntuneWinAppUtil.exe file. Usage and documentation on creating an `.intunewin` file can be found [here](https://learn.microsoft.com/en-us/mem/intune/apps/apps-win32-prepare).

> **Intune Package Contents**
>
> Intune packages may need to be created for each Organization, as the Installation Key must be provided at the time of installation.
>
> We recommend first creating a [custom MSI installer](/v2/docs/building-a-custom-msi-installer-for-windows), bundled with the appropriate installation key, and then including that in your `.intunewin` file.

After clicking `+ Add`, choose `Windows app (Win32)`:

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2863%29.png)

---

# Anomalies

## hidden_module_scan

Look for hidden modules in a process's (or all) memory. Hidden modules are DLLs or dylibs loaded manually (not by the OS).

**Platforms:**

**Response Event:**
[HIDDEN_MODULE_DETECTED](/v1/docs/reference-events-responses-anomalies#hiddenmoduledetected)

**Usage:**

```
usage: hidden_module_scan [-h] pid

positional arguments:
  pid         pid of the process to scan, or "-1" for ALL processes
```

---

# Building a custom MSI installer for Windows

You can white label the LimaCharlie installer for Windows by using an MSI wrapper. By going through this process you can not only brand the installer to show your name / details, but you can also make installation of the Sensor easier for end users. We have provided instructions below on how to use a 3rd party tool called [exemsi](https://www.exemsi.com/).

## Prerequisites

1. An MSI wrapper application, such as the exemsi application referenced in the instructions below
2. A digital code signing certificate (optional, but highly recommended)

Without a digital code signing certificate the installer will show a warning that it is from an unknown publisher.

![UAC Signed](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/uac-signed.png)
- vs -
![UAC Warning](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/uac-warning.png)

## Instructions

1. Download the [LimaCharlie sensor EXE](https://downloads.limacharlie.io/sensor/windows/64)
2. Download the [MSI Wrapper application from exemsi.com](https://exemsi.com)
3. Install the exemsi application on your computer
4. Launch the exemsi application and go through the EXE to MSI Converter Wizard steps as shown below:

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_1_-_First_screen_after_launch.png)

5. Select the executable

* Set the `Setup executable input file name` to be the LimaCharlie EXE that you'd downloaded
* Optionally, specify a MSI output file name of your choosing (e.g. Acme_Installer.msi)
* Set the MSI platform architecture to match the executable (i.e. x86 for 32-bit, and x64 for 64-bit)

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_2_-__Select_the_executable.png)

6. Set the visibility in Apps & features

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_3_-_Visibility_in_Apps_&_features.png)

7. Set the Security and User Context

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_4_-_Security_and_User_Context.png)

8. Specify Application IDs

* In the Upgrade Code section, click the "Create New" button next to generate a code. This will be used to allow uninstallation.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_5_-_Application_Ids.png)

9. Specify Properties (optional: customize options here to have the installer show your brand)

* You can change the drop-down menu of each line item from "Executable" to "Manual" in order to set your own values for the Product Name, Manufacturer, Version, Comments, and Product icon

*Original*

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_6a_-_Properties_-_Defaults.png)

*Customized*

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_6b_-_Properties_-_Customized.png)

10. Specify More Properties (optional)

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_7_-_More_properties.png)

11. Specify Parameters

* In the "Install arguments" box, enter "-i", add a space and then enter your installation key
* -i YOUR_INSTALLATION_KEY_GOES_HERE

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_8b_-_Parameters_-_filled.png)

To provide the option to uninstall, set the Uninstall argument to "-c" (note that you do not need to specify your Installation Key for uninstallation).

12. Actions

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_9_-_Actions.png)

13. Summary

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_10_-_Summary.png)

14. Status

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_11_-_Status.png)

Once you have created the MSI package you should sign it using your digital signature. You can [learn more about signing the MSI on the exemsi website](https://www.exemsi.com/documentation/sign-your-msi/).

## Experience when running the MSI

When installing the application using the MSI you'll see your application name in the title bar.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Installation.png)

When inspecting the properties of the MSI you'll see the details you'd specified.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/Created_MSI_Properties-Details.png)

In the Apps & Features section of Windows, you'll see the application listed under your name.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/Shown_in_Control_Panel_-_Apps_and_Features.png)

---

# Building a custom MSI installer for Windows

You can white label the LimaCharlie installer for Windows by using an MSI wrapper. By going through this process you can not only brand the installer to show your name / details, but you can also make installation of the Sensor easier for end users. We have provided instructions below on how to use a 3rd party tool called [exemsi](https://www.exemsi.com/).

## Prerequisites

1. An MSI wrapper application, such as the exemsi application referenced in the instructions below
2. A digital code signing certificate (optional, but highly recommended)

Without a digital code signing certificate the installer will show a warning that it is from an unknown publisher.

![UAC Signed](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/uac-signed.png)
- vs -
![UAC Warning](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/uac-warning.png)

## Instructions

1. Download the [LimaCharlie sensor EXE](https://downloads.limacharlie.io/sensor/windows/64)
2. Download the [MSI Wrapper application from exemsi.com](https://exemsi.com)
3. Install the exemsi application on your computer
4. Launch the exemsi application and go through the EXE to MSI Converter Wizard steps as shown below:

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_1_-_First_screen_after_launch.png)

5. Select the executable

* Set the `Setup executable input file name` to be the LimaCharlie EXE that you'd downloaded
* Optionally, specify a MSI output file name of your choosing (e.g. Acme_Installer.msi)
* Set the MSI platform architecture to match the executable (i.e. x86 for 32-bit, and x64 for 64-bit)

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_2_-__Select_the_executable.png)

6. Set the visibility in Apps & features

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_3_-_Visibility_in_Apps_&_features.png)

7. Set the Security and User Context

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_4_-_Security_and_User_Context.png)

8. Specify Application IDs

* In the Upgrade Code section, click the "Create New" button next to generate a code. This will be used to allow uninstallation.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_5_-_Application_Ids.png)

9. Specify Properties (optional: customize options here to have the installer show your brand)

* You can change the drop-down menu of each line item from "Executable" to "Manual" in order to set your own values for the Product Name, Manufacturer, Version, Comments, and Product icon

*Original*

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_6a_-_Properties_-_Defaults.png)

*Customized*

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_6b_-_Properties_-_Customized.png)

10. Specify More Properties (optional)

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_7_-_More_properties.png)

11. Specify Parameters

* In the "Install arguments" box, enter "-i", add a space and then enter your installation key
* -i YOUR_INSTALLATION_KEY_GOES_HERE

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_8b_-_Parameters_-_filled.png)

To provide the option to uninstall, set the Uninstall argument to "-c" (note that you do not need to specify your Installation Key for uninstallation).

12. Actions

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_9_-_Actions.png)

13. Summary

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_10_-_Summary.png)

14. Status

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Wrapper_-_11_-_Status.png)

Once you have created the MSI package you should sign it using your digital signature. You can [learn more about signing the MSI on the exemsi website](https://www.exemsi.com/documentation/sign-your-msi/).

## Experience when running the MSI

When installing the application using the MSI you'll see your application name in the title bar.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/MSI_Installation.png)

When inspecting the properties of the MSI you'll see the details you'd specified.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/Created_MSI_Properties-Details.png)

In the Apps & Features section of Windows, you'll see the application listed under your name.

![exemsi](https://storage.googleapis.com/limacharlie-io/doc/white-label/exemsi-instructions/Shown_in_Control_Panel_-_Apps_and_Features.png)

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Chrome Agent Installation

LimaCharlie's Chrome Sensor is built as a browser extension and provides visibility for activity performed within the browser. This sensor is particularly useful for gaining affordable network visibility in organizations that make heavy use of ChromeOS.

It is delivered as the [LimaCharlie Sensor](https://chrome.google.com/webstore/detail/limacharlie-sensor/ljdgkaegafdgakkjekimaehhneieecki) extension available in the Chrome Web Store.

## Installation Instructions

The Chrome sensor is available in the Chrome Web Store.

1. In the LimaCharlie web app (app.limacharlie.io), go to the "Installation Keys" section, select your Installation Key and click the "Chrome Key" copy icon to copy the key to your clipboard.
2. Install the sensor from: <https://downloads.limacharlie.io/sensor/chrome>
3. A new tab will open where you can add your installation key from before. If you close it by mistake, you can re-open it by:
   1. From the Extensions page at chrome://extensions/ click on the "Details" button of the LimaCharlie Sensor extension.
   2. Go to the "Extension options" section, and enter your installation key from the previous step. Click save.

The installation key can also be pre-configured through the Managed Storage feature (key named `installation_key`) if you are using a managed Chrome deployment.

## Troubleshooting the Chrome Sensor

If the Chrome extension is giving connectivity issues, the following may help.

First, try uninstalling/re-installing the extension.

If the extension continues to fail to connect, please provide the LimaCharlie support team with the following details:

1. Open a new browser tab
2. Go to `chrome://extensions/`
3. Ensure "Developer Mode" is enabled (see toggle in the top right)

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2838%29.png)

4. Click the `background.html` link in the LimaCharlie Sensor entry.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%2839%29.png)

5. In the window that opens, click Console and provide us with a screenshot of what appears for analysis.

Please also include your Organization ID, which can be found within the LimaCharlie web interface in the REST API section under `OID`.

---

# Config Hive: Cloud Sensors

## Format

## Permissions

* `cloudsensor.get`
* `cloudsensor.set`
* `cloudsensor.del`
* `cloudsensor.get.mtd`
* `cloudsensor.set.mtd`

## Command-Line Usage

Hive secrets can be managed from the command-line, via the `limacharlie hive` command. Positional and optional arguments for command-line usage are below:

```
usage: limacharlie hive [-h] [-k KEY] [-d DATA] [-pk PARTITIONKEY] [--etag ETAG] [--expiry EXPIRY] [--enabled ENABLED] [--tags TAGS] action hive_name

positional arguments:
  action                the action to take, one of: list, list_mtd, get, get_mtd, set, update, remove
  hive_name             the hive name

options:
  -h, --help            show this help message and exit
  -k KEY, --key KEY     the name of the key.
  -d DATA, --data DATA  file containing the JSON data for the record, or "-" for stdin.
  -pk PARTITIONKEY, --partition-key PARTITIONKEY
                        the partition key to use instead of the default OID.
  --etag ETAG           the optional previous etag expected for transactions.
  --expiry EXPIRY       a millisecond epoch timestamp when the record should expire.
  --enabled ENABLED     whether the record is enabled or disabled.
  --tags TAGS           comma separated list of tags.
```

## Usage

## Example

```json
{
    "sensor_type": "webhook",
    "webhook": {
        "client_options": {
            "hostname": "test-webhook",
            "identity": {
                "installation_key": "3bc13b74-0d27-4633-9773-62293bf940a7",
                "oid": "aecec56a-046c-4078-bc08-8ebdc84dcad5"
            },
            "platform": "json",
            "sensor_seed_key": "test-webhook"
        },
        "secret": "super-secret-hook"
    }
}
```

---

# Documents

## doc_cache_get

Retrieve a document / file that was cached on the sensor.

**Platforms:** (All platforms)

**Response Event:** [GET_DOCUMENT_REP](/v1/docs/reference-events-responses-documents)

This command is currently limited to the following document types:

* .bat
* .js
* .ps1
* .sh
* .py
* .exe
* .scr
* .pdf
* .doc
* .docm
* .docx
* .ppt
* .pptm
* .pptx
* .xlt
* .xlsm
* .xlsx
* .vbs
* .rtf
* .hta
* .lnk
* Any files created in `system32` on Windows.

**Usage:**

```
usage: doc_cache_get [-h] [-f FILE_PATTERN] [-s HASHSTR]

optional arguments:
  -f FILE_PATTERN, --file_pattern FILE_PATTERN
                        a pattern to match on the file path and name of the
                        document, simple wildcards ? and * are supported
  -s HASHSTR, --hash HASHSTR
                        hash of the document to get
```

---

# Edge Agent Installation

LimaCharlie's Edge Sensor is delivered as the [LimaCharlie Sensor](https://microsoftedge.microsoft.com/addons/detail/limacharlie-sensor/nomgmkpkkncolnpbkbamfnjhbhmnjehp) extension, available as an Edge Add-on.

## Edge Installation Instructions

The Edge sensor is available in the Edge Add-ons section.

1. In the LimaCharlie web app (app.limacharlie.io), go to the "Installation Keys" section, select your Installation Key and click the "Chrome Key" copy icon to copy the key to your clipboard.
2. Install the sensor from: <https://microsoftedge.microsoft.com/addons/detail/limacharlie-sensor/nomgmkpkkncolnpbkbamfnjhbhmnjehp>
3. A new tab will open where you can add your installation key from before.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Endpoint Agent

The LimaCharlie Endpoint Agent is a key component of LimaCharlie's security platform, acting as a Sensor that collects rich endpoint detection and response (EDR) telemetry. Installed on endpoint devices, the agent continuously monitors system activity, gathering data on processes, network connections, file changes, and user behavior. This telemetry is then sent to LimaCharlie's cloud for analysis, enabling real-time detection of potential threats and incidents.

Beyond data collection, the agent also supports automated response actions based on customizable detection rules. These actions include isolating compromised endpoints, killing malicious processes, and quarantining suspicious files, allowing for quick containment and mitigation of security risks. By functioning as both a sensor and an active responder, the LimaCharlie Endpoint Agent provides organizations with a powerful tool for detecting, analyzing, and responding to threats in real-time.

## Endpoint Agent Types

* [Windows](/v2/docs/windows-agent-installation)
* [Mac](/v2/docs/macos-agent-installation)
* [Linux](/v2/docs/linux-agent-installation)
* [Chrome](/v2/docs/chrome-agent-installation)
* [Edge](/v2/docs/edge-agent-installation)

Don't see what you're looking for?

Need support for a platform you don't see here? Get in touch via [Slack](https://slack.limacharlie.io) or email.

## Quota

All sensors register with the cloud, and many of them may go online / offline over the course of a regular day. For billing purposes, organizations must specify a sensor quota which represents the number of **concurrent online sensors** allowed to be connected to the cloud.

If the quota is maxed out when a sensor attempts to come online, the sensor will be dismissed and a `sensor_over_quota` event will be emitted in the deployments stream.

For more information, see [Billing](/v2/docs/billing) and [Billing FAQ](/v2/docs/faq-billing).

## Events

All sensors observe host & network activity, packaging telemetry and sending it to the cloud. The types of observable events are dependent on the sensor's type.

For an introduction to events and their structure, check out [Events](/v2/docs/events).

## Commands

Windows, Mac, Linux, Chrome, and Edge sensors all offer commands as a safe way of interacting with a host for investigation, management, or threat mitigation purposes.

For an introduction to commands and their usage, check out [Endpoint Agent Commands](/v2/docs/endpoint-agent-commands).

## Installation Keys

Read more about Installation Keys and their recommended usage [here](/v2/docs/installation-keys).

## Sensor Versions & Upgrades

Read more about Endpoint Agent Versioning [here](/v2/docs/endpoint-agent-versioning-and-upgrades).

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Endpoint Agent

The LimaCharlie Endpoint Agent is a key component of LimaCharlie's security platform, acting as a Sensor that collects rich endpoint detection and response (EDR) telemetry. Installed on endpoint devices, the agent continuously monitors system activity, gathering data on processes, network connections, file changes, and user behavior. This telemetry is then sent to LimaCharlie's cloud for analysis, enabling real-time detection of potential threats and incidents.

Beyond data collection, the agent also supports automated response actions based on customizable detection rules. These actions include isolating compromised endpoints, killing malicious processes, and quarantining suspicious files, allowing for quick containment and mitigation of security risks. By functioning as both a sensor and an active responder, the LimaCharlie Endpoint Agent provides organizations with a powerful tool for detecting, analyzing, and responding to threats in real-time.

## Endpoint Agent Types

* [Windows](/v2/docs/windows-agent-installation)
* [Mac](/v2/docs/macos-agent-installation)
* [Linux](/v2/docs/linux-agent-installation)
* [Chrome](/v2/docs/chrome-agent-installation)
* [Edge](/v2/docs/edge-agent-installation)

Don't see what you're looking for?

Need support for a platform you don't see here? Get in touch via [Slack](https://slack.limacharlie.io) or email.

## Quota

All sensors register with the cloud, and many of them may go online / offline over the course of a regular day. For billing purposes, organizations must specify a sensor quota which represents the number of **concurrent online sensors** allowed to be connected to the cloud.

If the quota is maxed out when a sensor attempts to come online, the sensor will be dismissed and a `sensor_over_quota` event will be emitted in the deployments stream.

For more information, see [Billing](/v2/docs/billing) and [Billing FAQ](/v2/docs/faq-billing).

## Events

All sensors observe host & network activity, packaging telemetry and sending it to the cloud. The types of observable events are dependent on the sensor's type.

For an introduction to events and their structure, check out [Events](/v2/docs/events).

## Commands

Windows, Mac, Linux, Chrome, and Edge sensors all offer commands as a safe way of interacting with a host for investigation, management, or threat mitigation purposes.

For an introduction to commands and their usage, check out [Endpoint Agent Commands](/v2/docs/endpoint-agent-commands).

## Installation Keys

Read more about Installation Keys and their recommended usage [here](/v2/docs/installation-keys).

## Sensor Versions & Upgrades

Read more about Endpoint Agent Versioning [here](/v2/docs/endpoint-agent-versioning-and-upgrades).

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Endpoint Agent Commands

Endpoint Agent commands offer a safe way to interact with a Sensor's host either for investigation, management, or threat mitigation purposes.

## Sending Commands

Commands can be sent to Sensors via:

* Manually using the Console of a sensor in the [web application](https://app.limacharlie.io).
* Manually using the [CLI](https://github.com/refractionPOINT/python-limacharlie)
* Programmatically in the response action of a [Detection & Response](/v2/docs/detection-and-response) rule, via the `task` action.
* Programmatically using the [REST API](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI0OQ-task-sensor)

### Sensor REPort/REPly Events

Regardless of which you choose, sent commands will be acknowledged immediately with an empty response, followed by a `CLOUD_NOTIFICATION` event being sent by the sensor. The content of command outputs are delivered as sensor [events](/v2/docs/endpoint-agent-events-overview) suffixed with `_REP`, depending on the command.

**Please ensure that you have enabled the appropriate response event(s) in** [**Event Collection**](/v2/docs/ext-exfil) **to ensure that you will receive the Sensor response.**

This non-blocking approach makes responses accessible via the [event streams](/v2/docs/sensors) passing through Detection & Response rules and Outputs.

## Structure

Commands follow typical CLI conventions using a mix of positional arguments and named optional arguments.

Here's `dir_list` as an example:

```
dir_list [-h] [-d DEPTH] rootDir fileExp

positional arguments:
    rootDir     the root directory where to begin the listing from
    fileExp     a file name expression supporting basic wildcards like * and ?

optional arguments:
    -h, --help      show this help message and exit
    -d DEPTH, --depth DEPTH     optional maximum depth of the listing, defaults to a single level
```

The Console in the web application will provide autocompletion hints of possible commands for a sensor and their parameters. For API users, commands and their usage details may be retrieved via the `/tasks` and `/task` REST API endpoints.

## Investigation IDs

To assist in finding the responses more easily, you may specify an arbitrary `investigation_id` string with a command. The response will then include that value under `routing/investigation_id`. Under the hood, this is exactly how the Console view in the web application works.

If an `investigation_id` is prefixed with `__` (double underscore) it will omit the resulting events from being forwarded to Outputs. This is primarily to allow Services to interact with sensors without spamming.

## Command Line Format

When issuing commands to sensors as a command line (versus a list of tokens), the quoting and escaping of arguments can be confusing. This is a short explanation:

The command line tasks are parsed as if they were issued to a shell like `sh` or `cmd.exe` with a few tweaks to make it easier and more intuitive to use.

Arguments are parsed as separated by spaces, like: `dir_list /home/user *` is equal to 2 arguments: `/home/user` and `*`.

If an argument contains spaces, for example a single directory like `/file/my files`, you must use either single (`'`) or double (`"`) quotes around the argument, like: `dir_list "/files/my files"`.

A backslash (`\`), like in Windows file paths does not need to be escaped. It is only interpreted as an escape character when it is followed by a single or double quote.

The difference between single quotes and double quotes is that double quotes support escaping characters within using `\`, while single quotes never interpret `\` as an escape character. For example:

* `log_get --file "c:\\temp\\my dir\\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file 'c:\\temp\\my dir\\' --type json` becomes `log_get`, `--file`, `c:\\temp\\my dir\\`, `--type`, `json`
* `log_get --file 'c:\temp\my dir\' --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file "c:\temp\my dir\\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`

This means that as a general statement, unless you want to embed quoted strings within specific arguments, it is easier to use single quotes around arguments and not worry about escaping `\`.

---

# Endpoint Agent Commands

Endpoint Agent commands offer a safe way to interact with a Sensor's host either for investigation, management, or threat mitigation purposes.

## Sending Commands

Commands can be sent to Sensors via:

* Manually using the Console of a sensor in the [web application](https://app.limacharlie.io).
* Manually using the [CLI](https://github.com/refractionPOINT/python-limacharlie)
* Programmatically in the response action of a [Detection & Response](/v2/docs/detection-and-response) rule, via the `task` action.
* Programmatically using the [REST API](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI0OQ-task-sensor)

### Sensor REPort/REPly Events

Regardless of which you choose, sent commands will be acknowledged immediately with an empty response, followed by a `CLOUD_NOTIFICATION` event being sent by the sensor. The content of command outputs are delivered as sensor [events](/v2/docs/endpoint-agent-events-overview) suffixed with `_REP`, depending on the command.

**Please ensure that you have enabled the appropriate response event(s) in** [**Event Collection**](/v2/docs/ext-exfil) **to ensure that you will receive the Sensor response.**

This non-blocking approach makes responses accessible via the [event streams](/v2/docs/sensors) passing through Detection & Response rules and Outputs.

## Structure

Commands follow typical CLI conventions using a mix of positional arguments and named optional arguments.

Here's `dir_list` as an example:

```
dir_list [-h] [-d DEPTH] rootDir fileExp

positional arguments:
    rootDir     the root directory where to begin the listing from
    fileExp     a file name expression supporting basic wildcards like * and ?

optional arguments:
    -h, --help      show this help message and exit
    -d DEPTH, --depth DEPTH     optional maximum depth of the listing, defaults to a single level
```

The Console in the web application will provide autocompletion hints of possible commands for a sensor and their parameters. For API users, commands and their usage details may be retrieved via the `/tasks` and `/task` REST API endpoints.

## Investigation IDs

To assist in finding the responses more easily, you may specify an arbitrary `investigation_id` string with a command. The response will then include that value under `routing/investigation_id`. Under the hood, this is exactly how the Console view in the web application works.

If an `investigation_id` is prefixed with `__` (double underscore) it will omit the resulting events from being forwarded to Outputs. This is primarily to allow Services to interact with sensors without spamming.

## Command Line Format

When issuing commands to sensors as a command line (versus a list of tokens), the quoting and escaping of arguments can be confusing. This is a short explanation:

The command line tasks are parsed as if they were issued to a shell like `sh` or `cmd.exe` with a few tweaks to make it easier and more intuitive to use.

Arguments are parsed as separated by spaces, like: `dir_list /home/user *` is equal to 2 arguments: `/home/user` and `*`.

If an argument contains spaces, for example a single directory like `/file/my files`, you must use either single (`'`) or double (`"`) quotes around the argument, like: `dir_list "/files/my files"`.

A backslash (`\`), like in Windows file paths does not need to be escaped. It is only interpreted as an escape character when it is followed by a single or double quote.

The difference between single quotes and double quotes is that double quotes support escaping characters within using `\`, while single quotes never interpret `\` as an escape character. For example:

* `log_get --file "c:\\temp\\my dir\\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file 'c:\\temp\\my dir\\' --type json` becomes `log_get`, `--file`, `c:\\temp\\my dir\\`, `--type`, `json`
* `log_get --file 'c:\temp\my dir\' --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file "c:\temp\my dir\\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`

This means that as a general statement, unless you want to embed quoted strings within specific arguments, it is easier to use single quotes around arguments and not worry about escaping `\`.

---

# Endpoint Agent Events Overview

## Overview

This category describes and provides samples for the various events emitted by the LimaCharlie Endpoint Agent Sensor. These events can be leveraged in [D&R rules](/v2/docs/detection-and-response) and queried with [LCQL](/v2/docs/lcql).

> **Important note about Event Collection**
>
> Only events enabled in the Exfil configuration will be shipped by the endpoint agent. If you're not seeing a specific event you expect, make sure that the desired event type is enabled in the [Exfil extension](/v2/docs/ext-exfil) configuration. Ensure your Exfil settings are properly configured to capture all required event types for your use case.

## Atoms

Atoms are Globally Unique Identifiers (GUIDs). An example might be: `1e9e242a512d9a9b16d326ac30229e7b`. You can treat them as opaque values. These unique values are used to relate events together rather than using Process IDs, which are themselves unreliable.

### Relationships

Atoms can be found in up to 3 spots in an event:

* `routing/this`: current event
* `routing/parent`: parent of the current event
* `routing/target`: target of the current event

Using atom references from a single event, the chain of ancestor events can be constructed. Here's a simplified example of an event and its parent event:

**Child event:**

```
{
  "event": {...},
  "routing": {
    "this": "abcdef",
    "parent": "zxcv"
    ...
  }
}
```

**Parent event:**

```
{
  "event": {...},
  "routing": {
    "this": "zxcv",
    "parent": "poiuy"
    ...
  }
}
```

API users may construct a tree from a single atom using these 2 endpoints:

* `/insight/{oid}/{sid}/{atom}` - get event by atom
* `/insight/{oid}/{sid}/{atom}/children` - get children of an atom

These can be called recursively on each event's `routing/parent` and/or child events to complete a full tree if required - this is how the tree view works in the Timeline of a sensor in the web application.

The parent-child relationship serves to describe parent and child processes via the `NEW_PROCESS` or `EXISTING_PROCESS` events, but other types of events may also have parents. For example, on `NETWORK_SUMMARY` events, the `parent` will be the process that generated the network connections.

> **Tip:** when using custom storage and/or searching solutions it's helpful to index the values of `routing/this` and `routing/parent` for each event. Doing so will speed up searching during threat hunting and investigations.

Finally, the `routing/target` is only sometimes found in an event, and it represents an event that interacts with another event without having a parent-child relationship. For example, in the `NEW_REMOTE_THREAD` event, this `target` represents a process where a remote thread was created.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, Exfil (Event Collection) is a configuration extension that determines which types of events are collected and sent from endpoint agents to the cloud. It controls the data flow, ensuring only specified events are transmitted for monitoring and analysis. To capture specific events, they must be enabled within the Exfil or Event Collection settings.

---

# Endpoint Agent Events Overview

## Overview

This category describes and provides samples for the various events emitted by the LimaCharlie Endpoint Agent Sensor. These events can be leveraged in [D&R rules](/v2/docs/detection-and-response) and queried with [LCQL](/v2/docs/lcql).

> **Important note about Event Collection**
>
> Only events enabled in the Exfil configuration will be shipped by the endpoint agent. If you're not seeing a specific event you expect, make sure that the desired event type is enabled in the [Exfil extension](/v2/docs/ext-exfil) configuration. Ensure your Exfil settings are properly configured to capture all required event types for your use case.

## Atoms

Atoms are Globally Unique Identifiers (GUIDs). An example might be: `1e9e242a512d9a9b16d326ac30229e7b`. You can treat them as opaque values. These unique values are used to relate events together rather than using Process IDs, which are themselves unreliable.

### Relationships

Atoms can be found in up to 3 spots in an event:

* `routing/this`: current event
* `routing/parent`: parent of the current event
* `routing/target`: target of the current event

Using atom references from a single event, the chain of ancestor events can be constructed. Here's a simplified example of an event and its parent event:

**Child event:**

```json
{
  "event": {...},
  "routing": {
    "this": "abcdef",
    "parent": "zxcv"
    ...
  }
}
```

**Parent event:**

```json
{
  "event": {...},
  "routing": {
    "this": "zxcv",
    "parent": "poiuy"
    ...
  }
}
```

API users may construct a tree from a single atom using these 2 endpoints:

* `/insight/{oid}/{sid}/{atom}` - get event by atom
* `/insight/{oid}/{sid}/{atom}/children` - get children of an atom

These can be called recursively on each event's `routing/parent` and/or child events to complete a full tree if required - this is how the tree view works in the Timeline of a sensor in the web application.

The parent-child relationship serves to describe parent and child processes via the `NEW_PROCESS` or `EXISTING_PROCESS` events, but other types of events may also have parents. For example, on `NETWORK_SUMMARY` events, the `parent` will be the process that generated the network connections.

> **Tip:** when using custom storage and/or searching solutions it's helpful to index the values of `routing/this` and `routing/parent` for each event. Doing so will speed up searching during threat hunting and investigations.

Finally, the `routing/target` is only sometimes found in an event, and it represents an event that interacts with another event without having a parent-child relationship. For example, in the `NEW_REMOTE_THREAD` event, this `target` represents a process where a remote thread was created.

## Key Concepts

**Endpoint Agents** are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Similar to agents, **Sensors** send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Exfil (Event Collection)** is a configuration extension that determines which types of events are collected and sent from endpoint agents to the cloud. It controls the data flow, ensuring only specified events are transmitted for monitoring and analysis. To capture specific events, they must be enabled within the Exfil or Event Collection settings.

---

# Endpoint Agent Installation

The Endpoint Agent is signed, and the same for everyone. The endpoint agent's customization, which indicates the owner, is done at installation based on the installation key used. The Installation Key specifies where the Sensor should connect to enroll, as well as the encryption key used to start the enrollment process.

> **Enterprise-wide deployment**: Looking to deploy many endpoint agents at once? Check out Enterprise Sensor Deployment.

Installing the endpoint agent does not require a reboot.

## Installing the Endpoint Agent

The sensors are designed to be simple to use and re-package for any deployment methodology you use in your Organization.

The sensor requires administrative privileges to install. On Windows this means an Administrator or System account, on macOS and Linux it means the root account.

Before installing, you will need the installation key you want to use.

For OS-specific installation instructions, choose your OS in the nav bar on the left.

## Required Permissions

**Windows**

* Administrative privileges - Must run as LocalSystem service
* SeDebugPrivilege - Debug programs privilege
* SeBackupPrivilege - Back up files and directories privilege
* SeRestorePrivilege - Restore files and directories privilege

**Linux**

* Root privileges (UID 0) - Required for system monitoring
* RLIMIT_MEMLOCK set to RLIM_INFINITY - For eBPF program loading
* Mount capabilities - For filesystem mounting
* CAP_BPF or CAP_SYS_ADMIN - For eBPF kernel module operation
* CAP_NET_ADMIN - For network monitoring

**macOS**

* Root privileges (UID 0) - Required for system monitoring
* Kernel extension entitlements - Including com.apple.security.cs.debugger
* Apple KPI dependencies - bsd, libkern, dsep, mach kernel programming interfaces

**Cross-Platform Requirements**

* File system read/write access to system directories
* Process monitoring capabilities
* Network monitoring and outbound HTTPS access
* Registry access (Windows) for system configuration

> **Note**: The sensors require these elevated privileges for legitimate security monitoring including process detection, file system monitoring, network analysis, and kernel-level telemetry collection.

## Downloading the Agents

To download the single installers relevant for your deployment, access the `/download/[platform]/[architecture]` control plane. The `platform` component is one of `win`, `linux` or `osx` while the `architecture` component is either `32` or `64`.

For example:

* <https://downloads.limacharlie.io/sensor/windows/32> for the Windows 32 bit executable installer
* <https://downloads.limacharlie.io/sensor/windows/64> for the Windows 64 bit executable installer
* <https://downloads.limacharlie.io/sensor/windows/msi32> for the Windows 32 bit MSI installer
* <https://downloads.limacharlie.io/sensor/windows/msi64> for the Windows 64 bit MSI installer
* <https://downloads.limacharlie.io/sensor/linux/64> for the Linux 64 bit installer
* <https://downloads.limacharlie.io/sensor/linux/alpine64> for the Linux Alpine 64 bit installer
* <https://downloads.limacharlie.io/sensor/linux/deb32> for the Linux 32 bit Debian package
* <https://downloads.limacharlie.io/sensor/linux/deb64> for the Linux 64 bit Debian package
* <https://downloads.limacharlie.io/sensor/linux/debarm64> for the Linux ARM 64 bit Debian package
* <https://downloads.limacharlie.io/sensor/mac/64> for the macOS 64 bit installer
* <https://downloads.limacharlie.io/sensor/mac/arm64> for the macOS ARM 64 bit (Apple Silicon) installer
* <https://downloads.limacharlie.io/sensor/chrome> for the Chrome extension
* <https://downloads.limacharlie.io/sensor/edge> for the MS Edge extension

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Endpoint Agent Installation

The Endpoint Agent is signed, and the same for everyone. The endpoint agent's customization, which indicates the owner, is done at installation based on the installation key used. The Installation Key specifies where the Sensor should connect to enroll, as well as the encryption key used to start the enrollment process.

> **Enterprise-wide deployment**: Looking to deploy many endpoint agents at once? Check out Enterprise Sensor Deployment.

Installing the endpoint agent does not require a reboot.

## Installing the Endpoint Agent

The sensors are designed to be simple to use and re-package for any deployment methodology you use in your Organization.

The sensor requires administrative privileges to install. On Windows this means an Administrator or System account, on macOS and Linux it means the root account.

Before installing, you will need the installation key you want to use.

For OS-specific installation instructions, choose your OS in the nav bar on the left.

## Required Permissions

**Windows**

* Administrative privileges - Must run as LocalSystem service
* SeDebugPrivilege - Debug programs privilege
* SeBackupPrivilege - Back up files and directories privilege
* SeRestorePrivilege - Restore files and directories privilege

**Linux**

* Root privileges (UID 0) - Required for system monitoring
* RLIMIT_MEMLOCK set to RLIM_INFINITY - For eBPF program loading
* Mount capabilities - For filesystem mounting
* CAP_BPF or CAP_SYS_ADMIN - For eBPF kernel module operation
* CAP_NET_ADMIN - For network monitoring

**macOS**

* Root privileges (UID 0) - Required for system monitoring
* Kernel extension entitlements - Including com.apple.security.cs.debugger
* Apple KPI dependencies - bsd, libkern, dsep, mach kernel programming interfaces

**Cross-Platform Requirements**

* File system read/write access to system directories
* Process monitoring capabilities
* Network monitoring and outbound HTTPS access
* Registry access (Windows) for system configuration

> Note: The sensors require these elevated privileges for legitimate security monitoring including process detection, file system monitoring, network analysis, and kernel-level telemetry collection.

## Downloading the Agents

To download the single installers relevant for your deployment, access the `/download/[platform]/[architecture]` control plane. The `platform` component is one of `win`, `linux` or `osx` while the `architecture` component is either `32` or `64`.

For example:

* <https://downloads.limacharlie.io/sensor/windows/32> for the Windows 32 bit executable installer
* <https://downloads.limacharlie.io/sensor/windows/64> for the Windows 64 bit executable installer
* <https://downloads.limacharlie.io/sensor/windows/msi32> for the Windows 32 bit MSI installer
* <https://downloads.limacharlie.io/sensor/windows/msi64> for the Windows 64 bit MSI installer
* <https://downloads.limacharlie.io/sensor/linux/64> for the Linux 64 bit installer
* <https://downloads.limacharlie.io/sensor/linux/alpine64> for the Linux Alpine 64 bit installer
* <https://downloads.limacharlie.io/sensor/linux/deb32> for the Linux 32 bit Debian package
* <https://downloads.limacharlie.io/sensor/linux/deb64> for the Linux 64 bit Debian package
* <https://downloads.limacharlie.io/sensor/linux/debarm64> for the Linux ARM 64 bit Debian package
* <https://downloads.limacharlie.io/sensor/mac/64> for the macOS 64 bit installer
* <https://downloads.limacharlie.io/sensor/mac/arm64> for the macOS ARM 64 bit (Apple Silicon) installer
* <https://downloads.limacharlie.io/sensor/chrome> for the Chrome extension
* <https://downloads.limacharlie.io/sensor/edge> for the MS Edge extension

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Endpoint Agent Uninstallation

There are multiple options available to uninstall the LimaCharlie Sensor, depending on the operating system and/or method of installation. macOS and Windows systems allow for easy uninstallation via sensor commands or rules. Linux systems may require additional steps, as detailed below.

## Manually Uninstalling the Endpoint Agent

When uninstalling macOS and Windows Sensors, please attempt to utilize a method similar to sensor deployment. For example, if sensors were deployed via a package manager, then the same package manager may have uninstall options as well. This will help keep software inventories up to date.

Details on manual uninstallation is found at the bottom of each respective OS' installation procedures.

## Uninstalling Endpoint Agents from the Platform

### Sensor Commands

For macOS and Windows operating systems, you can uninstall a sensor with the `uninstall` command. More information on that can be found [here](/v2/docs/endpoint-agent-commands#uninstall).

On Windows, the command defaults to uninstalling the sensor as if installed from the direct installer exe. If an MSI was used for installation, you can add a `--msi` flag to the `uninstall` command to trigger an uninstallation that is compatible with MSI.

### SDK

To run the uninstall command against *all* Sensors, a simple loop with the SDK in Python would work:

```python
import limacharlie
lc = limacharlie.Manager()
for sensor in lc.sensors():
  sensor.task( 'uninstall' )
```

### Using a D&R Rule

As an alternative approach, you can also use a Detection & Response (D&R) rule to automatically trigger an uninstall of the LimaCharlie sensor when a sensor connects to the LimaCharlie cloud. Below is an example of the rule you can use for this purpose. This example is specific to Windows-based endpoints, but can be modified based on your needs:

```yaml
# Detect
event: SYNC
op: is windows

# Respond
- action: task
  command: uninstall --is-confirmed
- action: add tag
  tag: uninstalled
```

## Package Management Tools

For Package Management tools, and other enterprise application-management tools, we recommend utilizing the integrated program removal options, rather than installing from LimaCharlie. This will help keep software inventories up to date.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Endpoint Agents are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

---

# FAQ - Sensor Installation

## How can I add LimaCharlie traffic to an allow list?

The tables below show the hostnames and IPs used to connect to LimaCharlie. All connections use TCP port 443 and TLS 1.2+

## What Hostnames and IPs does LimaCharlie use for each region?

### Canada (Quebec)

| Hostname | IP | Use |
| --- | --- | --- |
| aae67d7e76570ec1.lc.limacharlie.io | 35.203.33.203 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| aae67d7e76570ec1.edr.limacharlie.io | 35.201.82.57 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| aae67d7e76570ec1.wss.limacharlie.io | 35.201.96.199 | Chrome, Edge and Adapters |
| aae67d7e76570ec1.ingest.limacharlie.io | 34.149.216.238 | Logs and Artifacts |
| aae67d7e76570ec1.replay.limacharlie.io | 142.250.115.121 | Replay |
| aae67d7e76570ec1.live.limacharlie.io | 34.120.175.14 | Live feed |
| aae67d7e76570ec1.hook.limacharlie.io | 142.250.115.121 | Webhooks |

### US (Iowa)

| Hostname | IP | Use |
| --- | --- | --- |
| 9157798c50af372c.lc.limacharlie.io | 35.194.62.236 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| 9157798c50af372c.edr.limacharlie.io | 34.149.165.165 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| 9157798c50af372c.wss.limacharlie.io | 34.102.223.182 | Chrome, Edge and Adapters |
| 9157798c50af372c.ingest.limacharlie.io | 34.120.157.194 | Logs and Artifacts |
| 9157798c50af372c.replay.limacharlie.io | 142.250.115.121 | Replay |
| 9157798c50af372c.live.limacharlie.io | 34.120.123.4 | Live feed |
| 9157798c50af372c.hook.limacharlie.io | 142.250.115.121 | Webhooks |

### India (Mumbai)

| Hostname | IP | Use |
| --- | --- | --- |
| 4d897015b0815621.lc.limacharlie.io | 35.200.151.24 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| 4d897015b0815621.edr.limacharlie.io | 34.102.207.18 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| 4d897015b0815621.wss.limacharlie.io | 34.98.108.101 | Chrome, Edge and Adapters |
| 4d897015b0815621.ingest.limacharlie.io | 34.149.161.19 | Logs and Artifacts |
| 4d897015b0815621.replay.limacharlie.io | 142.250.115.121 | Replay |
| 4d897015b0815621.live.limacharlie.io | 35.244.221.119 | Live feed |
| 4d897015b0815621.hook.limacharlie.io | 142.250.115.121 | Webhooks |

### UK (London)

| Hostname | IP | Use |
| --- | --- | --- |
| 70182cf634c346bd.lc.limacharlie.io | 35.242.152.114 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| 70182cf634c346bd.edr.limacharlie.io | 34.107.134.233 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| 70182cf634c346bd.wss.limacharlie.io | 35.244.147.201 | Chrome, Edge and Adapters |
| 70182cf634c346bd.ingest.limacharlie.io | 34.149.56.238 | Logs and Artifacts |
| 70182cf634c346bd.replay.limacharlie.io | 142.250.115.121 | Replay |
| 70182cf634c346bd.live.limacharlie.io | 35.244.146.102 | Live feed |
| 70182cf634c346bd.hook.limacharlie.io | 142.250.115.121 | Webhooks |

### Europe (Emshaven)

| Hostname | IP | Use |
| --- | --- | --- |
| b76093c3662d5b4f.lc.limacharlie.io | 35.204.142.125 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| b76093c3662d5b4f.edr.limacharlie.io | 34.111.194.87 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| b76093c3662d5b4f.wss.limacharlie.io | 130.211.22.248 | Chrome, Edge and Adapters |
| b76093c3662d5b4f.ingest.limacharlie.io | 34.120.5.160 | Logs and Artifacts |
| b76093c3662d5b4f.replay.limacharlie.io | 142.250.115.121 | Replay |
| b76093c3662d5b4f.live.limacharlie.io | 34.120.64.23 | Live feed |
| b76093c3662d5b4f.hook.limacharlie.io | 142.250.115.121 | Webhooks |

### Australia (Sydney)

| Hostname | IP | Use |
| --- | --- | --- |
| abc32764762fce67.lc.limacharlie.io | 34.151.84.52 | Windows, Mac, & Linux EDR Agent  Note: Pinned SSL certificates (SSL interception unsupported) |
| abc32764762fce67.edr.limacharlie.io | 34.54.253.51 | Windows, Mac, & Linux EDR Agent  Note: Non-Pinned SSL certificates (SSL interception supported) |
| abc32764762fce67.wss.limacharlie.io | 34.96.104.54 | Chrome, Edge and Adapters |
| abc32764762fce67.ingest.limacharlie.io | 35.241.63.128 | Logs and Artifacts |
| abc32764762fce67.replay.limacharlie.io | 34.49.249.16 | Replay |
| abc32764762fce67.live.limacharlie.io | 34.8.102.215 | Live feed |
| abc32764762fce67.hook.limacharlie.io | 34.49.185.177 | Webhooks |

## How much data does the LimaCharlie Sensor produce per day?

The amount of data that is produced by the sensor is dependent on how much, and what kind of activity is taking place on the endpoint. That being said, the average data produced per endpoint across thousands of deployments is approximately 1MB per day.

## What resources does the LimaCharlie agent consume?

The total footprint of the agent on disk combined with what is in memory is approximately 50MB. The agent typically runs under 1% CPU.

Depending on what actions you may be performing it may increase (e.g. if you're doing a full YARA scan it's expected that the CPU usage will increase). When you use our YARA trickle scan, that also keeps CPU usage within reasonable bounds. You'll only see YARA scans spike CPU when you do a full manual scan.

Depending on the configuration of the agent (it's fully customizable), the network bandwidth will vary, but we typically see approximately 2MB per day on Windows hosts.

## Why does my sensor initially connect successfully but then disappear?

Sometimes we see the agent connect to the LimaCharlie cloud, enrolls, then disconnects (which is normal the first time after enrollment) and never connects again, or it doesn't show that kernel has been acquired.

This behavior is typical with SSL interception. Sometimes it's a network device, but at other times some security products on the host can do that without being very obvious.

You can confirm if there is SSL interception by performing the following steps to check the SSL fingerprint of the LimaCharlie cloud from the host.

**Confirm the region of your Organization**

If you already know where your organization's region is located, you can move to the next step. To verify the organization's region where the data is processed and stored, click `Add Sensor` from the `Sensors` view. You will then see the region listed under `Sensor Connectivity`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/installation(1).png)

**Open the test URL**

Via web browser, navigate to one of the below test URLs that corresponds to the correct region:

[Test URL - US Region](https://9157798c50af372c.lc.limacharlie.io/)
[Test URL - UK Region](https://70182cf634c346bd.lc.limacharlie.io/)
[Test URL - India Region](https://4d897015b0815621.lc.limacharlie.io/)
[Test URL - Europe Region](https://b76093c3662d5b4f.lc.limacharlie.io/)
[Test URL - Canada Region](https://aae67d7e76570ec1.lc.limacharlie.io/)

No website will open; you should get a "Your connection is not private" type of message instead.

**Display the SSL Certificate**

By clicking near the URL bar on the exclamation mark, you will open a small menu and you can click "Certificate status"/"Certificate validity"/"Certificate is not valid" which will display the certificate information.

![certifricate](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/certifricate.png)

![certificate-1](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/certificate-1.png)

**Confirm the SHA-1 and SHA-256 fingerprints**

The SHA-1 and SHA-256 fingerprints should match the values below that correspond to the region your organization is in.

If the SHA-1 and SHA-256 fingerprints you are seeing do not match what's listed below, that's an indicator of the SSL interception.

| Region | SHA-256 Fingerprint | SHA-1 Fingerprint |
| --- | --- | --- |
| US | 14 44 8C B6 A1 19 A5 BE 18 AE 28 07 E3 D6 BD 55 B8 7A 5E 0C 3F 2D 78 03 6E 7C 6A 2A AA 45 8F 60 | 1A 72 67 08 D0 83 7D A9 62 85 39 55 A1 12 1B 10 B0 F4 56 1A |
| UK | 49 49 B0 41 D6 14 F3 3B 86 BF DF 14 24 F8 BD 2F E1 98 39 41 5A 99 E6 F1 C7 A2 C8 AB 34 0C FE 1D | 2E 49 00 DB F8 3A 2A 88 E0 15 76 D5 C5 4F 8F F3 7D 27 77 DD |
| India | 68 6F 08 3D 53 3F 08 E0 22 EB F6 67 0C 3C 41 08 75 D6 0E 67 03 88 D9 B6 E1 F8 19 6B DA 54 5A A3 | 37 57 DD 4E CF 2B 25 0B CA EA E2 E6 E3 B2 98 48 29 19 F3 6B |
| Europe | EF B3 FA A7 78 AB F0 B0 41 00 CF A3 5F 44 3F 9A 4D 16 28 B9 83 22 85 E3 36 44 D5 DC F9 5C 78 5B | 07 72 B3 31 1A 89 D6 54 1D 71 C3 07 AD B5 8A 26 FD 30 7E 5D |
| Canada | D3 40 8B 59 AE 5A 28 75 D1 65 71 50 52 2E 6F 45 26 EE E8 19 3A 9A 74 39 C1 64 60 B8 6A 92 15 47 | E3 EF AE 6A 0E 7F 18 83 15 FE F2 02 6C F3 2D 4E 59 95 4D 0A |

## What happens if a host is offline?

When the host is offline, the Sensor will keep collecting telemetry and store it locally in a "ring buffer" (which limits the total possible size). The buffer is ~60mb, so the amount of time it will cover will vary based on how much telemetry the individual endpoint generates. e.g. A domain controller will likely be generating many more events than a regular end user workstation.

When the host is back online, the content of this buffer will be flushed to the cloud where detection and response rules will apply as usual.

The same ring buffer is used when the Sensor runs normally, even if data is not sent to the cloud in real-time. The cloud can then retroactively request the full or partial content of the ring buffer, bringing your telemetry current.

## How can I tell which Installation Key was used to enroll a sensor?

On occasion you may need to check which installation key was used to enroll a sensor. You can do so by comparing the sensors `Installer ID` with the Installation Key's `Adapter Key` value.

1. Go to the Sensors section and click into the sensor in question to view its details page. Take note of the `Installer ID`.
2. Go to the Install Sensors section. Click the copy icon under the `Adapter Key`.
3. Compare these two values; the Installer ID on a sensor should be the same as the Adapter Key of the installation key used.

If you need to check a large list of sensors, you can perform an export of all sensors from the main sensors list page, or use the LimaCharlie API.

---

# FAQ - Sensor Removal

## How do I verify the LimaCharlie agent was uninstalled from macOS systems?

After [performing an uninstallation](https://docs.limacharlie.io/docs/macos-agent-installation-latest-os-versions#uninstallation-flow) of the LimaCharlie Sensor for macOS, you can verify that the process was successful by manually checking several items on the endpoint, as described below.

## Verify the LimaCharlie processes are not running

1. Open Activity Monitor (`/Applications/Utilities/Activity Monitor.app`).
2. From the View menu, ensure "All Processes" is selected.
3. In the Search box, type: `rphcp`
4. Ensure that neither of the following processes appear:

`rphcp`

`com.refractionpoint.rphcp.extension`

If either appear, the uninstallation likely did not complete successfully and it should be re-attempted.

## Verify all files on disk were removed

The following LimaCharlie sensor-related files should no longer exist on disk:

/Applications/RPHCP.app

/usr/local/bin/rphcp

/usr/local/hcp

/usr/local/hcp\_conf

/usr/local/hcp\_hbs

You may optionally remove the log file located at: /usr/local/hcp.log

## Verify LimaCharlie Network Extension was removed

1. Open System Settings
2. Navigate to Network
3. Select VPN & Filters
4. Check if "RPHCP" appears in the list.

 If it does not appear, the Network Extension was successfully removed.

 If you see RPHCP in the list, the Network Extension was not properly removed and you should perform uninstallation again.

## Verify LimaCharlie Security Extension was removed

Open the Terminal and run the following commands

**Run Command #1**

`sudo systemextensionsctl list | grep rphcp`

 No result would indicate that the uninstall was successful.

 The following result would indicate that the uninstall was not successful:

```
*	*	N7N82884NH	com.refractionpoint.rphcp.extension (1.0.241204/1.0.241204)	RPHCP	[activated enabled]
```

**Run Command #2**

`sudo cat /Library/SystemExtensions/db.plist | grep rphcp`

 If no result is returned, the security extension was successfully removed.

 If instead you see something similar to the below, the extension was not properly removed and you may need to take some additional measures to do so (i.e. manual removal after booting into Recovery mode).

---

# File and Registry Integrity Monitoring (FIM) Deployments

Use the SecOps Cloud Platform to gain comprehensive visibility, control, and proactive protection for sensitive files and registry keys with LimaCharlie's robust FIM capabilities.

## FIM deployment problems

* **Undetected unauthorized changes:** Malicious actors often target sensitive files and registry keys to install malware, exfiltrate data, or disrupt operations, often evading traditional security measures.
* **Challenges of manual monitoring:** Manually tracking changes to critical files and registry entries across large environments is time-consuming, error-prone, and often reactive rather than proactive.
* **Limited visibility into past events:** Traditional FIM tools might lack comprehensive historical data storage, hindering investigations and threat hunting efforts.
* **Fragmented solutions:** File and registry integrity monitoring can be siloed in separate platforms or integrated with data loss prevention (DLP) tools, lacking the comprehensive visibility and detection capabilities of a unified security platform.

## LimaCharlie's solution

* **Unified Visibility and Response:** Consolidate FIM with other endpoint detection and response (EDR) capabilities within LimaCharlie, eliminating the need for separate platforms and streamlining security operations.
* **Continuous Monitoring and Alerting:** LimaCharlie's FIM capability continuously monitors designated files and registry keys for any modifications, generating real-time alerts to security teams for immediate action.
* **Granular Configuration and Rules:** Define specific files, directories, and registry paths to monitor based on your unique security needs, ensuring focused protection for critical assets.
* **Historical Data Storage and Analysis:** LimaCharlie stores one year of historical FIM data, enabling in-depth investigations, threat hunting, and identification of potential attack patterns that might have been missed initially.

---

# File and Registry Integrity Monitoring (FIM) Deployments

Use the SecOps Cloud Platform to gain comprehensive visibility, control, and proactive protection for sensitive files and registry keys with LimaCharlie's robust FIM capabilities.

## FIM deployment problems

* **Undetected unauthorized changes:** Malicious actors often target sensitive files and registry keys to install malware, exfiltrate data, or disrupt operations, often evading traditional security measures.
* **Challenges of manual monitoring:** Manually tracking changes to critical files and registry entries across large environments is time-consuming, error-prone, and often reactive rather than proactive.
* **Limited visibility into past events:** Traditional FIM tools might lack comprehensive historical data storage, hindering investigations and threat hunting efforts.
* **Fragmented solutions:** File and registry integrity monitoring can be siloed in separate platforms or integrated with data loss prevention (DLP) tools, lacking the comprehensive visibility and detection capabilities of a unified security platform.

## LimaCharlie's solution

* **Unified Visibility and Response:** Consolidate FIM with other endpoint detection and response (EDR) capabilities within LimaCharlie, eliminating the need for separate platforms and streamlining security operations.
* **Continuous Monitoring and Alerting:** LimaCharlie's FIM capability continuously monitors designated files and registry keys for any modifications, generating real-time alerts to security teams for immediate action.
* **Granular Configuration and Rules:** Define specific files, directories, and registry paths to monitor based on your unique security needs, ensuring focused protection for critical assets.
* **Historical Data Storage and Analysis:** LimaCharlie stores one year of historical FIM data, enabling in-depth investigations, threat hunting, and identification of potential attack patterns that might have been missed initially.

---

# Installation Keys

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

There are four components of an Installation Key:

* **Organization ID (OID)**: The Organization ID that this key should enroll into.
* **Installer ID (IID)**: Installer ID that is generated and associated with every Installation Key.
* **Tags**: A list of Tags automatically applied to sensors enrolling with the key.
* **Description**: The description used to help you differentiate uses of various keys.

## Management

Installation keys can be managed on the **Sensors > Installation Keys** page in the web app.

On this page, under the `Connectivity` section, you will see the various URLs associated with Sensor and Adapter connectivity.

### Pinned Certificates

Typically, Sensors require access over port 443 and use pinned SSL certificates. This is the default deployment option, and does not support traffic interception.

If you need to install sensors without pinned certificates, an installation key must be created with a specific flag. This must be done via the REST API, by setting the `use_public_root_ca` flag to `true`.

More details can be found [here](https://github.com/refractionPOINT/python-limacharlie/blob/master/limacharlie/Manager.py#L1386).

## Use of Tags

Generally speaking, we use at least one Installation Key per organization. Then we use different keys to help differentiate parts of our infrastructure. For example, you may create a key with Tag "server" that you will use to install on your servers, a key with "vip" for executives in your organization, or a key with "sales" for the sales department, etc. This way you can use the tags on various sensors to figure out different detection and response rules for different types of hosts on your infrastructure.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

In LimaCharlie, an Organization ID is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Installation Keys

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

There are four components of an Installation Key:

* **Organization ID (OID)**: The Organization ID that this key should enroll into.
* **Installer ID (IID)**: Installer ID that is generated and associated with every Installation Key.
* **Tags**: A list of Tags automatically applied to sensors enrolling with the key.
* **Description**: The description used to help you differentiate uses of various keys.

## Management

Installation keys can be managed on the **Sensors > Installation Keys** page in the web app.

On this page, under the `Connectivity` section, you will see the various URLs associated with Sensor and Adapter connectivity.

### Pinned Certificates

Typically, Sensors require access over port 443 and use pinned SSL certificates. This is the default deployment option, and does not support traffic interception.

If you need to install sensors without pinned certificates, an installation key must be created with a specific flag. This must be done via the REST API, by setting the `use_public_root_ca` flag to `true`.

More details can be found [here](https://github.com/refractionPOINT/python-limacharlie/blob/master/limacharlie/Manager.py#L1386).

## Use of Tags

Generally speaking, we use at least one Installation Key per organization. Then we use different keys to help differentiate parts of our infrastructure. For example, you may create a key with Tag "server" that you will use to install on your servers, a key with "vip" for executives in your organization, or a key with "sales" for the sales department, etc. This way you can use the tags on various sensors to figure out different detection and response rules for different types of hosts on your infrastructure.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

In LimaCharlie, an Organization ID (OID) is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Browser Agent

## Related Articles

### Reference: EDR Events
Comprehensive reference documentation for all EDR events captured by LimaCharlie agents.

### Reference: Endpoint Agent Commands
Complete reference for all commands available to control and query LimaCharlie endpoint agents.

### ChromeOS with Google Chrome Enterprise
Documentation for deploying and managing LimaCharlie agents on ChromeOS devices using Google Chrome Enterprise.

### Edge Agent Installation
Step-by-step guide for installing the LimaCharlie agent on Microsoft Edge browsers.

### Chrome Agent Installation
Step-by-step guide for installing the LimaCharlie agent on Google Chrome browsers.

### Endpoint Agent Installation
Complete guide for installing LimaCharlie endpoint agents across different platforms and environments.

### Endpoint Agent Commands
Documentation for executing commands on LimaCharlie endpoint agents to perform security operations and investigations.

### Endpoint Agent
Overview of the LimaCharlie endpoint agent architecture, capabilities, and core functionality.

### Endpoint Agent Events Overview
Introduction to the event telemetry captured by LimaCharlie endpoint agents for security monitoring and detection.

---

# Ingesting Linux Audit Logs

The LimaCharlie sensor can ingest Linux Audit logs and forward them to the LimaCharlie cloud for processing, storage, and analysis.

## Prerequisites

- LimaCharlie sensor installed on a Linux system
- Linux Audit daemon (`auditd`) installed and configured
- Appropriate permissions to read audit logs

## Configuration

To enable Linux Audit log ingestion, you need to configure the sensor to monitor the audit log files.

### Step 1: Configure Linux Audit

First, ensure that `auditd` is installed and running on your Linux system:

```bash
sudo systemctl status auditd
```

If not installed, install it using your package manager:

```bash
# Debian/Ubuntu
sudo apt-get install auditd

# RHEL/CentOS
sudo yum install audit
```

### Step 2: Configure Audit Rules

Configure audit rules to capture the events you're interested in. Edit `/etc/audit/rules.d/audit.rules` or use the `auditctl` command.

Example audit rules:

```bash
# Monitor file access
-w /etc/passwd -p wa -k passwd_changes
-w /etc/shadow -p wa -k shadow_changes

# Monitor system calls
-a always,exit -F arch=b64 -S execve -k exec_commands

# Monitor network connections
-a always,exit -F arch=b64 -S socket -k network_socket
```

Load the rules:

```bash
sudo auditctl -R /etc/audit/rules.d/audit.rules
```

### Step 3: Configure LimaCharlie Sensor

Configure the LimaCharlie sensor to ingest audit logs by creating or modifying the sensor configuration.

You can configure this through the LimaCharlie web interface or via the CLI:

1. Navigate to your organization in the LimaCharlie web interface
2. Go to **Sensors**  select your sensor  **Configurations**
3. Add or modify the configuration to include audit log ingestion

Alternatively, use the LimaCharlie CLI or API to set the configuration:

```yaml
# Example configuration
audit_logs:
  enabled: true
  log_path: /var/log/audit/audit.log
  parser: linux_audit
```

### Step 4: Configure Log Collection

The sensor needs to be configured to collect the audit logs. This is typically done through the sensor's configuration file or via a D&R rule.

Example sensor command to enable audit log collection:

```bash
sensor_config:
  audit:
    enabled: true
    log_file: /var/log/audit/audit.log
```

### Step 5: Verify Ingestion

After configuration, verify that audit logs are being ingested:

1. Generate some audit events (e.g., modify `/etc/passwd`)
2. Check the LimaCharlie timeline for your sensor
3. Look for events with the `AUDIT_LOG` event type

## Event Format

Linux Audit logs ingested by LimaCharlie will appear as events with the following structure:

```json
{
  "event_type": "AUDIT_LOG",
  "timestamp": 1234567890,
  "audit_type": "SYSCALL",
  "audit_data": {
    "type": "SYSCALL",
    "msg": "audit(1234567890.123:456): ...",
    "arch": "x86_64",
    "syscall": "execve",
    "success": "yes",
    // Additional audit fields...
  }
}
```

## Detection and Response

You can create D&R rules to detect and respond to specific audit events. Example rule:

```yaml
detect:
  event: AUDIT_LOG
  op: and
  rules:
    - op: is
      path: event/audit_data/type
      value: EXECVE
    - op: contains
      path: event/audit_data/a0
      value: /bin/bash

respond:
  - action: report
    name: bash_execution_detected
```

## Performance Considerations

- Audit logs can be verbose; configure rules to capture only necessary events
- Monitor sensor resource usage when ingesting audit logs
- Consider log rotation and retention policies
- Use filters to reduce noise and focus on security-relevant events

## Troubleshooting

### Logs Not Appearing

1. Verify `auditd` is running: `sudo systemctl status auditd`
2. Check audit rules are loaded: `sudo auditctl -l`
3. Verify sensor has read permissions for audit log files
4. Check sensor connectivity and status in LimaCharlie console

### High Volume of Events

1. Refine audit rules to be more specific
2. Use exclusion rules to filter out noisy events
3. Adjust audit log buffer size if needed
4. Consider using audit dispatcher for real-time processing

## Best Practices

1. **Start Small**: Begin with a limited set of audit rules and expand as needed
2. **Test Rules**: Validate audit rules in a test environment before production deployment
3. **Monitor Performance**: Track the impact of audit logging on system performance
4. **Regular Review**: Periodically review and update audit rules based on security requirements
5. **Coordinate with SIEM**: If using a SIEM, ensure audit log ingestion complements your existing logging strategy

## Additional Resources

- [Linux Audit Documentation](https://man7.org/linux/man-pages/man8/auditd.8.html)
- [LimaCharlie D&R Rules](/docs/en/detection-and-response-rules)
- [Reference: EDR Events](/docs/en/reference-edr-events)

---

# Stdin

The Stdin adapter allows you to pipe data into the LimaCharlie sensor adapter from standard input.

## Basic Usage

```bash
cat data.log | limacharlie-adapter --type stdin
```

## Configuration

The stdin adapter accepts data line-by-line and forwards it to LimaCharlie for ingestion.

### Parameters

- `--type stdin`: Specifies the stdin adapter type
- Standard adapter parameters apply (API key, organization ID, etc.)

## Use Cases

- Quick testing and prototyping
- Integration with shell scripts
- Processing output from other commands
- One-time data imports

## Example

```bash
echo '{"event": "test"}' | limacharlie-adapter \
  --type stdin \
  --client-options '{"identity": {"oid": "your-org-id", "installation_key": "your-key"}}'
```

## Notes

- Input is processed line-by-line
- Each line should contain a complete event
- The adapter will continue reading until EOF is reached
- Suitable for streaming data sources

---

# Ingesting Linux Audit Logs

Linux Audit is a native kernel-level auditing framework that provides detailed logging of system calls, file access, and security events. LimaCharlie can ingest and process these audit logs alongside native EDR telemetry.

## Overview

The Linux Audit framework (auditd) generates logs that are valuable for security monitoring and compliance. LimaCharlie's sensor can ingest these logs and normalize them into the event stream for detection and response.

## Prerequisites

- Linux system with audit framework installed (`auditd` package)
- LimaCharlie sensor installed and running
- Appropriate permissions to read audit logs

## Configuration

### 1. Enable Audit Log Ingestion

Configure the sensor to ingest audit logs by adding a log ingestion rule:

```yaml
- log_type: audit
  log_path: /var/log/audit/audit.log
  format: audit
```

### 2. Set Up Audit Rules

Configure audit rules to capture relevant events. Example audit rules:

```bash
# Monitor file access
auditctl -w /etc/passwd -p rwxa -k passwd_changes
auditctl -w /etc/shadow -p rwxa -k shadow_changes

# Monitor system calls
auditctl -a always,exit -F arch=b64 -S execve -k exec_tracking

# Monitor network connections
auditctl -a always,exit -F arch=b64 -S socket -S connect -k network_activity
```

### 3. Sensor Configuration

Add the ingestion configuration to the sensor via the web UI or API:

```json
{
  "log_ingestion": {
    "audit": {
      "enabled": true,
      "log_path": "/var/log/audit/audit.log",
      "parser": "audit"
    }
  }
}
```

## Event Format

Audit logs are normalized into LimaCharlie events with the following structure:

```json
{
  "event_type": "AUDIT_LOG",
  "audit": {
    "type": "EXECVE",
    "timestamp": "2025-10-10T12:34:56.789Z",
    "serial": 12345,
    "pid": 1234,
    "uid": 1000,
    "gid": 1000,
    "command": "/usr/bin/example",
    "key": "exec_tracking",
    "raw": "type=EXECVE msg=audit(1696944896.789:12345): argc=2 a0=\"/usr/bin/example\" a1=\"arg1\""
  }
}
```

## Detection Rules

Create detection rules that leverage audit log data:

```yaml
event: AUDIT_LOG
op: and
rules:
  - path: audit/type
    op: is
    value: EXECVE
  - path: audit/command
    op: contains
    value: suspicious_binary
respond:
  - action: report
    name: suspicious_audit_execution
```

## Common Use Cases

### Monitor Privileged Access

```bash
# Track sudo usage
auditctl -w /usr/bin/sudo -p x -k sudo_usage

# Track su command
auditctl -w /bin/su -p x -k su_usage
```

### File Integrity Monitoring

```bash
# Monitor critical system files
auditctl -w /bin -p wa -k binary_changes
auditctl -w /sbin -p wa -k system_binary_changes
auditctl -w /usr/bin -p wa -k user_binary_changes
```

### Network Monitoring

```bash
# Track network connections
auditctl -a always,exit -F arch=b64 -S socket -S connect -S bind -k network_events
```

## Performance Considerations

- Audit logs can be verbose; filter appropriately to reduce volume
- Use specific audit rules rather than broad wildcards
- Monitor sensor resource usage when enabling audit ingestion
- Consider log rotation policies to manage disk space

## Troubleshooting

### Logs Not Appearing

1. Verify audit daemon is running: `systemctl status auditd`
2. Check audit rules are loaded: `auditctl -l`
3. Verify log file permissions allow sensor to read
4. Check sensor logs for ingestion errors

### High Volume Issues

- Review and refine audit rules to reduce noise
- Implement rate limiting at the audit level
- Use audit dispatcher filters
- Consider sampling for high-frequency events

## Additional Resources

- Linux Audit documentation: `man auditd`
- Audit rules examples: `man auditctl`
- LimaCharlie sensor configuration reference

---

# Sensors

No documentation content is available for this tag. This appears to be a tag index page that would normally list articles related to sensors, but currently contains no articles.

---

# Linux Agent Installation

The LimaCharlie Linux Sensor interfaces with the kernel to acquire deep visibility into the host's activity while taking measures to preserve the host's performance. We make full use of eBPF, which **requires Linux 4.4 or above**.

The Sensor current supports all Linux distributions (including ARM and MIPS).

## Linux Distribution Support

Our Linux Sensor fully utilizes eBPF, which requires at least Linux 4.4 or above. Use the command `uname -r` to check your kernel version to determine support.

## Installation Instructions

### System Requirements

All versions of Debian and CentOS starting around Debian 5 should be supported. Due to the high diversity of the ecosystem it's also likely to be working on other distributions. If you need a specific platform, contact us.

### Deb Package

If you are deploying on a Debian Linux system, we recommend using the `.deb` package. You can find a link to the Debian package for various architectures at [Downloading the Agent](https://docs.limacharlie.io/docs/endpoint-agent-installation#downloading-the-agents).

The deb package will install the LimaCharlie sensor using a `systemd` service, or if unavailable a `system V` service.

The Installation Key is required by the installer via the `debconf` configuration mechanism. By default, installing the package interactively will request the installation key via a local command/GUI interface. To perform large scale installations, we recommend setting the installation key programmatically.

**Installating interactively:**

```
sudo dpkg -i limacharlie.deb
```

or

```
sudo apt install ./limacharlie.deb
```

**Uninstalling interactively:**

```
sudo dpkg -r limacharlie
```

or

```
sudo apt remove limacharlie
```

**Installing and setting the installation key programmatically with dpkg:**

```
echo "limacharlie limacharlie/installation_key string INSTALLATION_KEY_HERE" | sudo debconf-set-selections && sudo dpkg -i limacharlie.deb
```

**Installing and setting the installation key programmatically with apt:**

```
echo "limacharlie limacharlie/installation_key string INSTALLATION_KEY_HERE" | sudo debconf-set-selections && sudo apt install ./limacharlie.deb -y
```

Debian packages are offered for the various architectures the Linux sensor suppports, like:

```
https://downloads.limacharlie.io/sensor/linux/64
https://downloads.limacharlie.io/sensor/linux/deb64
```

### Custom Installation

Executing the installer via the command line, pass the `-d INSTALLATION_KEY` argument where `INSTALLATION_KEY` is the key mentioned above.

Because Linux supports a plethora of service management frameworks, by default the LC sensor does not install itself onto the system. Rather it assumes the "current working directory" is the installation directory and immediately begins enrollment from there.

This means you can wrap the executable using the specific service management technology used within your Organization by simply specifying the location of the installer, the `-d INSTALLATION_KEY` parameter and making sure the current working directory is the directory where you want the few sensor-related files written to disk to reside.

A common methodology for Linux is to use `init.d`, if this is sufficient for your needs, see this [sample install script](https://github.com/refractionPOINT/lce_doc/blob/master/docs/lc_linux_installer.sh). You can invoke it like this:

```
sudo chmod +x ./lc_linux_installer.sh
sudo ./lc_linux_installer.sh <PATH_TO_LC_SENSOR> <YOUR_INSTALLATION_KEY>
```

You may also pass the value `-` instead of the `INSTALLATION_KEY` like: `-d -`. This will make the installer look for the installation key in an alternate place in the following order:

* Environment variable `LC_INSTALLATION_KEY`
* Text file in current working directory: `lc_installation_key.txt`

### Disabling Netlink

By default, the Linux sensor makes use of Netlink if available. In some rare configurations this auto-detection may be unwanted and Netlink usage can be disabled by setting the environment variable `DISABLE_NETLINK` to any value on the sensor process.

## Uninstalling the Agent

For additional agent uninstall options, see [Endpoint Agent Uninstallation](/v2/docs/endpoint-agent-uninstallation)

Linux agent uninstallation depends on how the sensor was installed. For example, if installed via a Debian package (`dpkg` file), you should uninstall via the same mechanism. If you installed via the SystemV installation method, please utilize the bottom of [this script](https://github.com/refractionPOINT/lce_doc/blob/master/docs/lc_linux_installer.sh#L97).

### Sensor Command

The `uninstall` command does not work for Linux systems. However, there is a chained command that can be run from the Sensor Console:

```
run --shell-command "service limacharlie stop; rm /bin/rphcp; update-rc.d limacharlie remove -f; rm -rf /etc/init.d/limacharlie; rm /etc/hcp ; rm /etc/hcp_conf; rm /etc/hcp_hbs"
```

The above command removes LimaCharlie and associated files from the system when run remotely. Note that the above command could also be coupled with a rule for automated sensor uninstallation, if necessary.

### Debian Systems

If the sensor was originally installed with the .deb file, this option is the cleanest uninstall method.

```
apt remove limacharlie
```

---

# Linux Agent Installation

The LimaCharlie Linux Sensor interfaces with the kernel to acquire deep visibility into the host's activity while taking measures to preserve the host's performance. We make full use of eBPF, which **requires Linux 4.4 or above**.

The Sensor current supports all Linux distributions (including ARM and MIPS).

## Linux Distribution Support

Our Linux Sensor fully utilizes eBPF, which requires at least Linux 4.4 or above. Use the command `uname -r` to check your kernel version to determine support.

## Installation Instructions

### System Requirements

All versions of Debian and CentOS starting around Debian 5 should be supported. Due to the high diversity of the ecosystem it's also likely to be working on other distributions. If you need a specific platform, contact us.

### Deb Package

If you are deploying on a Debian Linux system, we recommend using the `.deb` package. You can find a link to the Debian package for various architectures at [Downloading the Agent](https://docs.limacharlie.io/docs/endpoint-agent-installation#downloading-the-agents).

The deb package will install the LimaCharlie sensor using a `systemd` service, or if unavailable a `system V` service.

The Installation Key is required by the installer via the `debconf` configuration mechanism. By default, installing the package interactively will request the installation key via a local command/GUI interface. To perform large scale installations, we recommend setting the installation key programmatically.

**Installating interactively:**

```
sudo dpkg -i limacharlie.deb
```

or

```
sudo apt install ./limacharlie.deb
```

**Uninstalling interactively:**

```
sudo dpkg -r limacharlie
```

or

```
sudo apt remove limacharlie
```

**Installing and setting the installation key programmatically with dpkg:**

```
echo "limacharlie limacharlie/installation_key string INSTALLATION_KEY_HERE" | sudo debconf-set-selections && sudo dpkg -i limacharlie.deb
```

**Installing and setting the installation key programmatically with apt:**

```
echo "limacharlie limacharlie/installation_key string INSTALLATION_KEY_HERE" | sudo debconf-set-selections && sudo apt install ./limacharlie.deb -y
```

Debian packages are offered for the various architectures the Linux sensor suppports, like:

```
https://downloads.limacharlie.io/sensor/linux/64
https://downloads.limacharlie.io/sensor/linux/deb64
```

### Custom Installation

Executing the installer via the command line, pass the `-d INSTALLATION_KEY` argument where `INSTALLATION_KEY` is the key mentioned above.

Because Linux supports a plethora of service management frameworks, by default the LC sensor does not install itself onto the system. Rather it assumes the "current working directory" is the installation directory and immediately begins enrollment from there.

This means you can wrap the executable using the specific service management technology used within your Organization by simply specifying the location of the installer, the `-d INSTALLATION_KEY` parameter and making sure the current working directory is the directory where you want the few sensor-related files written to disk to reside.

A common methodology for Linux is to use `init.d`, if this is sufficient for your needs, see this [sample install script](https://github.com/refractionPOINT/lce_doc/blob/master/docs/lc_linux_installer.sh). You can invoke it like this:

```
sudo chmod +x ./lc_linux_installer.sh
sudo ./lc_linux_installer.sh <PATH_TO_LC_SENSOR> <YOUR_INSTALLATION_KEY>
```

You may also pass the value `-` instead of the `INSTALLATION_KEY` like: `-d -`. This will make the installer look for the installation key in an alternate place in the following order:

* Environment variable `LC_INSTALLATION_KEY`
* Text file in current working directory: `lc_installation_key.txt`

### Disabling Netlink

By default, the Linux sensor makes use of Netlink if available. In some rare configurations this auto-detection may be unwanted and Netlink usage can be disabled by setting the environment variable `DISABLE_NETLINK` to any value on the sensor process.

## Uninstalling the Agent

For additional agent uninstall options, see [Endpoint Agent Uninstallation](/v2/docs/endpoint-agent-uninstallation)

Linux agent uninstallation depends on how the sensor was installed. For example, if installed via a Debian package (`dpkg` file), you should uninstall via the same mechanism. If you installed via the SystemV installation method, please utilize the bottom of [this script](https://github.com/refractionPOINT/lce_doc/blob/master/docs/lc_linux_installer.sh#L97).

### Sensor Command

The `uninstall` command does not work for Linux systems. However, there is a chained command that can be run from the Sensor Console:

```
run --shell-command "service limacharlie stop; rm /bin/rphcp; update-rc.d limacharlie remove -f; rm -rf /etc/init.d/limacharlie; rm /etc/hcp ; rm /etc/hcp_conf; rm /etc/hcp_hbs"
```

The above command removes LimaCharlie and associated files from the system when run remotely. Note that the above command could also be coupled with a rule for automated sensor uninstallation, if necessary.

### Debian Systems

If the sensor was originally installed with the .deb file, this option is the cleanest uninstall method.

```
apt remove limacharlie
```

---

# Memory

The following sensor commands perform actions against memory on EDR sensors.

## get_debug_data

Retrieve debug data from the EDR sensor.

**Platforms:** Windows, Linux, macOS

**Return Event:** [DEBUG_DATA_REP](/v1/docs/reference-events-responses-memory#DEBUG_DATA_REP)

## mem_find_handle

Find specific open handles in memory on Windows.

**Platforms:** Windows

**Return Event:** [MEM_FIND_HANDLES_REP](/v1/docs/reference-events-responses-memory#MEM_FIND_HANDLES_REP)

**Usage:**

```
mem_find_handle [-h] needle

positional arguments:
  needle      substring of the handle names to get
```

## mem_find_string

Find specific strings in memory.

**Platforms:** Windows, Linux, macOS

**Return Event:** [MEM_FIND_STRING_REP](/v1/docs/reference-events-responses-memory#MEM_FIND_STRING_REP)

**Due to recent changes in MacOS, this may be less reliable on that platform.**

**Usage:**

```
mem_find_string [-h] -s STRING [STRING ...] pid

positional arguments:
  pid                   pid of the process to search in, 0 for all processes

optional arguments:
  -s STRING [STRING ...], --strings STRING [STRING ...]
                        list of strings to look for
```

## mem_handles

List all open handles from a process (or all) on Windows.

**Platforms:** Windows

**Return Event:** [MEM_HANDLES_REP](/v1/docs/reference-events-responses-memory#MEM_HANDLES_REP)

**Usage:**

```
mem_handles [-h] [-p PID] [-a PROCESSATOM]

optional arguments:
  -p PID, --pid PID     pid of the process to get the handles from, 0 for all
                        processes
  -a PROCESSATOM, --processatom PROCESSATOM
                        the atom of the target process
```

## mem_map

Display the map of memory pages from a process including size, access rights, etc.

**Platforms:** Windows, Linux, macOS

**Return Event:** [MEM_MAP_REP](/v1/docs/reference-events-responses-memory#MEM_MAP_REP)

**Due to recent changes in MacOS, this may be less reliable on that platform.**

**Usage:**

```
mem_map [-h] [-p PID] [-a PROCESSATOM]

optional arguments:
  -p PID, --pid PID     pid of the process to get the map from
  -a PROCESSATOM, --processatom PROCESSATOM
                        the atom of the target proces
```

## mem_read

Retrieve a chunk of memory from a process given a base address and size.

**Platforms:** Windows, Linux, macOS

**Return Event:** [MEM_READ_REP](/v1/docs/reference-events-responses-memory#MEM_READ_REP)

**Due to recent changes in MacOS, this may be less reliable on that platform.**

**Usage:**

```
mem_read [-h] [-p PID] [-a PROCESSATOM] baseAddr memSize

positional arguments:
  baseAddr              base address to read from, in HEX FORMAT
  memSize               number of bytes to read, in HEX FORMAT

optional arguments:
  -p PID, --pid PID     pid of the process to get the map from
  -a PROCESSATOM, --processatom PROCESSATOM
                        the atom of the target process
```

## mem_strings

List strings from a process's memory.

**Platforms:** Windows, Linux, macOS

**Return Event:** [MEM_STRINGS_REP](/v1/docs/reference-events-responses-memory#MEM_STRINGS_REP)

**Due to recent changes in MacOS, this may be less reliable on that platform.**

**Usage:**

```
mem_strings [-h] [-p PID] [-a PROCESSATOM]

optional arguments:
  -p PID, --pid PID     pid of the process to get the strings from
  -a PROCESSATOM, --processatom PROCESSATOM
                        the atom of the target process
```

---

# Mitigation

The following sensor commands perform mitigation actions against EDR sensors.

## deny_tree

Tells the sensor that all activity starting at a specific process (and its children) should be denied and killed. This particular command is excellent for ransomware mitigation.

**Platforms:**

**Usage:**

```
usage: deny_tree [-h] atom [atom ...]

positional arguments:
  atom        atoms to deny from
```

## rejoin_network

Tells the sensor to allow network connectivity again (after it was segregated).

**Platforms:**

**Return Event:**
 [REJOIN_NETWORK](/v1/docs/reference-events-responses-mitigation#REJOIN_NETWORK)

**Usage:**

```
usage: rejoin_network [-h]
```

## segregate_network

Tells the sensor to stop all network connectivity on the host except LC comms to the backend. So it's network isolation, great to stop lateral movement.

Note that you should never upgrade a sensor version while the network is isolated through this mechanism. Doing so may result in the agent not regaining connectivity to the cloud, requiring a reboot to undo.

This command primitive is NOT persistent, meaning a sensor you segregate from the network using this command alone, upon reboot will rejoin the network. To achieve isolation from the network in a persistent way, see the `isolate network` and `rejoin network` [Detection & Response rule actions](/v1/docs/detection-and-response).

**Platforms:**

**Return Event:**
 [SEGREGATE_NETWORK](/v1/docs/reference-events-responses-mitigation#SEGREGATE_NETWORK)

**Usage:**

```
usage: segregate_network [-h]
```

---

# Reference: Sensor Selector Expressions

Many components in LimaCharlie require selecting a set of Sensors based on some characteristics. The selector expression is a text field that describe what matching characteristics the selector is looking for.

## Available Fields

The following fields are available in this evaluation:

* `sid`: the Sensor ID
* `oid`: the Organization ID
* `iid`: the Installation Key ID
* `plat`: the Platform name (see [platforms](/v2/docs/reference-id-schema#platforms))
* `ext_plat`: the Extended Platform name (see [platforms](/v2/docs/reference-id-schema#platforms))
* `arch`: the Architecture name (see [architectures](/v2/docs/reference-id-schema#architecture))
* `enroll`: the Enrollment as a second epoch timestamp
* `hostname`: the hostname
* `mac_addr`: the latest MAC address
* `alive`: second epoch timestamp of the last time the Sensor connected to the cloud
* `ext_ip`: the last external IP
* `int_ip` the last internal IP
* `isolated`: a boolean True if the sensor's network is isolated
* `should_isolate`: a boolean True if the sensor is marked to be isolated
* `kernel`: a boolean True if the sensor has some sort of "kernel" enhanced visibility
* `did`: the Device ID the sensor belongs to
* `tags`: the list of tags the sensor currently has

## Available Operators

The following are the available operators:

* `==`: equals
* `!=`: not equal
* `in`: element in list, or substring in string
* `not in`: element not in list, or substring not in string
* `matches`: element matches regular expression
* `not matches`: element does not match regular expression

## Examples

* all sensors with the test tag: `test in tags`
* all windows boxes with an internal IP starting in 10.3.x.x: `` plat == windows and int_ip matches `^10\.3\..*` ``
* all 1password sensors, strings starting with a number need to be quoted with a backtick: `` plat == `1password` ``
* all linux with network isolation or evil tag: `plat == linux or (isolated == true or evil in tags)`

## Key Concepts

### Sensor ID

In LimaCharlie, a Sensor ID is a unique identifier assigned to each deployed endpoint agent (sensor). It distinguishes individual sensors across an organization's infrastructure, allowing LimaCharlie to track, manage, and communicate with each endpoint. The Sensor ID is critical for operations such as sending commands, collecting telemetry, and monitoring activity, ensuring that actions and data are accurately linked to specific devices or endpoints.

### Organization ID

In LimaCharlie, an Organization ID is a unique identifier assigned to each tenant or customer account. It distinguishes different organizations within the platform, enabling LimaCharlie to manage resources, permissions, and data segregation securely. The Organization ID ensures that all telemetry, configurations, and operations are kept isolated and specific to each organization, allowing for multi-tenant support and clear separation between different customer environments.

### Installation Keys

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

### Sensors

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Registry

## reg_list

List the keys and values in a Windows registry key.

**Platforms:** Windows

```
usage: reg_list [-h] reg

positional arguments:
  reg         registry path to list, must start with one of "hkcr", "hkcc", "hkcu", "hklm", "hku", e.g. "hklm\software"...
```

---

# Sensor Commands

Sensor commands offer a safe way to interact with a sensor's host either for investigation, management, or threat mitigation purposes. Commands are categorized by their overall functionality, and include the following:

* [Artifact Collection](/v1/docs/sensor-commands-artifact-collection)
  + `artifact_get`
* [Anomalies](/v1/docs/sensor-commands-anomalies)
  + `hidden_module_scan`
* [Documents](/v1/docs/sensor-commands-documents)
  + `doc_cache_get`
* [File and Registry Integrity Monitoring](/v1/docs/sensor-commands-fim)
  + `fim_add`
  + `fim_del`
  + `fim_get`
* [Files and Directories](/v1/docs/sensor-commands-files-and-directories)
  + `dir_find_hash`
  + `dir_list`
  + `file_del`
  + `file_get`
  + `file_hash`
  + `file_info`
  + `file_mov`
  + `log_get`
* [Management](/v1/docs/sensor-commands-management)
  + `exfil_add`
  + `exfil_del`
  + `exfil_get`
  + `history_dump`
  + `seal`
  + `set_performance_mode`
  + `restart`
  + `uninstall`
* [Memory](/v1/docs/sensor-commands-memory)
  + `get_debug_data`
  + `mem_find_handle`
  + `mem_find_string`
  + `mem_handles`
  + `mem_map`
  + `mem_read`
  + `mem_strings`
* [Mitigation](/v1/docs/sensor-commands-mitigation)
  + `deny_tree`
  + `rejoin_network`
  + `segregate_network`
* [Network](/v1/docs/sensor-commands-network)
  + `dns_resolve`
  + `netstat`
  + `pcap_ifaces`
* [Operating System](/v1/docs/sensor-commands-operating-system)
  + `os_autoruns`
  + `os_drivers`
  + `os_kill_process`
  + `os_packages`
  + `os_processes`
  + `os_resume`
  + `os_services`
  + `os_suspend`
  + `os_users`
* [Payloads](/v1/docs/sensor-commands-payloads)
  + `run`
  + `put`
* [Registry](/v1/docs/sensor-commands-registry)
  + `reg_list`
* [System](/v1/docs/sensor-commands-system)
  + `logoff`
  + `reboot`
  + `shutdown`
* [YARA](/v1/docs/sensor-commands-yara)
  + `yara_scan`
  + `yara_update`

## Sending Commands

Commands can be sent to Sensors via:

* Manually using the Console of a sensor in the [web application](https://app.limacharlie.io).
* Manually using the [CLI](https://github.com/refractionPOINT/python-limacharlie)
* Programatically in the response action of a [Detection & Response](/v1/docs/detection-and-response) rule, via the `task` action.
* Programatically using the [REST API](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI0OQ-task-sensor)

### Sensor Response Events

Regardless of which you choose, sent commands will be acknowledged immediately with an empty response, followed by a `CLOUD_NOTIFICATION` event being sent by the sensor. The content of command outputs are delivered as sensor [events](/v1/docs/reference-events-responses) suffixed with `_REP`, depending on the command.

**Please ensure that you have enabled the appropriate response event(s) in Event Collection to ensure that you will receive the Sensor response.**

This non-blocking approach makes responses accessible via the [event streams](/v1/docs/sensors) passing through Detection & Response rules and Outputs.

## Structure

Commands follow typical CLI conventions using a mix of positional arguments and named optional arguments.

Here's `dir_list` as an example:

```
dir_list [-h] [-d DEPTH] rootDir fileExp

positional arguments:
    rootDir     the root directory where to begin the listing from
    fileExp     a file name expression supporting basic wildcards like * and ?

optional arguments:
    -h, --help      show this help message and exit
    -d DEPTH, --depth DEPTH     optional maximum depth of the listing, defaults to a single level
```

The Console in the web application will provide autocompletion hints of possible commands for a sensor and their parameters. For API users, commands and their usage details may be retrieved via the [`/tasks`](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI1OQ-get-possible-tasks) and [`/task`](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI3OA-autocomplete-task) REST API endpoints.

## Investigation IDs

To assist in finding the responses more easily, you may specify an arbitrary `investigation_id` string with a command. The response will then include that value under `routing/investigation_id`. Under the hood, this is exactly how the Console view in the web application works.

If an `investigation_id` is prefixed with `__` (double underscore) it will omit the resulting events from being forwarded to Outputs. This is primarily to allow Services to interact with sensors without spamming.

## Command Line Format

When issuing commands to sensors as a command line (versus a list of tokens), the quoting and escaping of arguments can be confusing. This is a short explanation:

The command line tasks are parsed as if they were issued to a shell like `sh` or `cmd.exe` with a few tweaks to make it easier and more intuitive to use.

Arguments are parsed as separated by spaces, like: `dir_list /home/user *` is equal to 2 arguments: `/home/user` and `*`.

If an argument contains spaces, for example a single directory like `/file/my files`, you must use either single (`'`) or double (`"`) quotes around the argument, like: `dir_list "/files/my files"`.

A backslash (`\`), like in Windows file paths does not need to be escaped. It is only interpreted as an escape character when it is followed by a single or double quote.

The difference between single quotes and double quotes is that double quotes support escaping characters within using `\`, while single quotes never interpret `\` as an escape character. For example:

* `log_get --file "c:\temp\my dir\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file 'c:\temp\my dir\' --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file 'c:\temp\my dir\' --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`
* `log_get --file "c:\temp\my dir\" --type json` becomes `log_get`, `--file`, `c:\temp\my dir\`, `--type`, `json`

This means that as a general statement, unless you want to embed quoted strings within specific arguments, it is easier to use single quotes around arguments and not worry about escaping `\`.

---

# Sensor Connectivity

The network connection required by the LimaCharlie Sensor is very simple. It requires a single TCP connection over port 443 to a specific domain, and optionally another destination for the [Artifact Collection](/v2/docs/artifacts) service.

The specific domains are listed in the Sensor Downloads section of your Organization's dashboard. They will vary depending on the datacenter you chose to create your organization in. To find yours, see the screenshots below.

1. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(312).png)
2. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(313).png)

Currently, web proxies are not supported, but since LimaCharlie requires a single connection to a single dedicated domain, it makes creating a single exception safe and easy.

## Proxy Tunneling

The LimaCharlie sensor supports unauthenticated proxy tunneling through [HTTP CONNECT](https://en.wikipedia.org/wiki/HTTP_tunnel).

This allows the LimaCharlie connection to go through the proxy in an opaque way (since the sensor does not support SSL interception).

To activate this feature, set the `LC_PROXY` environment variable to the DNS or hostname of the proxy to use. For example you could use: `LC_PROXY=proxy.corp.com:8080`.

### Windows

On Windows, you may use a light auto-detection of a globally-configured, unauthenticated proxy.

To enable this, set the same environment variable to the `-` value, like `LC_PROXY=-`. This will make the sensor query the registry key `HKLM\Software\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\ProxyServer` and use its value as the proxy destination.

Also on Windows, in some cases the environment variable changes do not propagate to all processes in the expected way. Usually a reboot of the machine will fix it, but for machines that cannot be rebooted you have the ability to set a special value to the environment variable (deletion is usually problematic but setting a var works) that will disable the proxy specifically: `!`. So if you set the `LC_PROXY` variable to `!` (exclamation mark), the proxy will be disabled.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Sensor Connectivity

The network connection required by the LimaCharlie Sensor is very simple. It requires a single TCP connection over port 443 to a specific domain, and optionally another destination for the [Artifact Collection](/v2/docs/artifacts) service.

The specific domains are listed in the Sensor Downloads section of your Organization's dashboard. They will vary depending on the datacenter you chose to create your organization in. To find yours, see the screenshots below.

1. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(312).png)
2. ![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(313).png)

Currently, web proxies are not supported, but since LimaCharlie requires a single connection to a single dedicated domain, it makes creating a single exception safe and easy.

## Proxy Tunneling

The LimaCharlie sensor supports unauthenticated proxy tunneling through [HTTP CONNECT](https://en.wikipedia.org/wiki/HTTP_tunnel).

This allows the LimaCharlie connection to go through the proxy in an opaque way (since the sensor does not support SSL interception).

To activate this feature, set the `LC_PROXY` environment variable to the DNS or hostname of the proxy to use. For example you could use: `LC_PROXY=proxy.corp.com:8080`.

### Windows

On Windows, you may use a light auto-detection of a globally-configured, unauthenticated proxy.

To enable this, set the same environment variable to the `-` value, like `LC_PROXY=-`. This will make the sensor query the registry key `HKLM\Software\Policies\Microsoft\Windows\CurrentVersion\Internet Settings\ProxyServer` and use its value as the proxy destination.

Also on Windows, in some cases the environment variable changes do not propagate to all processes in the expected way. Usually a reboot of the machine will fix it, but for machines that cannot be rebooted you have the ability to set a special value to the environment variable (deletion is usually problematic but setting a var works) that will disable the proxy specifically: `!`. So if you set the `LC_PROXY` variable to `!` (exclamation mark), the proxy will be disabled.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Sensor Cull

The Sensor Cull Extension performs continuous cleaning of "old" sensors that have not connected to an Organization within a set period of time. This is useful for environments with cloud deployments or VM/template-based deployments that may enroll sensors repeatedly, and for a short period of time.

The extension works by creating rules that describe when specified sensors should be cleaned up.

## Enabling the Sensor Cull Extension

To enable the Sensor Cull extension, navigate to the Sensor Cull extension page in the LimaCharlie marketplace.

After clicking **Subscribe**, the Sensor Cull extension should be available almost immediately.

## Using the Sensor Cull Extension

Once enabled, you will see a **Sensor Cull** option under **Sensors** within the LimaCharlie web UI. You can also interact with the extension via REST API.

Within the Sensor Cull module, you have the ability to create rules. Sensor Cull rules are run automatically once a day, and can be edited as needed.

Each rule specifies a single sensor `tag` used as a selector for the sensors the rule applies to. A rule also has a `name` (simply used for your bookkeeping), and a `ttl` which is the number of days a sensor can remain unconnected to LimaCharlie before it becomes eligible for cleanup.

## Actions via REST API

The following REST API actions can be sent to interact with the Sensor Cull extension:

### get_rules

Get the list of existing rules

```
{
  "action": "get_rules"
}
```

### run

Perform an ad-hoc cleanup.

```
{
  "action": "run"
}
```

### add_rule

The following example creates a rule name `my new rule` that applies to all sensors with the `vip` Tag, and cleans them up when they have not connected in 30 days.

```
{
  "action": "add_rule",
  "name": "my new rule",
  "tag": "vip",
  "ttl": 30
}
```

### del_rule

Delete an existing rule by name.

```
{
  "action": "del_rule",
  "name": "my new rule"
}
```

---

# Sensor Cull

The Sensor Cull Extension performs continuous cleaning of "old" sensors that have not connected to an Organization within a set period of time. This is useful for environments with cloud deployments or VM/template-based deployments that may enroll sensors repeatedly, and for a short period of time.

The extension works by creating rules that describe when specified sensors should be cleaned up.

## Enabling the Sensor Cull Extension

To enable the Sensor Cull extension, navigate to the [Sensor Cull extension page](https://app.limacharlie.io/add-ons/extension-detail/ext-sensor-cull) in the LimaCharlie marketplace.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/sensor-cull-1.png)

After clicking **Subscribe**, the Sensor Cull extension should be available almost immediately.

## Using the Sensor Cull Extension

Once enabled, you will see a **Sensor Cull** option under **Sensors** within the LimaCharlie web UI. You can also interact with the extension via REST API.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/sensor-cull-2.png)

Within the Sensor Cull module, you have the ability to create rules. Sensor Cull rules are run automatically once a day, and can be edited as needed.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/sensor-cull-3.png)

Each rule specifies a single sensor `tag` used as a selector for the sensors the rule applies to. A rule also has a `name` (simply used for your bookkeeping), and a `ttl` which is the number of days a sensor can remain unconnected to LimaCharlie before it becomes eligible for cleanup.

## Actions via REST API

The following REST API actions can be sent to interact with the Sensor Cull extension:

### get_rules

Get the list of existing rules

```
{
  "action": "get_rules"
}
```

### run

Perform an ad-hoc cleanup.

```
{
  "action": "run"
}
```

### add_rule

The following example creates a rule name `my new rule` that applies to all sensors with the `vip` Tag, and cleans them up when they have not connected in 30 days.

```
{
  "action": "add_rule",
  "name": "my new rule",
  "tag": "vip",
  "ttl": 30
}
```

### del_rule

Delete an existing rule by name.

```
{
  "action": "del_rule",
  "name": "my new rule"
}
```

---

# Sensor Tags

Tags in LimaCharlie are simple strings that can be associated with any number of sensors. A Sensor can also have an arbitrary number of tags associated with it.

Tags appear in every event coming from a sensor under the `routing` component of the event. This greatly simplifies the writing of detection and response rules based on the presence of specific tags, at the cost of including more non-unique data per event.

Tags can be used for a variety of purposes, including:

* to classify endpoints
* automate detection and response
* create powerful workflows
* trigger automations

## Use Cases for Sensor Tags

### Classification

You can use tags to classify an endpoint in a number of different ways based on what is important to you. Some examples of classifications are shown below for inspiration.

**Departments**

Create tags to classify endpoints based on what business department they belong to. e.g. sales, finance, operations, development, support, legal, executives.

**Usage Type**

You may wish to tag endpoints based on their type of usage. e.g. workstation, server, production, staging.

By having endpoints tagged in this manner you can easily identify endpoints and decide what actions you may wish to take while considering the tag. For example, if you see an endpoint is tagged with `workstation` and `executives`, and you happen to see suspicious activity on the endpoint, it may be worthwhile for you to prioritize response.

### Automating detection and response

You can use tags to automate detection and response.

For example, you can create a detection & response rule so that when a specific user logs in on a device, the box is tagged as `VIP-sales` and the sensor starts collecting an extended list of events from that box.

### Creating workflows

You can use tags to create workflows and automations. For instance, you can configure an output (forwarder) to send all detections containing `VIP-sales` tag to Slack so that you can review them asap, while detections tagged as `sales` can be sent to an email address.

### Trigger Automations

Create a Yara scanning rule so that endpoints tagged as 'sales' are continuously scanned against the specific sets of Yara signatures.

## Adding Tags

Tags can be added to a sensor a few different ways:

1. Enrollment: the installation keys can optionally have a list of Tags that will get applied to sensors that use them.
2. Manually: using the API as described below, either manually by a human or through some other integration.
3. Detection & Response: automated detection and response rules can programatically add a tag (and check for tags).

### Manual API

Issue a `POST` to `/{sid}/tags` REST endpoint

### Detection & Response

In detection and response rules. To achieve this, in the response part of the detection & response rule, specify the add tag action. For example, to tag a device as DESKTOP, you would say:

```
- action: add tag
tag: DESKTOP
```

## Removing Tags

### Manual API

Issue a `DELETE` to `/{sid}/tags` REST endpoint

### Detection & Response

In detection and response rules

### Manual in the web app

In the web app, click on the sensor in question to expand it. You will see the list of tags you can add/edit/remove.

## Checking Tags

### Manual API

Issue a `GET` to `/{sid}/tags` REST endpoint

### Detection & Response

In detection and response rules

## System Tags

We provide system level functionality with a few system tags. Those tags are listed below for reference:

### lc:latest

When you tag a sensor with `lc:latest`, the sensor version currently assigned to the Organization will be ignored for that specific sensor, and the latest version of the sensor will be used instead. This means you can tag a representative set of computers in the Organization with the `lc:latest` tag in order to test-deploy the latest version and confirm no negative effects.

### lc:stable

When you tag a sensor with `lc:stable`, the sensor version currently assigned to the Organization will be ignored for that specific sensor, and the *stable* version of the sensor will be used instead. This means you can upgrade an organization as a whole, but leave a few specific sensors behind by assigning the lc:stable tag to them.

### lc:experimental

When you tag a sensor with `lc:experimental`, the sensor version currently assigned to the Organization will be ignored for that specific sensor. An experimental version of the sensor will be used instead. This tag is typically used when working with the LimaCharlie team to troubleshoot sensor-specific issues.

### lc:no_kernel

When you tag a sensor with `lc:no_kernel`, the kernel component will not be loaded on the host.

### lc:debug

When you tag a sensor with `lc:debug`, the debug version of the sensor currently assigned to the Organization will be used.

### lc:limit-update

When you tag a sensor with lc:limit-update, the sensor will not update the version it's running at run-time. The version will only be loaded when the sensor starts from scratch like after a reboot.

### lc:sleeper

When you tag a sensor with *lc:sleeper*, the sensor will keep its connection to the LimaCharlie Cloud, but will disable all other functionality to avoid any impact on the system.

### lc:usage

When you tag a sensor with *lc:usage*, the sensor will work as usual, but its connection will not count against the normal sensor quota. Instead, the time the sensor spends connected will be billed separately per second, and so will events received by the sensor. For more details, see [Sleeper Deployments](/v2/docs/sleeper).

---

# Sensor Uninstallation

There are multiple options available to uninstall the LimaCharlie sensor, depending on the operating system and/or method of installation. macOS and Windows systems allow for easy uninstallation via sensor commands or D&R rules. Linux systems may require additional steps, as detailed below.

## macOS and Windows

When uninstalling macOS and Windows Sensors, please attempt to utilize a method similar to sensor deployment. For example, if sensors were deployed via a package manager, then the same package manager may have uninstall options as well. This will help keep software inventories up to date.

### Manual Uninstallation

#### Windows EXE

On Windows, the LimaCharlie sensor can be uninstalled on individual endpoints by running the installer EXE with the `-c` argument, which will remove the Sensor and its service entirely.

Example:

```
C:\Windows\System32\rphcp.exe -c
del C:\Windows\System32\rphcp.exe
```

#### Windows MSI

If uninstalling via the Windows MSI installer, the `/x` switch can be used to uninstall.

Example:

```
msiexec /x lc_sensor.msi /qn
```

If you run into issues where the service is still present (`sc.exe query rphcpsvc`) or the exe is still left behind (`C:\Windows\System32\rphcp.exe`), you can remove them with the following:

```
C:\Windows\System32\rphcp.exe -c       # If this throws an error about the service, it's safe to ignore
del C:\Windows\System32\rphcp.exe
```

#### macOS

Manual installation of macOS Sensors is detailed [here](/v1/docs/macos-sensor-installation-latest-os-versions#uninstallation-flow).

### Sensor Commands

For macOS and Windows operating systems, you can uninstall a sensor with the `uninstall` command. More information on that can be found [here](/v1/docs/sensor-commands-management#uninstall).

On Windows, the command defaults to uninstalling the sensor as if installed from the direct installer exe. If an MSI was used for installation, you can add a `--msi` flag to the `uninstall` command to trigger an uninstallation that is compatible with MSI.

### SDK

To run the uninstall command against *all* Sensors, a simple loop with the SDK in Python would work:

```python
import limacharlie
lc = limacharlie.Manager()
for sensor in lc.sensors():
  sensor.task( 'uninstall' )
```

### Using a D&R Rule

As an alternative approach, you can also use a Detection & Response (D&R) rule to automatically trigger an uninstall of the LimaCharlie sensor when a sensor connects to the LimaCharlie cloud. Below is an example of the rule you can use for this purpose. This example is specific to Windows-based endpoints, but can be modified based on your needs:

```yaml
# Detect
event: SYNC
op: is windows

# Respond
- action: task
  command: uninstall --is-confirmed
- action: add tag
  tag: uninstalled
```

### Package Management Tools

For Package Management tools, and other enterprise application-management tools, we recommend utilizing the integrated program removal options, rather than installing from LimaCharlie. This will help keep software inventories up to date.

## Linux

Linux Sensor uninstallation depends on how the sensor was installed. For example, if installed via a Debian package (`dpkg` file), you should uninstall via the same mechanism. If you installed via the SystemV installation method, please utilize the bottom of [this script](https://github.com/refractionPOINT/lce_doc/blob/master/docs/lc_linux_installer.sh#L97).

### Sensor Command

The `uninstall` command does not work for Linux systems. However, there is a chained command that can be run from the Sensor Console:

```bash
run --shell-command "service limacharlie stop; rm /bin/rphcp; update-rc.d limacharlie remove -f; rm -rf /etc/init.d/limacharlie; rm /etc/hcp ; rm /etc/hcp_conf; rm /etc/hcp_hbs"
```

The above command removes LimaCharlie and associated files from the system when run remotely. Note that the above command could also be coupled with a D&R rule for automated sensor uninstallation, if necessary.

### Debian Systems

If the sensor was originally installed with the .deb file, this option is the cleanest uninstall method.

```bash
apt remove limacharlie
```

---

# Sensors

## Overview

In LimaCharlie, **Sensors** refer to the broad set of telemetry collection mechanisms used to gather, process, and act on data across a variety of environments. Sensors come in different forms, each designed to capture specific types of data to provide comprehensive visibility and security across endpoints, networks, and cloud platforms.

* [**Endpoint Agents**](/v2/docs/endpoint-agent) are lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more. The data is streamed back to the LimaCharlie platform where it can be analyzed, used in detection and response rules, or forwarded to other systems. Endpoint agents provide detailed, granular insight into endpoint security and are typically used for threat detection, investigation, and response.
* [**Adapters**](/v2/docs/adapters) serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

  + **On-Premise Adapters** are deployed within a local infrastructure and are capable of ingesting structured data from various sources like JSON, Syslog, or CEFL. These adapters transform any available telemetry into first-class data streams that can be integrated into LimaCharlie's detection and automation systems, allowing you to act on data from non-traditional sources.
  + **Cloud Adapters** operate in cloud-to-cloud connections, enabling direct data intake from cloud platforms and services. LimaCharlie offers pre-defined Adapters for common cloud services like AWS CloudTrail, Google Cloud, and Microsoft 365. These Adapters are typically configured to ingest log data or events without the need for custom deployment, making it easy to centralize cloud-based telemetry for detection and analysis.

All Sensor typeswhether endpoint agents or adaptersare recognized as first-class data sources in LimaCharlie, giving users complete flexibility to manage, analyze, and automate security workflows based on the data collected.

> Note on network connectivity:
>
> You can find details about how the LimaCharlie Sensors communicate with the cloud in our [FAQ](/v2/docs/faq-sensor-installation).

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Sensors

Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Once installed, they send telemetry and artifacts from a host to their registered organization. The sensor is grounded in LimaCharlie's open source EDR roots, but is flexible in bringing security data in from different sources.

## Sensor Types

* [Windows](/v1/docs/windows)
* [Mac](/v1/docs/sensors-operating-systems-macos)
* [Linux](/v1/docs/linux)
* [Chrome](/v1/docs/chrome)
* [Edge](/v1/docs/sensors-operating-systems-edge)

> **Note**: Don't see what you're looking for? Need support for a platform you don't see here? Get in touch via [Slack](https://slack.limacharlie.io) or email.

## Quota

All sensors register with the cloud, and many of them may go online / offline over the course of a regular day. For billing purposes, organizations must specify a sensor quota which represents the number of **concurrent online sensors** allowed to be connected to the cloud.

If the quota is maxed out when a sensor attempts to come online, the sensor will be dismissed and a [`sensor_over_quota`](/v1/docs/reference-events-deployment) event will be emitted in the deployments stream.

## Events

All sensors observe host & network activity, packaging telemetry and sending it to the cloud. The types of observable events are dependent on the sensor's type.

> For an introduction to events and their structure, check out [Events](/v1/docs/reference-events).

## Commands

Windows, Mac, Linux, Chrome, and Edge sensors all offer commands as a safe way of interacting with a host for investigation, management, or threat mitigation purposes.

> For an introduction to commands and their usage, check out [Sensor Commands](/v1/docs/sensors-sensor-commands). Alternatively, check out any the sensor types individually to see their supported commands.

## Installation Keys

An Installation Key binds a sensor to the Organization that generated the key, optionally tagging them as well to differentiate groups of sensors from one another.

It has the following properties:

* OID: The Organization Id that this key should enroll into.
* IID: Installer ID that is generated and associated with every Installation Key.
* Tags: A list of Tags automatically applied to sensors enrolling with the key.
* Desc: The description used to help you differentiate uses of various keys.

### Recommended Usage

We recommend using multiple installation keys per organization to differentiate endpoints in your deployment.

For example, you may create a key with Tag "server" that you will use to install on your servers, a key with "vip" for executives in your organization, or a key with "sales" for the sales department, etc. This way you can use the tags on various sensors to figure out different detection and response rules for different types of hosts on your infrastructure.

## Sensor Versions

Windows, Mac, and Linux (EDR-class) sensors' versions are fixed and can be managed per-organization. They will not upgrade unless you choose to do so.

There are always two versions available to roll out  `Stable` or `Latest`  which can be deployed via the web application or via the [`/modules` REST API](https://doc.limacharlie.io/docs/api/b3A6MTk2NDI2OA-update-sensors).

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

---

# Test a New Sensor Version

Prior to rolling out a new Sensor version, we recommend testing to ensure everything works as intended within your environment. While we test Sensors before releasing them, we cannot predict every niche use case. We also recommend testing on `dev` or `test` systems prior to deployment in production, again, to eliminate any concerns of resource utilization or Sensor operations.

Sensor version testing is done via LimaCharlie's tagging functionality.

When you tag a Sensor with `lc:latest`, the sensor version currently assigned to the Organization will be ignored for that specific sensor, and the latest version of the sensor will be used instead. You can apply this tag to a handful of systems to test-deploy the latest version.

Alternatively, you can tag a sensor with `lc:stable`. Similarly, the sensor version currently assigned to the Organization will be ignored for that specific sensor, and the stable version of the sensor will be used instead.

You can tag a Sensor by opening the sensors list, selecting a sensor you would like to test, and navigating to the `tags` field on the sensor `Overview`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(314).png)

Simply type `lc:stable` and click `Update Tags`.

Note: It can take up to 10 minutes to update the sensor to the tagged version.

## Related Concepts

**Sensors**: Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Organization**: In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Updating Sensors to the Newest Version

LimaCharlie releases a new version of the Sensor frequently - often every few weeks. However, we give you full control over what sensor version is running in your Organization. Sensors are not updated by default.

There are two methods for updating sensors in your organization to the latest version.

## Manual Update

Upgrading sensors is done transparently for you once you click the "Update to Latest" button, located at `Sensors > Deployed Versions`.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(316).png)

The new version should be in effect across the organization within about 20 minutes.

## Automated Update

You can also configure sensors in your organization to auto-update to the new version when it's released. To do it, tag applicable (or all) sensors in your fleet with the `lc:stable` tag (`lc:stable` tag means that the package it provides rarely changes).

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(315).png)

This will ensure that when a new sensor version is released, it will be in effect across the organization within about 20 minutes.

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

In LimaCharlie, an Organization represents a tenant within the SecOps Cloud Platform, providing a self-contained environment to manage security data, configurations, and assets independently. Each Organization has its own sensors, detection rules, data sources, and outputs, offering complete control over security operations. This structure enables flexible, multi-tenant setups, ideal for managed security providers or enterprises managing multiple departments or clients.

---

# Windows Agent Installation

## Windows Agent Installation Instructions

### Windows MSI Installation Versioning

Our MSI packages are created programmatically, and thus will have a version of `1.0.0.0` upon compilation and download. Note Sensor versions will reflect the latest version as of the MSI download, and sensor version control is managed via the LimaCharlie application, not the MSI package.

### System Requirements

The LimaCharlie.io agent supports Windows XP 32 bit and up (32 and 64 bit). However, Windows XP and 2003 support is for the more limited capabilities of the agent that do not require kernel support.

### Installing via MSI

[Windows Installer](https://learn.microsoft.com/en-us/windows/win32/msi/windows-installer-portal) is an installation and configuration service provided with Windows. Microsoft Software Installer, or MSI, files allow for an easy and portable installation format on Windows systems.

LimaCharlie makes portable MSI files available with each new sensor release. They are available at a static URL for easy downloading:

* [32-bit MSI installer](https://downloads.limacharlie.io/sensor/windows/msi32)
* [64-bit MSI installer](https://downloads.limacharlie.io/sensor/windows/msi64)

Similar to executable installation, when installing with an MSI, you will need to provide the desired Installation Key, passed through as the `InstallationKey` variable.

**32-bit MSI installation command:**

```
hcp_win_x86_release_<sensor_version>.msi InstallationKey=<installation_key>
```

**64-bit MSI installation command:**

```
hcp_win_x64_release_<sensor_version>.msi InstallationKey=<installation_key>
```

Executing the installer via the command line, pass the `-i INSTALLATION_KEY` argument where `INSTALLATION_KEY` is the key mentioned above. This will install the sensor as a Windows service and trigger its enrollment.

You may also install the Windows sensor using the MSI version. With the MSI, install using:

```
installer.msi InstallationKey="INSTALLATION_KEY"
```

You may also pass the value `-` instead of the `INSTALLATION_KEY` like: `-i -`. This will make the installer look for the installation key in an alternate place in the following order:

* Environment variable `LC_INSTALLATION_KEY`
* Text file in current working directory: `lc_installation_key.txt`

#### Verify the service is running

In an administrative command prompt issue the command `sc query rphcpsvc` and confirm the `STATE` displayed is `RUNNING`.

## Uninstalling the Agent

For additional agent uninstall options, see [Endpoint Agent Uninstallation](/v2/docs/endpoint-agent-uninstallation)

### Manual Uninstallation

#### Windows EXE

On Windows, the LimaCharlie sensor can be uninstalled on individual endpoints by running the installer EXE with the `-c` argument, which will remove the Sensor and its service entirely.

Example:

```
C:\Windows\System32\rphcp.exe -c
del C:\Windows\System32\rphcp.exe
```

#### Windows MSI

If uninstalling via the Windows MSI installer, the `/x` switch can be used to uninstall.

Example:

```
msiexec /x lc_sensor.msi /qn
```

If you run into issues where the service is still present (`sc.exe query rphcpsvc`) or the exe is still left behind (`C:\Windows\System32\rphcp.exe`), you can remove them with the following:

```
C:\Windows\System32\rphcp.exe -c       # If this throws an error about the service, it's safe to ignore
del C:\Windows\System32\rphcp.exe
```

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# Windows Agent Installation

## Windows Agent Installation Instructions

### Windows MSI Installation Versioning

Our MSI packages are created programmatically, and thus will have a version of `1.0.0.0` upon compilation and download. Note Sensor versions will reflect the latest version as of the MSI download, and sensor version control is managed via the LimaCharlie application, not the MSI package.

### System Requirements

The LimaCharlie.io agent supports Windows XP 32 bit and up (32 and 64 bit). However, Windows XP and 2003 support is for the more limited capabilities of the agent that do not require kernel support.

### Installing via MSI

Windows Installer is an installation and configuration service provided with Windows. Microsoft Software Installer, or MSI, files allow for an easy and portable installation format on Windows systems.

LimaCharlie makes portable MSI files available with each new sensor release. They are available at a static URL for easy downloading:

* [32-bit MSI installer](https://downloads.limacharlie.io/sensor/windows/msi32)
* [64-bit MSI installer](https://downloads.limacharlie.io/sensor/windows/msi64)

Similar to executable installation, when installing with an MSI, you will need to provide the desired Installation Key, passed through as the `InstallationKey` variable.

**32-bit MSI installation command:**

```
hcp_win_x86_release_<sensor_version>.msi InstallationKey=<installation_key>
```

**64-bit MSI installation command:**

```
hcp_win_x64_release_<sensor_version>.msi InstallationKey=<installation_key>
```

Executing the installer via the command line, pass the `-i INSTALLATION_KEY` argument where `INSTALLATION_KEY` is the key mentioned above. This will install the sensor as a Windows service and trigger its enrollment.

You may also install the Windows sensor using the MSI version. With the MSI, install using:

```
installer.msi InstallationKey="INSTALLATION_KEY"
```

You may also pass the value `-` instead of the `INSTALLATION_KEY` like: `-i -`. This will make the installer look for the installation key in an alternate place in the following order:

* Environment variable `LC_INSTALLATION_KEY`
* Text file in current working directory: `lc_installation_key.txt`

#### Verify the service is running

In an administrative command prompt issue the command `sc query rphcpsvc` and confirm the `STATE` displayed is `RUNNING`.

## Uninstalling the Agent

For additional agent uninstall options, see Endpoint Agent Uninstallation.

### Manual Uninstallation

#### Windows EXE

On Windows, the LimaCharlie sensor can be uninstalled on individual endpoints by running the installer EXE with the `-c` argument, which will remove the Sensor and its service entirely.

Example:

```
C:\Windows\System32\rphcp.exe -c
del C:\Windows\System32\rphcp.exe
```

#### Windows MSI

If uninstalling via the Windows MSI installer, the `/x` switch can be used to uninstall.

Example:

```
msiexec /x lc_sensor.msi /qn
```

If you run into issues where the service is still present (`sc.exe query rphcpsvc`) or the exe is still left behind (`C:\Windows\System32\rphcp.exe`), you can remove them with the following:

```
C:\Windows\System32\rphcp.exe -c       # If this throws an error about the service, it's safe to ignore
del C:\Windows\System32\rphcp.exe
```

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS

LimaCharlie's Mac sensor interfaces with the kernel to acquire deep visibility into the host's activity while taking measures to preserve the host's performance. The Mac sensor currently supports all versions of MacOS 10.7 and up.

## Installation Instructions

Basic sensor installation instructions can be found [here](/v1/docs/sensor-installation).

Looking for alternative installation methods?

* macOS Sensor Installation - [Latest OS Versions](/v1/docs/macos-sensor-installation-latest-os-versions)
* macOS Sensor Installation - [Older OS Versions](/v1/docs/macos-sensor-installation-older-versions)
* macOS Sensor Installation - [MDM Configuration profiles](/v1/docs/macos-sensor-installation-mdm-configuration-profiles)

## Supported Events

* [`AUTORUN_CHANGE`](/v1/docs/reference-events#autorun_change)
* [`CLOUD_NOTIFICATION`](/v1/docs/reference-events#cloud_notification)
* [`CODE_IDENTITY`](/v1/docs/reference-events#code_identity)
* [`CONNECTED`](/v1/docs/reference-events#connected)
* [`DATA_DROPPED`](/v1/docs/reference-events#data_dropped)
* [`DNS_REQUEST`](/v1/docs/reference-events#dns_request)
* [`EXEC_OOB`](/v1/docs/reference-events#exec_oob)
* [`FILE_CREATE`](/v1/docs/reference-events#file_create)
* [`FILE_DELETE`](/v1/docs/reference-events#file_delete)
* [`FILE_MODIFIED`](/v1/docs/reference-events#file_modified)
* [`FILE_TYPE_ACCESSED`](/v1/docs/reference-events#file_type_accessed)
* [`FIM_HIT`](/v1/docs/reference-events#fim_hit)
* [`HIDDEN_MODULE_DETECTED`](/v1/docs/reference-events#hidden_module_detected)
* [`MODULE_LOAD`](/v1/docs/reference-events#module_load) -- *temporarily disabled*
* [`MODULE_MEM_DISK_MISMATCH`](/v1/docs/reference-events#module_mem_disk_mismatch)
* [`NETWORK_CONNECTIONS`](/v1/docs/reference-events#network_connections)
* [`NETWORK_SUMMARY`](/v1/docs/reference-events#network_summary)
* [`NEW_DOCUMENT`](/v1/docs/reference-events#new_document)
* [`NEW_PROCESS`](/v1/docs/reference-events#new_process)
* [`NEW_TCP4_CONNECTION`](/v1/docs/reference-events#new_tcp4_connection)
* [`NEW_UDP4_CONNECTION`](/v1/docs/reference-events#new_udp4_connection)
* [`NEW_TCP6_CONNECTION`](/v1/docs/reference-events#new_tcp6_connection)
* [`NEW_UDP6_CONNECTION`](/v1/docs/reference-events#new_udp6_connection)
* [`RECEIPT`](/v1/docs/reference-events#receipt)
* [`SERVICE_CHANGE`](/v1/docs/reference-events#service_change)
* [`SHUTTING_DOWN`](/v1/docs/reference-events#shutting_down)
* `SSH_LOGIN`
* `SSH_LOGOUT`
* [`STARTING_UP`](/v1/docs/reference-events#starting_up)
* [`TERMINATE_PROCESS`](/v1/docs/reference-events#terminate_process)
* [`TERMINATE_TCP4_CONNECTION`](/v1/docs/reference-events#terminate_tcp4_connection)
* [`TERMINATE_UDP4_CONNECTION`](/v1/docs/reference-events#terminate_udp4_connection)
* [`TERMINATE_TCP6_CONNECTION`](/v1/docs/reference-events#terminate_tcp6_connection)
* [`TERMINATE_UDP6_CONNECTION`](/v1/docs/reference-events#terminate_udp6_connection)
* `USER_LOGIN`
* `USER_LOGOUT`
* [`USER_OBSERVED`](/v1/docs/reference-events#user_observed)
* [`VOLUME_MOUNT`](/v1/docs/reference-events#volume_mount)
* [`VOLUME_UNMOUNT`](/v1/docs/reference-events#volume_unmount)
* [`YARA_DETECTION`](/v1/docs/reference-events#yara_detection)

## Supported Commands

* [`artifact_get`](/v1/docs/reference-sensor-commands#artifact_get)
* [`deny_tree`](/v1/docs/reference-sensor-commands#deny_tree)
* [`dir_find_hash`](/v1/docs/reference-sensor-commands#dir_find_hash)
* [`dir_list`](/v1/docs/reference-sensor-commands#dir_list)
* [`dns_resolve`](/v1/docs/reference-sensor-commands#dns_resolve)
* [`doc_cache_get`](/v1/docs/reference-sensor-commands#doc_cache_get)
* [`exfil_add`](/v1/docs/reference-sensor-commands#exfil_add)
* [`exfil_del`](/v1/docs/reference-sensor-commands#exfil_del)
* [`exfil_get`](/v1/docs/reference-sensor-commands#exfil_get)
* [`file_del`](/v1/docs/reference-sensor-commands#file_del)
* [`file_get`](/v1/docs/reference-sensor-commands#file_get)
* [`file_hash`](/v1/docs/reference-sensor-commands#file_hash)
* [`file_info`](/v1/docs/reference-sensor-commands#file_info)
* [`file_mov`](/v1/docs/reference-sensor-commands#file_mov)
* [`fim_add`](/v1/docs/reference-sensor-commands#fim_add)
* [`fim_del`](/v1/docs/reference-sensor-commands#fim_del)
* [`fim_get`](/v1/docs/reference-sensor-commands#fim_get)
* [`hidden_module_scan`](/v1/docs/reference-sensor-commands#hidden_module_scan)
* [`history_dump`](/v1/docs/reference-sensor-commands#history_dump)
* [`mem_find_handle`](/v1/docs/reference-sensor-commands#mem_find_handle)
* [`mem_find_string`](/v1/docs/reference-sensor-commands#mem_find_string)
* [`mem_handles`](/v1/docs/reference-sensor-commands#mem_handles)
* [`mem_map`](/v1/docs/reference-sensor-commands#mem_map)
* [`mem_read`](/v1/docs/reference-sensor-commands#mem_read)
* [`mem_strings`](/v1/docs/reference-sensor-commands#mem_strings)
* [`netstat`](/v1/docs/reference-sensor-commands#netstat)
* [`os_autoruns`](/v1/docs/reference-sensor-commands#os_autoruns)
* [`os_kill_process`](/v1/docs/reference-sensor-commands#os_kill_process)
* [`os_processes`](/v1/docs/reference-sensor-commands#os_processes)
* [`os_resume`](/v1/docs/reference-sensor-commands#os_resume)
* [`os_services`](/v1/docs/reference-sensor-commands#os_services)
* [`os_suspend`](/v1/docs/reference-sensor-commands#os_suspend)
* [`os_version`](/v1/docs/reference-sensor-commands#os_version)
* [`put`](/v1/docs/reference-sensor-commands#put)
* [`reg_list`](/v1/docs/reference-sensor-commands#reg_list)
* [`rejoin_network`](/v1/docs/reference-sensor-commands#rejoin_network)
* [`restart`](/v1/docs/reference-sensor-commands#restart)
* [`run`](/v1/docs/reference-sensor-commands#run)
* [`segregate_network`](/v1/docs/reference-sensor-commands#segregate_network)
* [`set_performance_mode`](/v1/docs/reference-sensor-commands#set_performance_mode)
* [`uninstall`](/v1/docs/reference-sensor-commands#uninstall)
* [`yara_scan`](/v1/docs/reference-sensor-commands#yara_scan)
* [`yara_update`](/v1/docs/reference-sensor-commands#yara_update)

## Artifacts

Given configured paths to collect from, the Mac sensor can batch upload logs / artifacts directly from the host.

Learn more about collecting Artifacts [here](/v1/docs/artifacts).

## Payloads

For more complex needs not supported by [Events](/v1/docs/detecting-related-events), [Artifacts](/v1/docs/artifacts), or [Commands](/v1/docs/sensor-commands), it's possible to execute payloads on hosts via the Mac sensor.

Learn more about executing Payloads [here](/v1/docs/payloads).

---

# macOS Agent Installation

LimaCharlie's Mac Sensor interfaces with the kernel to acquire deep visibility into the host's activity while taking measures to preserve the host's performance. The Mac sensor currently supports all versions of MacOS 10.7 and up.

## Installation Instructions

Executing the installer via the command line, pass the `-i INSTALLATION_KEY` argument where `INSTALLATION_KEY` is the key mentioned above.

* [Step-by-step instructions for macOS 10.15 (Catalina) and newer](/v2/docs/macos-agent-installation-latest-os-versions)
* [Step-by-step instructions for macOS 10.14 (Mojave) and older](/v2/docs/macos-agent-installation-older-versions)

You may also pass the value `-` instead of the `INSTALLATION_KEY` like: `-i -`. This will make the installer look for the Installation Key in an alternate place in the following order:

* Environment variable `LC_INSTALLATION_KEY`
* Text file in current working directory: `lc_installation_key.txt`

### Looking for alternative installation methods?

* macOS Agent Installation - [MDM Configuration profiles](/v2/docs/macos-agent-installation-mdm-configuration-profiles)

## Uninstalling the Agent

For additional agent uninstall options, see [Endpoint Agent Uninstallation](/v2/docs/endpoint-agent-uninstallation)

Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS Agent Installation - Latest Versions (macOS 15 Sequoia and newer)

## macOS Sensor (macOS 15 Sequoia)

This document provides details of how to install, verify, and uninstall the LimaCharlie Endpoint Agent on macOS (versions 15 Sonoma). We also offer separate documentation for older versions.

### Installer Options

When running the installer from the command line, you can pass the following arguments:

```
-v: display build version.
-q: quiet; do not display banner.
-d <INSTALLATION_KEY>: the installation key to use to enroll, no permanent installation.
-i <INSTALLATION_KEY>: install executable as a service with deployment key.
-r: uninstall executable as a service.
-c: uninstall executable as a service and delete identity files.
-w: executable is running as a macOS service.
-h: displays the list of accepted arguments.
```

### Installation Flow

1. Download the Sensor installer file. Installer for: [Intel Mac](https://downloads.limacharlie.io/sensor/mac/64) -or- [Apple Silicon Mac](https://downloads.limacharlie.io/sensor/mac/arm64).

2. Add execute permission to the installer file via the command line

```
chmod +x lc_sensor
```

3. Run the installer via the command line. You'll pass the argument -i and your Installation Key.

```
sudo ./lc_sensor -i YOUR_INSTALLATION_KEY_GOES_HERE
```

You can obtain the installation key from the Installation Keys section of the LimaCharlie web application.

The sensor will be installed as a launchctl service. Installation will trigger the sensors enrollment with the LimaCharlie cloud

![macOS Terminal application showing LimaCharlie installation](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/1-Terminal_install.png)

4. An application (`RPHCP.app`) will be installed in the /Applications folder and will automatically launch. Note that it may take a few minutes before you see this happened after installation.

You will be prompted to grant permissions for system extensions to be installed. Click the "**Open System Settings**" button

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/2-Endpoint_Extension_Installation_Dialog.png)

6. Ensure the toggle for "Allow in the Background" next to "Refraction Point, Inc." is toggled On.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/2.5-Login_Items_and_Extensions.png)

7. Click the "i" info icon next to "Endpoint Security Extensions", then ensure the toggle next to "RPHCP" is on.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/3-Endpoint_Extension_Enablement.png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/EndpointSecurityExtension-Enabled.png)

8. After enabling that toggle you'll need to click the "Allow" button to allow RPHCP to filter network content.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/4-Network_Filter_Enablement.png)

8. You'll be prompted to grant Full Disk Access. Check the checkbox next to the RPHCP app in System Preferences  Privacy  Full Disk Access

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/5-Full_Disk_Access_Permission_Dialog.png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/6-Full_Disk-Access_Enablement.png)

The installation is now complete and you should see a message indicating that the installation was successful.

![Success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/09-Success.png)

### Verifying Installation

To verify that the sensor was installed successfully, you can log into the LimaCharlie web application and see if the device has appeared in the Sensors section. Additionally, you can check the following on the device itself:

In a Terminal, run the command:

```
sudo launchctl list | grep com.refractionpoint.rphcp
```

![Successful installation verification](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Verification/Verification-installation-successful.png)

If the agent is running, this command should return records as shown above.

You can also check the /Applications folder and launch the RPHCP.app.

![Applications folder](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/10-Applications.png)

You can confirm the network filter was properly installed and enabled by going to System Settings  Network  VPN & Filters. You should expect to see "RPHCP" in the list with the status showing as Enabled.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/7-Network_Filter_Confirmation(1).png)

The application will show a message to indicate if the required permissions have been granted.

![App installed correctly](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/11-App_Installed_Correctly.png)

As described in the dialog, the RPHCP.app application must be left in the /Applications folder in order for it to continue operating properly.

#### A note on permissions

Apple has purposely made installing extensions (like the ones used by LimaCharlie) a process that requires several clicks on macOS. The net effect of this is that the first time the sensor is installed on a macOS system, permissions will need to be granted via System Preferences

Currently, the only way to automate the installation is to use an Apple-approved MDM solution. These solutions are often used by large organizations to manage their Mac fleet. If you are using such a solution, see your vendor's documentation on how to add extensions to the allow list which can be applied to your entire fleet.

We're aware this is an inconvenience and hope Apple will provide better solutions for security vendors in future.

### Uninstallation Flow

To uninstall the sensor:

1. Run the installer via the command line. You'll pass the argument -c

```
sudo ./hcp_osx_x64_release_4.23.0 -c
```

![Uninstall progress](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/1-Uninstall_Progress.png)

2. You will be prompted for credentials to modify system extensions. Enter your password and press OK.

![Uninstall permissions](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/2-Uninstaller_Permissions.png)

The related system extension will be removed and the `RPHCP.app` will be removed from the /Applications folder.

3. You should see a message indicating that the uninstallation was successful.

![Uninstall success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/3-Uninstall_Success.png)

> **Note:** After uninstallation the LimaCharlie sensor along with the related extensions will be removed. macOS requires a reboot to fully unload and remove extensions.

### Install Using MDM Solutions

See our document macOS Agent Installation with MDM Solutions for the Mobile Device Management (MDM) Configuration Profile that can be used to deploy the LimaCharlie agent to an enterprise fleet.

## Glossary

**Sensors:** Similar to agents, Sensors send telemetry to the LimaCharlie platform in the form of EDR telemetry or forwarded logs. Sensors are offered as a scalable, serverless solution for securely connecting endpoints of an organization to the cloud.

**Endpoint Agents:** Lightweight software agents deployed directly on endpoints like workstations and servers. These sensors collect real-time data related to system activity, network traffic, file changes, process behavior, and much more.

**Installation keys:** Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS Agent Installation - Latest Versions (macOS 15 Sequoia and newer)

## macOS Sensor (macOS 15 Sequoia)

This document provides details of how to install, verify, and uninstall the LimaCharlie Endpoint Agent on macOS (versions 15 Sonoma). We also offer separate documentation for older versions.

### Installer Options

When running the installer from the command line, you can pass the following arguments:

```
-v: display build version.
-q: quiet; do not display banner.
-d <INSTALLATION_KEY>: the installation key to use to enroll, no permanent installation.
-i <INSTALLATION_KEY>: install executable as a service with deployment key.
-r: uninstall executable as a service.
-c: uninstall executable as a service and delete identity files.
-w: executable is running as a macOS service.
-h: displays the list of accepted arguments.
```

### Installation Flow

1. Download the Sensor installer file.  Installer for: [Intel Mac](https://downloads.limacharlie.io/sensor/mac/64) -or- [Apple Silicon Mac](https://downloads.limacharlie.io/sensor/mac/arm64).

2. Add execute permission to the installer file via the command line

```
chmod +x lc_sensor
```

3. Run the installer via the command line.  You'll pass the argument -i and your Installation Key.

```
sudo ./lc_sensor -i YOUR_INSTALLATION_KEY_GOES_HERE
```

You can obtain the installation key from the Installation Keys section of the LimaCharlie web application.

The sensor will be installed as a launchctl service. Installation will trigger the sensors enrollment with the LimaCharlie cloud

![macOS Terminal application showing LimaCharlie installation](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/1-Terminal_install.png)

4. An application (`RPHCP.app`) will be installed in the /Applications folder and will automatically launch.  Note that it may take a few minutes before you see this happened after installation.

   You will be prompted to grant permissions for system extensions to be installed. Click the "**Open System Settings**" button

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/2-Endpoint_Extension_Installation_Dialog.png)

6. Ensure the toggle for "Allow in the Background" next to "Refraction Point, Inc." is toggled On.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/2.5-Login_Items_and_Extensions.png)

7. Click the "i" info icon next to "Endpoint Security Extensions", then ensure the toggle next to "RPHCP" is on.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/3-Endpoint_Extension_Enablement.png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/EndpointSecurityExtension-Enabled.png)

8. After enabling that toggle you'll need to click the "Allow" button to allow RPHCP to filter network content.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/4-Network_Filter_Enablement.png)

8. You'll be prompted to grant Full Disk Access.  Check the checkbox next to the RPHCP app in System Preferences -> Privacy -> Full Disk Access

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/5-Full_Disk_Access_Permission_Dialog.png)

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/6-Full_Disk-Access_Enablement.png)

The installation is now complete and you should see a message indicating that the installation was successful.

![Success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/09-Success.png)

### Verifying Installation

To verify that the sensor was installed successfully, you can log into the LimaCharlie web application and see if the device has appeared in the Sensors section. Additionally, you can check the following on the device itself:

In a Terminal, run the command:

```
sudo launchctl list | grep com.refractionpoint.rphcp
```

![Successful installation verification](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Verification/Verification-installation-successful.png)

If the agent is running, this command should return records as shown above.

You can also check the /Applications folder and launch the RPHCP.app.

![Applications folder](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/10-Applications.png)

You can confirm the network filter was properly installed and enabled by going to System Settings  Network  VPN & Filters. You should expect to see "RPHCP" in the list with the status showing as Enabled.

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/7-Network_Filter_Confirmation(1).png)

The application will show a message to indicate if the required permissions have been granted.

![App installed correctly](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/11-App_Installed_Correctly.png)

As described in the dialog, the RPHCP.app application must be left in the /Applications folder in order for it to continue operating properly.

#### A note on permissions

Apple has purposely made installing extensions (like the ones used by LimaCharlie) a process that requires several clicks on macOS. The net effect of this is that the first time the sensor is installed on a macOS system, permissions will need to be granted via System Preferences

Currently, the only way to automate the installation is to use an Apple-approved MDM solution. These solutions are often used by large organizations to manage their Mac fleet. If you are using such a solution, see your vendor's documentation on how to add extensions to the allow list which can be applied to your entire fleet.

We're aware this is an inconvenience and hope Apple will provide better solutions for security vendors in future.

### Uninstallation Flow

To uninstall the sensor:

1. Run the installer via the command line.  You'll pass the argument -c

```
sudo ./hcp_osx_x64_release_4.23.0 -c
```

![Uninstall progress](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/1-Uninstall_Progress.png)

2. You will be prompted for credentials to modify system extensions.  Enteryour password and press OK.

![Uninstall permissions](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/2-Uninstaller_Permissions.png)

The related system extension will be removed and the `RPHCP.app` will be removed from the /Applications folder.

3. You should see a message indicating that the uninstallation was successful.

![Uninstall success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/3-Uninstall_Success.png)

> **Note:** After uninstallation the LimaCharlie sensor along with the related extensions will be removed. macOS requires a reboot to fully unload and remove extensions.

### Install Using MDM Solutions

See our document macOS Agent Installation with MDM Solutions for the Mobile Device Management (MDM) Configuration Profile that can be used to deploy the LimaCharlie agent to an enterprise fleet.

---

# macOS Agent Installation - MDM Configuration Profiles

This document provides details of the Mobile Device Management (MDM) Configuration Profile that can be used to deploy the LimaCharlie agent to your enterprise fleet on macOS (versions 10.15 and newer).

## Affected Dialogs

Once the configuration profile is deployed using an approved MDM server, users will not need to provide approval to complete the agent installation. In particular, the following three system approval dialogs will no longer be presented:

### System Extension
![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/04-System_Extension_Required.png)

### Network Filter
![Network filter](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/07--Network_Filter.png)

### Full Disk Access
![Full disk access](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/08-Full_Disk_Access.png)

### Application Installation
![RPHCP application install](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/03-Permissions_Required.png)

## Configuration Profile Details

We have provided a sample configuration profile for reference:

[Download LimaCharlie.mobileconfig sample configuration profile](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/LimaCharlie.mobileconfig.zip)

This profile includes the following permissions:

* System Extension
* Full Disk Access
* Network Content Filter

## Silent Installation Preference

In addition to the MDM profile, you will also want to place the following preference file in the /Library/Preferences folder on the endpoint prior to installation. With this preference file in place the application will provide for a silent installation.

[Download com.refractionpoint.rphcp.client.plist preference file (to be placed in the /Library/Preferences folder on the endpoint)](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/com.refractionpoint.rphcp.client.plist.zip)

## Installation Scripts

We have made a sample installation and uninstallation script available. You can use these with MDM providers to mass install/remove LimaCharlie. Note that the installation script should be edited prior to use as it requires your unique Installation Key to be entered.

These scripts will determine the machine architecture (Intel or Apple Silicon), download the appropriate installer, and then perform the installation or uninstallation. They also will automatically add (or remove, for uninstallations) the Silent Installation Preference File.

[Sample Installation Script](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/sample-install-limacharlie.sh)

[Sample Uninstallation Script](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/sample-uninstall-limacharlie.sh)

## Example Jamf Pro Setup

While any Apple / user approved MDM provider may be used, we have provided specific instructions for Jamf Pro as a matter of convenience.

1. Log into Jamf Pro and go to Computers -> Configuration Profiles
2. Add a new profile
3. In the General section choose a name for the profile and set Level to "Computer Level"

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-1-General.png)

4. Add a Privacy Preferences Policy Control configuration and set the parameters as follows:

**Identifier:**
com.refractionpoint.rphcp.extension

**Identifier Type:**
Bundle ID

**Code Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.extension" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

**App or Service:**
SystemPolicyAllFiles

**Access:**
Allow

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-2-PPPC.png)

5. Add a System Extensions configuration and set the parameters as follows:

Enter your desired display name

**System Extension Types:** Allowed System Extensions

**Team Identifier:** N7N82884NH

**Allowed System Extensions:** com.refractionpoint.rphcp.extension

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-2-SystemExtensions.png)

6. Add a Content Filter configuration and set the parameters as follows:

Enter your desired filter name

**Identifier:** com.refractionpoint.rphcp.client

**Filter Order:** Firewall

**Add a Socket Filter with the following details:**

**Socket Filter Bundle Identifier:**
com.refractionpoint.rphcp.client

**Socket Filter Designated Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.client" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

**Add a Network Filter with the following details:**

**Network Filter Bundle Identifier:**
com.refractionpoint.rphcp.client

**Network Filter Designated Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.client" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-4-ContentFilter.png)

7. Deploy the configuration profile to your devices.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS Agent Installation - MDM Configuration Profiles

This document provides details of the Mobile Device Management (MDM) Configuration Profile that can be used to deploy the LimaCharlie agent to your enterprise fleet on macOS (versions 10.15 and newer).

## Affected Dialogs

Once the configuration profile is deployed using an approved MDM server, users will not need to provide approval to complete the agent installation. In particular, the following three system approval dialogs will no longer be presented:

System Extension
![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/04-System_Extension_Required.png)

Network Filter
![Network filter](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/07--Network_Filter.png)

Full Disk Access
![Full disk access](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/08-Full_Disk_Access.png)

Application Installation
![RPHCP application install](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/03-Permissions_Required.png)

## Configuration Profile Details

We have provided a sample configuration profile for reference:

[Download LimaCharlie.mobileconfig sample configuration profile](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/LimaCharlie.mobileconfig.zip)

This profile includes the following permissions:

* System Extension
* Full Disk Access
* Network Content Filter

## Silent Installation Preference

In addition to the MDM profile, you will also want to place the following preference file in the /Library/Preferences folder on the endpoint prior to installation. With this preference file in place the application will provide for a silent installation.

The required preference file can be downloaded here:

[Download com.refractionpoint.rphcp.client.plist preference file (to be placed in the /Library/Preferences folder on the endpoint)](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/com.refractionpoint.rphcp.client.plist.zip)

## Installation Scripts

We have made a sample installation and uninstallation script available. You can use these with MDM providers to mass install/remove LimaCharlie. Note that the installation script should be edited prior to use as it requires your unique Installation Key to be entered.

These scripts will determine the machine architecture (Intel or Apple Silicon), download the appropriate installer, and then perform the installation or uninstallation. They also will automatically add (or remove, for uninstallations) the Silent Installation Preference File.

[Sample Installation Script](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/sample-install-limacharlie.sh)

[Sample Uninstallation Script](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/sample-uninstall-limacharlie.sh)

## Example Jamf Pro Setup

While any Apple / user approved MDM provider may be used, we have provided specific instructions for Jamf Pro as a matter of convenience.

1. Log into Jamf Pro and go to Computers -> Configuration Profiles
2. Add a new profile
3. In the General section choose a name for the profile and set Level to "Computer Level"

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-1-General.png)

4. Add a Privacy Preferences Policy Control configuration and set the parameters as follows:

**Identifier:**
com.refractionpoint.rphcp.extension

**Identifier Type:**
Bundle ID

**Code Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.extension" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

**App or Service:**
SystemPolicyAllFiles

**Access:**
Allow

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-2-PPPC.png)

5. Add a System Extensions configuration and set the parameters as follows:

Enter your desired display name

**System Extension Types:** Allowed System Extensions

**Team Identifier:** N7N82884NH

**Allowed System Extensions:** com.refractionpoint.rphcp.extension

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-2-SystemExtensions.png)

6. Add a Content Filter configuration and set the parameters as follows:

Enter your desired filter name

**Identifier:** com.refractionpoint.rphcp.client

**Filter Order:** Firewall

**Add a Socket Filter with the following details:**

**Socket Filter Bundle Identifier:**
com.refractionpoint.rphcp.client

**Socket Filter Designated Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.client" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

**Add a Network Filter with the following details:**

**Network Filter Bundle Identifier:**
com.refractionpoint.rphcp.client

**Network Filter Designated Requirement:**
anchor apple generic and identifier "com.refractionpoint.rphcp.client" and (certificate leaf[field.1.2.840.113635.100.6.1.9] /\* exists \*/ or certificate 1[field.1.2.840.113635.100.6.2.6] /\* exists \*/ and certificate leaf[field.1.2.840.113635.100.6.1.13] /\* exists \*/ and certificate leaf[subject.OU] = N7N82884NH)

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/JamfPro-4-ContentFilter.png)

7. Deploy the configuration profile to your devices.

Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS Agent Installation - Older Versions (macOS 10.14 and prior)

This document provides details of how to install, verify, and uninstall the LimaCharlie sensor on macOS (versions 10.14 and prior). We also offer documentation for macOS 10.15 and newer.

## macOS Sensor (macOS 10.14 and prior)

### Installer Options

When running the installer from the command line, you can pass the following arguments:

```
-v: display build version.
-q: quiet; do not display banner.
-d <INSTALLATION_KEY>: the installation key to use to enroll, no permanent installation.
-i <INSTALLATION_KEY>: install executable as a service with deployment key.
-r: uninstall executable as a service.
-c: uninstall executable as a service and delete identity files.
-w: executable is running as a macOS service.
-h: displays the list of accepted arguments.
```

### Installation Flow

1. Download the [Sensor installer file](https://downloads.limacharlie.io/sensor/mac/64)

2. Add execute permission to the installer file via the command line

```bash
chmod +x hcp_osx_x64_release_4.23.0
```

3. Run the installer via the command line. You'll pass the argument -i and your Installation Key.

```bash
sudo ./hcp_osx_x64_release_4.23.0 -i YOUR_INSTALLATION_KEY_GOES_HERE
```

You can obtain the installation key from the Installation Keys section of the LimaCharlie web application.

The sensor will be installed as a launchctl service. Installation will trigger the sensors enrollment with the LimaCharlie cloud.

4. You will be prompted to grant permissions for system extensions to be installed.

5. Click the "Open System Preferences" button

6. Unlock the preference pane using the padlock in the bottom left corner, then click the Allow button next to `System software from developer "Refraction Point, Inc" was blocked from loading.`

The installation is now complete and you should see a message indicating that the installation was successful.

## Verifying Installation

To verify that the sensor was installed successfully, you can log into the LimaCharlie web application and see if the device has appeared in the Sensors section. Additionally, you can check the following on the device itself:

### Ensure the process is running

In a Terminal, run the command:

```bash
sudo launchctl list | grep com.refractionpoint.rphcp
```

If the agent is running, this command should return a record.

### Ensure the Kernel Extension is loaded

You can confirm that the kernel extension is loaded by running the command:

```bash
kextstat | grep com.refractionpoint.
```

If the extension is loaded, this command should return a record.

### A note on permissions

Apple has purposely made installing extensions (like the ones used by LimaCharlie) a process that requires several clicks on macOS. The net effect of this is that the first time the sensor is installed on a macOS system, permissions will need to be granted via System Preferences.

Currently, the only way to automate the installation is to use an Apple-approved MDM solution. These solutions are often used by large organizations to manage their Mac fleet. If you are using such a solution, see your vendor's documentation on how to add extensions to the allow list which can be applied to your entire fleet.

We're aware this is an inconvenience and hope Apple will provide better solutions for security vendors in future.

## Uninstallation Flow

To uninstall the sensor:

1. Run the installer via the command line. You'll pass the argument -c

```bash
sudo ./hcp_osx_x64_release_4.23.0 -c
```

2. You should see a message indicating that the uninstallation was successful.

---

**Note:** Installation keys are Base64-encoded strings provided to Sensors and Adapters in order to associate them with the correct Organization. Installation keys are created per-organization and offer a way to label and control your deployment population.

---

# macOS Agent Installation - Older Versions (macOS 10.15 Catalina to macOS 14 Sonoma)

## macOS Sensor (macOS 10.15 Catalina to macOS 14 Sonoma)

This document provides details of how to install, verify, and uninstall the LimaCharlie Endpoint Agent on macOS (versions 10.15 Catalina though to macOS 14 Sonoma). We also offer documentation for [macOS 10.14 and prior](/v2/docs/macos-agent-installation-older-versions), and [macOS 10.15 and newer](/v2/docs/clone-macos-agent-installation-latest-versions-macos-15-sequoia-and-newer).

### Installer Options

When running the installer from the command line, you can pass the following arguments:

```
-v: display build version.
-q: quiet; do not display banner.
-d <INSTALLATION_KEY>: the installation key to use to enroll, no permanent installation.
-i <INSTALLATION_KEY>: install executable as a service with deployment key.
-r: uninstall executable as a service.
-c: uninstall executable as a service and delete identity files.
-w: executable is running as a macOS service.
-h: displays the list of accepted arguments.
```

### Installation Flow

1. Download the Sensor installer file. Installer for: [Intel Mac](https://downloads.limacharlie.io/sensor/mac/64) -or- [Apple Silicon Mac](https://downloads.limacharlie.io/sensor/mac/arm64).

2. Add execute permission to the installer file via the command line

```bash
chmod +x lc_sensor
```

3. Run the installer via the command line. You'll pass the argument -i and your Installation Key.

```bash
sudo ./lc_sensor -i YOUR_INSTALLATION_KEY_GOES_HERE
```

![Basic installation](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/01-Basic_installation.png)

You can obtain the installation key from the [Installation Keys](/v2/docs/installation-keys) section of the LimaCharlie web application.

The sensor will be installed as a launchctl service. Installation will trigger the sensors enrollment with the LimaCharlie cloud.

![Installation success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/02-Installation_success.png)

4. An application (`RPHCP.app`) will be installed in the /Applications folder and will automatically launch. You will be prompted to grant permissions for system extensions to be installed.

![Permissions required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/03-Permissions_Required.png)

5. Click the "Open System Preferences" button

![System Extensions Required](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/04-System_Extension_Required.png)

6. Unlock the preference pane using the padlock in the bottom left corner, then click the Allow button next to `System software from application "RPHCP" was blocked from loading.`

![Unlocked](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/06-Allow_System_Software_Unlocked.png)

7. You'll be prompted to allow the application to Filter Network Content. Click the Allow button.

![Network filter](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/07--Network_Filter.png)

8. You'll be prompted to grant Full Disk Access. Check the checkbox next to the RPHCP app in System Preferences -> Privacy -> Full Disk Access

![Full disk access](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/08-Full_Disk_Access.png)

The installation is now complete and you should see a message indicating that the installation was successful.

![Success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/09-Success.png)

### Verifying Installation

To verify that the sensor was installed successfully, you can log into the LimaCharlie web application and see if the device has appeared in the Sensors section. Additionally, you can check the following on the device itself:

In a Terminal, run the command:

```bash
sudo launchctl list | grep com.refractionpoint.rphcp
```

![Successful installation verification](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Verification/Verification-installation-successful.png)

If the agent is running, this command should return records as shown above.

You can also check the /Applications folder and launch the RPHCP.app.

![Applications folder](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/10-Applications.png)

The application will show a message to indicate if the required permissions have been granted.

![App installed correctly](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Installation/11-App_Installed_Correctly.png)

As described in the dialog, the RPHCP.app application must be left in the /Applications folder in order for it to continue operating properly.

#### A note on permissions

Apple has purposely made installing extensions (like the ones used by LimaCharlie) a process that requires several clicks on macOS. The net effect of this is that the first time the sensor is installed on a macOS system, permissions will need to be granted via System Preferences

Currently, the only way to automate the installation is to use an Apple-approved MDM solution. These solutions are often used by large organizations to manage their Mac fleet. If you are using such a solution, see your vendor's documentation on how to add extensions to the allow list which can be applied to your entire fleet.

We're aware this is an inconvenience and hope Apple will provide better solutions for security vendors in future.

### Uninstallation Flow

To uninstall the sensor:

1. Run the installer via the command line. You'll pass the argument -c

```bash
sudo ./hcp_osx_x64_release_4.23.0 -c
```

![Uninstall progress](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/1-Uninstall_Progress.png)

2. You will be prompted for credentials to modify system extensions. Enter your password and press OK.

![Uninstall permissions](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/2-Uninstaller_Permissions.png)

The related system extension will be removed and the `RPHCP.app` will be removed from the /Applications folder.

3. You should see a message indicating that the uninstallation was successful.

![Uninstall success](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/images/Uninstallation/3-Uninstall_Success.png)

### Install Using MDM Solutions

See our document [macOS Agent Installation with MDM Solutions](/docs/macos-agent-installation-mdm-configuration-profiles) for the Mobile Device Management (MDM) Configuration Profile that can be used to deploy the LimaCharlie agent to an enterprise fleet.

---

# macOS Agent Installation via Jamf Now

[Jamf Now](https://www.jamf.com/products/jamf-now/) is an MDM solution that provides an easy way to manage Apple devices for small and medium-sized businesses. LimaCharlie sensors can be deployed via Jamf Now for easy app distribution and inventory capabilities.

## Prerequisites

* a Jamf Now account;
* a provisioning profile that grants the necessary pre-authorizations (such as is [available here](/v2/docs/macos-agent-installation-latest-os-versions)) for deployment on the clients;
* a LimaCharlie Mac Sensor installer package (`.pkg`) that's configured as desired for deployment on the clients.

## Set up your account on Jamf Now

1. Create a Jamf Now account at [https://signup.jamfnow.com](https://signup.jamfnow.com/), and log in.
2. Choose the "APNs" tab in the sidebar, and click "Get Started".
3. Click "Download Certificate Signing Request.plist" and save the plist.
4. Click Next in the lower right.
5. As per the "Create an Apple Push Certificate" checklist shown, click "Open the Apple Push Certificates Portal".
6. Log in with your Apple ID.
7. On the "Apple Push Certificates Portal" page to which you are redirected, click the green "Create Certificate" button.
8. Accept the Terms of Use, and click Continue.
9. On the "Create a New Push Certificate" page to which you're redirected, specify the plist you downloaded in step 2 and click Upload.
10. On the "Confirmation" page, click Download and save the new PEM certificate file.
11. Navigate back to the Jamf Now page as at step 5, and click Next in the lower right.
12. On the "Upload Push Certificate" page, specify the PEM you downloaded in step 10.
13. Under "Save Your Apple ID", annotate same as Jamf invites to do so, and click Save.

## Prepare the LimaCharlie sensor installer package on Jamf

As a prerequisite you must have on hand a LimaCharlie Sensor installer package (.pkg) that's configured as desired.

1. Choose the "Apps" tab in the Jamf Now sidebar. It will show "No apps yet, let's fix that."
2. Click "Add an App".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28117%29.png)

3. On the "Add an App" page, click "Upload Your App" in the top menu.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28118%29.png)

4. Drag your LC Sensor package installer onto the page (or click "browse" to locate it) to upload it to Jamf.
5. Give the package an appropriate name, and click Done.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28119%29.png)

## Prepare the LimaCharlie sensor provisioning on Jamf

1. Choose the "Blueprints" tab in the Jamf Now sidebar.
2. Click "Create New Blueprint" at the top.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28120%29.png)

3. Enter a meaningful Name and Description as prompted, and click Save Blueprint.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28121%29.png)

4. Click on the entry for your new Blueprint.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28122%29.png)

5. On the inner tab bar that appears, click "Custom Profiles", and then "Add a Custom Profile".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28123%29.png)

6. Drag your LimaCharlie mobileconfig file onto the page (or click "browse" to locate it) to upload it to Jamf.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28124%29.png)

7. Click "Add Custom Profile" in the lower right.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28125%29.png)

8. On the inner tab bar, click "Apps", and then click "Add App".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28126%29.png)

9. In the list, enable the "Install Automatically" checkbox for with the installer package that you uploaded earlier.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28127%29.png)

10. Click "Save Changes" in the lower right.

## Prepare Jamf Now to enroll devices

1. Choose the "Devices" tab in the Jamf Now sidebar. It will show "No devices yet, let's fix that."
2. Click "Enable Open Enrollment".
3. On the "Open Enrollment" page, activate the "Enable Open Enrollment" checkbox, enter an Access Code as prompted, and click Save Settings.
4. Take note of the indicated enrollment link.

## Enroll a Mac for management in Jamf

The following recipe presumes the use of MacOS 13 (Ventura).

1. On a subject Mac, visit the enrollment link from step 4 in the section above.
2. Enter the appropriate Access Code and user name, and click Start Enrollment.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28128%29.png)

3. Save the "enroll.mobileconfig" file that begins to download, and then open it in the Finder by double-clicking.
4. Open the System Settings app and navigate to the newly-installed profile.

   1. Choose "Privacy & Security".
   2. Scroll to the bottom, and under the "Others" heading, click "Profiles".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28129%29.png)

5. Double-click on the " Profile".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28130%29.png)

6. Click "Install" in the lower left.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28131%29.png)

7. Authenticate with the appropriate password when prompted with "Profiles is trying to enroll you in a remote management (MDM) service".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28132%29.png)

8. Observe that System Settings declares "This Mac is supervised and managed by ".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28133%29.png)

## Provision a Mac with the LimaCharlie sensor

1. Choose the "Blueprints" tab in the Jamf Now sidebar.
2. Click the entry for the custom Blueprint you created from Step 6 onward in the "Prepare the LC sensor package on Jamf" section above.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28134%29.png)

3. On the inner tab bar that appears, click "Devices", and then "Add a Device".

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28135%29.png)

4. Click on a device you want to provision, and then click "Add Devices" in the lower right corner.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28136%29.png)

5. Observe after a few moments that both the provisioning profile and the LimaCharlie sensor have been installed on the subject Mac.

   1. The Mac appear in the Jamf Devices list on the Blueprints tab with the label "Settings applied". (It may initially appear as "Settings not applied"; simply refresh the page.)

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28137%29.png)

2. On the Mac itself, an additional profile appears in System Settings > Privacy & Security > Profiles.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28138%29.png)

3. A "Background Items Added" notification is displayed.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28139%29.png)

4. The RPHCP.app appears in the Mac's Applications folder, and the rphcp daemon is running.

---

# macOS Agent Installation via Microsoft Intune

You can deploy the LimaCharlie Sensor for macOS using the MDM provider of your choice. Below are instructions for deploying the LimaCharlie Sensor for macOS using Microsoft Intune.

## MDM Profile

Set up the installation script by following these steps:

1. In the [Microsoft Intune admin center](https://intune.microsoft.com/), go to Devices  Manage Devices  Configuration.

![Screenshot of MS Intune -> Devices | Configuration](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Configurations.png)

2. Choose [Policies](https://intune.microsoft.com/?ref=AdminCenter#view/Microsoft_Intune_DeviceSettings/DevicesMenu/~/configuration), click the Create button and choose New Policy

   1. Set the Platform to be macOS

   2. Set the Profile Type to be Templates, then choose the template name "Custom"

   3. Click Create

3. Enter the custom policy details as follows:

   1. Name:  LimaCharlie

   2. Custom configuration profile name:  LimaCharlie

   3. Deployment channel: Device channel

   4. Configuration profile file: Download and use the [LimaCharlie MDM profile](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/LimaCharlie.mobileconfig.zip).

Set the Assignments to include all users who need the profile installed.

![Screenshot of MS Intune -> Devices | Configuration | Details](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Configuration-details.png)

## Installation Script

Set up the installation script by following these steps:

1. In the [Microsoft Intune admin center](https://intune.microsoft.com/), go to Devices  Manage Devices  Scripts and remediations.

![Screenshot of MS Intune -> Devices | Scripts](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Scripts.png)

2. Choose [Platform scripts](https://intune.microsoft.com/?ref=AdminCenter#view/Microsoft_Intune_DeviceSettings/DevicesMenu/~/scripts), click the Add button and choose macOS

3. Set up the script with the following parameters:

Name: Install LimaCharlie

Shell script:  [Download this template shell script](https://storage.googleapis.com/limacharlie-io/doc/sensor-installation/macOS/MDM_profiles/sample-install-limacharlie.sh); be sure to edit it to include your Installation Key before uploading it in MS Intune.

Run script as signed-in user:  No

Hide script notifications on devices:  Yes

Script frequency:  Not configured

Max number of times to retry if script fails:  3

Assignments:  Set the `Included groups` to be `All Users` if you wish all users to get the application to be installed, or simply select the correct group to whom you wish to have LimaCharlie be installed for.

![Screenshot of MS Intune -> Devices | Scripts | Details](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/Script-details.png)

---

# Tutorials & Guides

# Tutorial

Articles tagged with **tutorial** (16 total):

1. **Ingesting Linux Audit Logs** - 10 Oct 2025
2. **Ingesting Sysmon Event Logs** - 09 Oct 2025
3. **Create a D&R Rule Using a Threat Feed** - 08 Oct 2025
4. **Velociraptor to BigQuery** - 08 Oct 2025
5. **Ingesting MacOS Unified Logs** - 07 Oct 2025
6. **Ingesting Windows Event Logs** - 07 Oct 2025
7. **Building Reports with BigQuery + Looker Studio** - 07 Oct 2025
8. **VirusTotal Integration** - 07 Oct 2025
9. **Updating Sensors to the Newest Version** - 25 Apr 2025
10. **Test a New Sensor Version** - 25 Apr 2025
11. **Tutorial: Ingesting Telemetry from Cloud-Based External Sources** - 25 Apr 2025
12. **Tutorial: Ingesting Google Cloud Logs** - 25 Apr 2025
13. **Writing and Testing Rules** - 02 Apr 2025
14. **Detection and Response Examples** - 03 Jan 2025
15. **Tutorial: Creating a Webhook Adapter** - 02 Jan 2025
16. **Hayabusa to BigQuery** - 15 Oct 2024

---

# Tutorial

Articles

(16)

* [Ingesting Linux Audit Logs](/docs/ingesting-linux-audit-logs)
* [Ingesting Sysmon Event Logs](/docs/ingesting-sysmon-event-logs)
* [Create a D&R Rule Using a Threat Feed](/docs/create-a-dr-rule-using-a-threat-feed)
* [Velociraptor to BigQuery](/docs/velociraptor-to-bigquery)
* [Ingesting MacOS Unified Logs](/docs/ingesting-macos-unified-logs)
* [Ingesting Windows Event Logs](/docs/ingesting-windows-event-logs)
* [Building Reports with BigQuery + Looker Studio](/docs/tutorials-reporting-building-reports-with-bigquery-looker-studio)
* [VirusTotal Integration](/docs/tutorials-integratons-virustotal-integration)
* [Updating Sensors to the Newest Version](/docs/updating-sensors-to-the-newest-version)
* [Test a New Sensor Version](/docs/test-a-new-sensor-version)
* [Tutorial: Ingesting Telemetry from Cloud-Based External Sources](/docs/tutorial-ingesting-telemetry-from-cloud-based-external-sources)
* [Tutorial: Ingesting Google Cloud Logs](/docs/tutorial-ingesting-google-cloud-logs)
* [Writing and Testing Rules](/docs/writing-and-testing-rules)
* [Detection and Response Examples](/docs/detection-and-response-examples)
* [Tutorial: Creating a Webhook Adapter](/docs/tutorial-creating-a-webhook-adapter)
* [Hayabusa to BigQuery](/docs/hayabusa-to-bigquery)

---

# Log Collection Guide

This guide covers how to collect various Linux system logs into LimaCharlie using USP adapters. LimaCharlie provides flexible log collection capabilities through file monitoring and syslog ingestion.

## Collection Methods

### File Adapter (Recommended for Log Files)

The file adapter monitors log files for changes and streams new entries to LimaCharlie. It supports glob patterns for monitoring multiple files and handles log rotation automatically.

#### Key Features:

* Glob pattern support (/var/log/\*.log)
* Automatic log rotation handling
* Backfill support for historical data
* Multi-line JSON parsing
* Grok pattern parsing for structured log extraction

#### Basic Configuration:

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"  # or "json" for structured logs
    sensor_seed_key: "linux-logs"
  file_path: "/path/to/logfile"
  backfill: false  # Set true to read existing content
  no_follow: false # Set true to stop after reading existing content
```

### Syslog Adapter

runs as a syslog server, accepting logs via TCP or UDP. This is useful for centralizing logs from multiple systems or integrating with existing syslog infrastructure.

#### Key Features:

* TCP and UDP support
* TLS encryption support
* Mutual TLS authentication
* RFC 3164/5424 syslog format support

#### Basic Configuration:

```
syslog:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "syslog-server"
  port: 514
  interface: "0.0.0.0"
  is_udp: false  # Use TCP by default
```

### Log Parsing Options

LimaCharlie supports two methods for parsing unstructured log data:

* **parsing\_grok**: Uses Grok patterns (recommended) - pre-built patterns for common log formats, easier to read and maintain
* **parsing\_re**: Uses regular expressions - for custom formats or when Grok patterns don't meet specific needs

Grok patterns are built on regular expressions but provide named patterns for common elements like timestamps, IP addresses, and log formats. Use Grok when possible for better maintainability.

## Common Log Sources

### System Logs (/var/log/messages, /var/log/syslog)

Traditional system logs contain kernel messages, service logs, and general system events.

**File Adapter Approach:**

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "system-logs"
    mapping:
      parsing_grok:
        message: "%{SYSLOGTIMESTAMP:date} %{HOSTNAME:host} %{DATA:service}(?:\\[%{POSINT:pid}\\])?: %{GREEDYDATA:message}"
      sensor_hostname_path: "host"
      event_type_path: "service"
  file_path: "/var/log/messages"  # or /var/log/syslog
```

### Kernel Logs (/var/log/kern.log)

Kernel-specific messages including hardware events, driver messages, and security events.

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "kernel-logs"
    mapping:
      parsing_grok:
        message: "%{SYSLOGTIMESTAMP:date} %{HOSTNAME:host} kernel: %{GREEDYDATA:message}"
      sensor_hostname_path: "host"
      event_type_path: "kernel"
  file_path: "/var/log/kern.log"
```

### Apache Logs (/var/log/httpd/*, /var/log/apache2/*)

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "apache-logs"
    mapping:
      parsing_grok:
        message: "%{COMMONAPACHELOG}"
      event_type_path: "verb"
  file_path: "/var/log/apache2/access.log"  # or /var/log/httpd/access_log
```

### Nginx Logs (/var/log/nginx/\*)

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "nginx-logs"
    mapping:
      parsing_grok:
        message: "%{NGINXACCESS}"
      event_type_path: "verb"
  file_path: "/var/log/nginx/access.log"
```

### Audit Logs (/var/log/audit/audit.log)

Linux audit logs are critical for CIS Controls compliance and security monitoring.

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "audit-logs"
    mapping:
      parsing_grok:
        message: "type=%{DATA:audit_type} msg=audit\\(%{NUMBER:timestamp}:%{NUMBER:serial}\\): %{GREEDYDATA:audit_data}"
      event_type_path: "audit_type"
      event_time_path: "timestamp"
  file_path: "/var/log/audit/audit.log"
```

## Journalctl

Modern logging solution that can output in JSON format for structured parsing.

**Method 1: Pipe to Syslog Adapter**

```
# Stream journalctl to syslog adapter
journalctl -f -q --output=json | nc localhost 514
```

**Method 2: Output to File and Monitor**

```
# Create a systemd service to write journal to file
sudo tee /etc/systemd/system/journal-export.service << EOF
[Unit]
Description=Export systemd journal to file
After=systemd-journald.service

[Service]
ExecStart=/usr/bin/journalctl -f -q --output=json
StandardOutput=append:/var/log/journal-export.json
Restart=always

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable journal-export.service
sudo systemctl start journal-export.service
```

Then monitor the file:

```
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "json"  # JSON format for structured data
    sensor_seed_key: "journalctl-logs"
    mapping:
      sensor_hostname_path: "_HOSTNAME"
      event_type_path: "_SYSTEMD_UNIT"
      event_time_path: "__REALTIME_TIMESTAMP"
  file_path: "/var/log/journal-export.json"
```

## Multi-File Collection

For collecting multiple log types simultaneously:

```
# /var/log/messages
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "system-logs"
  file_path: "/var/log/messages"

---

# Kernel logs
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "kernel-logs"
  file_path: "/var/log/kern.log"

---

# Audit logs
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "audit-logs"
  file_path: "/var/log/audit/audit.log"

---

# Web server logs (glob pattern for multiple files)
file:
  client_options:
    identity:
      oid: "your-organization-id"
      installation_key: "your-installation-key"
    platform: "text"
    sensor_seed_key: "web-logs"
  file_path: "/var/log/nginx/*.log"
```

## Best Practices

* **Use JSON format when possible** - Modern logs often support JSON output, which provides better structure and parsing.
* **Configure appropriate Grok patterns** - Grok provides pre-built patterns for common log formats and is easier to maintain than regex. Use `parsing_grok` over `parsing_re` when possible.
* **Set sensor\_seed\_key appropriately** - Use descriptive names that identify the log source for easier management.
* **Monitor file permissions** - Ensure the adapter has read access to log files.
* **Use backfill carefully** - Only enable for initial historical data collection to avoid duplicates.
* **Implement proper field mapping** - Extract hostname, timestamps, and event types for better searchability.
* **Pattern testing** - Test Grok patterns against sample log lines before deployment. Common patterns include %{COMMONAPACHELOG}, %{SYSLOGTIMESTAMP}, and %{NGINXACCESS}.

## Troubleshooting

Common issues:

* **File permission errors**: Check that the adapter process has read access to log files
* **Parse failures**: Validate Grok patterns against actual log formats
* **Missing logs**: Verify file paths and glob patterns
* **Connection issues**: Check network connectivity and authentication credentials

Adapters serve as flexible data ingestion mechanisms for both on-premise and cloud environments.

---

# Tutorial: Ingesting Google Cloud Logs

With LimaCharlie, you can easily ingest Google Cloud logs for further processing and automation. This article covers the following high-level steps of shipping logs from GCP into LimaCharlie:

1. Create a Log Sink to Pubsub in GCP
2. Create a Subscription for the Topic
3. Create a Service Account with the required permissions.
4. [Optional] Create a GCE instance to run the Adapter.
5. Create an Installation Key in LimaCharlie
6. Run the LC Adapter to ingest the logs.

Note: This tutorial is a synthesized version of this [official GCP article](https://cloud.google.com/logging/docs/export/configure_export_v2).

## Step 1: Create a Log Sink

In your GCP Project, or Organization, go to the Logging product and the Logs Router section.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28145%29.png)

Click the Create Sink button, give it a Name and Description.

In the Sink Destination choose Cloud Pub/Sub Topic as a sink service.

Below, select Create a Topic.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28146%29.png)

Give the Topic an ID and click Create Topic.

The Topic should now be creating, which can take a few seconds.

Click Next.

Now you need to choose which logs you want included. Be careful selecting exactly what you want as GCP logs can get very verbose.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28147%29.png)

Click the Preview Logs button in the top right to be taken to the main logging interface where you can experiment with selecting the right logs.

For this example, let's use the following log filter:

```
logName:cloudaudit.googleapis.com
protoPayload.serviceName!="k8s.io"
protoPayload.serviceName!="compute.googleapis.com"
```

This filter will include all cloudaudit logs, except some GKE and GCE logs.

Click Next. You can optionally define an exclusion filter. Let's skip this step.

Click Create Sink. You should get a confirmation the sink was created.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28148%29.png)

## Step 2: Create a Subscription

Go to the Pubsub product.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28149%29.png)

Click on your new Topic.

Click on the Create Subscription button and select Create Subscription.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28150%29.png)

Give this Subscription a name, you will need this name later when configuring the Adapter.

You can leave all other options to their default. Click Create.

## Step 3: Create a Service Account

Head over to the IAM & Admin product. Then the Service Accounts section.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28151%29.png)

Click Create Service Account.

Give the new Service Account a Name and Description. Click Create and Continue.

Select a Role. You want to select Pub/Sub Subscriber.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28152%29.png)

Click Continue. And finally click Done.

This new Service Account has access to the Topic created.

## [OPTIONAL] Step 4: Create a GCE Instance

This step is optional. You may already have a machine you want to run the collector from, in which case you can skip this step.

Head over to the Compute Engine product.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28153%29.png)

Click the Create Instance button.

There is a lot you can customize here, but we'll skip over the more complex aspects you don't need to worry about here.

* Give the instance a name.
* Select a zone nearby the LimaCharlie datacenter you're using.
* As a Machine Type, select e2-micro (the smallest and cheapest machine type).
* In the Identity and API access section, select the Service Account you created earlier. This will set this service account as the default identity of the machine, which in turn means you won't have to specify your credentials to the LimaCharlie Adapter we're about to run.

Click Create. This may take a minute.

Once created, click the SSH button to log on the machine.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28154%29.png)

This will bring you to a console on the machine, ready to install the Adapter.

## Step 5: Create an Installation Key in LimaCharlie

In your Org in LimaCharlie, go to the Sensors > Installation Keys section.

Click the Create Installation Key button. Enter a name for the key. This name will not impact the name given to the source of the logs.

Click on the copy-to-clipboard button next to the Adapter Key column. **The value should be a UUID, keep note of it, you'll need it in the next step.**

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(309).png)

## Step 6: Run the Adapter

First let's download the latest adapter for Linux.

```
curl -L https://downloads.limacharlie.io/adapter/linux/64 -o lc_adapter
chmod +x lc_adapter
```

We can confirm the adapter is running as expected:

```
./lc_adapter
```

You should see all the options available to all the collection methods being printed to the console.

Now let's run the adapter with all the relevant configurations, replacing the various values necessary.

```
./lc_adapter pubsub \
client_options.identity.installation_key=YOUR_INSTALLATION_KEY \
client_options.identity.oid=YOUR_LC_OID \
client_options.platform=gcp \
sub_name=YOUR_SUBSCRIPTION_NAME \
project_name=YOUR_GCP_PROJECT_NAME \
client_options.sensor_seed_key=SOME_ARBITRARY_ADAPTER_NAME
```

You should see some text letting you know the adapter is connecting to LimaCharlie, and if any errors occur fetching data from pubsub.

Within a few seconds you should see the new Sensor in your Sensor List in LimaCharlie.

Within a minute or two you should see the events flowing in the Timeline section of this new sensor.

That's it, you're good to go!

The next step towards production would be to run the Adapter as a service, or within tmux/screen on the Linux host. Alternatively you could also replicate the above setup using the [Docker container](https://hub.docker.com/r/refractionpoint/lc-adapter) and a serverless platform like Cloud Run.

For more documentation on configuring Adapters, see [here](/v2/docs/adapters).

---

# Tutorial: Ingesting Google Cloud Logs

With LimaCharlie, you can easily ingest Google Cloud logs for further processing and automation. This article covers the following high-level steps of shipping logs from GCP into LimaCharlie:

1. Create a Log Sink to Pubsub in GCP
2. Create a Subscription for the Topic
3. Create a Service Account with the required permissions
4. [Optional] Create a GCE instance to run the Adapter
5. Create an Installation Key in LimaCharlie
6. Run the LC Adapter to ingest the logs

Note: This tutorial is a synthesized version of this [official GCP article](https://cloud.google.com/logging/docs/export/configure_export_v2).

## Step 1: Create a Log Sink

In your GCP Project, or Organization, go to the Logging product and the Logs Router section.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28145%29.png)

Click the Create Sink button, give it a Name and Description.

In the Sink Destination choose Cloud Pub/Sub Topic as a sink service.

Below, select Create a Topic.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28146%29.png)

Give the Topic an ID and click Create Topic.

The Topic should now be creating, which can take a few seconds.

Click Next.

Now you need to choose which logs you want included. Be careful selecting exactly what you want as GCP logs can get very verbose.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28147%29.png)

Click the Preview Logs button in the top right to be taken to the main logging interface where you can experiment with selecting the right logs.

For this example, let's use the following log filter:

```
logName:cloudaudit.googleapis.com
protoPayload.serviceName!="k8s.io"
protoPayload.serviceName!="compute.googleapis.com"
```

This filter will include all cloudaudit logs, except some GKE and GCE logs.

Click Next. You can optionally define an exclusion filter. Let's skip this step.

Click Create Sink. You should get a confirmation the sink was created.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28148%29.png)

## Step 2: Create a Subscription

Go to the Pubsub product.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28149%29.png)

Click on your new Topic.

Click on the Create Subscription button and select Create Subscription.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28150%29.png)

Give this Subscription a name, you will need this name later when configuring the Adapter.

You can leave all other options to their default. Click Create.

## Step 3: Create a Service Account

Head over to the IAM & Admin product. Then the Service Accounts section.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28151%29.png)

Click Create Service Account.

Give the new Service Account a Name and Description. Click Create and Continue.

Select a Role. You want to select Pub/Sub Subscriber.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28152%29.png)

Click Continue. And finally click Done.

This new Service Account has access to the Topic created.

## [OPTIONAL] Step 4: Create a GCE Instance

This step is optional. You may already have a machine you want to run the collector from, in which case you can skip this step.

Head over to the Compute Engine product.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28153%29.png)

Click the Create Instance button.

There is a lot you can customize here, but we'll skip over the more complex aspects you don't need to worry about here.

* Give the instance a name.
* Select a zone nearby the LimaCharlie datacenter you're using.
* As a Machine Type, select e2-micro (the smallest and cheapest machine type).
* In the Identity and API access section, select the Service Account you created earlier. This will set this service account as the default identity of the machine, which in turn means you won't have to specify your credentials to the LimaCharlie Adapter we're about to run.

Click Create. This may take a minute.

Once created, click the SSH button to log on the machine.

![image.png](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image%28154%29.png)

This will bring you to a console on the machine, ready to install the Adapter.

## Step 5: Create an Installation Key in LimaCharlie

In your Org in LimaCharlie, go to the Sensors > Installation Keys section.

Click the Create Installation Key button. Enter a name for the key. This name will not impact the name given to the source of the logs.

Click on the copy-to-clipboard button next to the Adapter Key column. **The value should be a UUID, keep note of it, you'll need it in the next step.**

![](https://cdn.document360.io/84ec2311-0e05-4c58-90b9-baa9c041d22b/Images/Documentation/image(309).png)

## Step 6: Run the Adapter

First let's download the latest adapter for Linux.

```
curl -L https://downloads.limacharlie.io/adapter/linux/64 -o lc_adapter
chmod +x lc_adapter
```

We can confirm the adapter is running as expected:

```
./lc_adapter
```

You should see all the options available to all the collection methods being printed to the console.

Now let's run the adapter with all the relevant configurations, replacing the various values necessary.

```
./lc_adapter pubsub \
client_options.identity.installation_key=YOUR_INSTALLATION_KEY \
client_options.identity.oid=YOUR_LC_OID \
client_options.platform=gcp \
sub_name=YOUR_SUBSCRIPTION_NAME \
project_name=YOUR_GCP_PROJECT_NAME \
client_options.sensor_seed_key=SOME_ARBITRARY_ADAPTER_NAME
```

You should see some text letting you know the adapter is connecting to LimaCharlie, and if any errors occur fetching data from pubsub.

Within a few seconds you should see the new Sensor in your Sensor List in LimaCharlie.

Within a minute or two you should see the events flowing in the Timeline section of this new sensor.

That's it, you're good to go!

The next step towards production would be to run the Adapter as a service, or within tmux/screen on the Linux host. Alternatively you could also replicate the above setup using the [Docker container](https://hub.docker.com/r/refractionpoint/lc-adapter) and a serverless platform like Cloud Run.

For more documentation on configuring Adapters, see [here](/v2/docs/adapters).

---

